@inproceedings{10.1145/3737905.3769284,
author = {Zeng, Lan and Huang, Chunhao and Xie, Ruihan and Huang, Zhuohan and Guo, Yunqi and He, Lixing and Xie, Zhiyuan and Xing, Guoliang},
title = {ThermiKit: Edge-Optimized LWIR Analytics with Agent-Driven Interactions},
year = {2025},
isbn = {9798400719820},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3737905.3769284},
doi = {10.1145/3737905.3769284},
abstract = {Low-cost long-wave infrared (LWIR) cameras offer viable privacy-preserving perception for home environments, though deploying effective analytics on diverse, resource-limited devices remains challenging. We present ThermiKit, a plug-and-play thermal analytics stack with two key components: (1) an edge-optimized thermal sensing suite with a unified backbone and compact modules for detection, pose estimation, and tracking, adapted to common LWIR sensors through RGB→thermal transfer; and (2) a micro-MCP server that provides structured model outputs to large language models (LLMs) agents for natural-language interaction without transmitting raw images. Our evaluation uses both a new multi-camera parallel dataset and a long-term nursing-home deployment. The models deliver up to 49.34\% mAP improvement over baseline performance while maintaining real-time operation on sub-1 TOPS devices, and the MCP-enabled agent correctly answers all logically formulated scene queries in our test set (10/10).},
booktitle = {Proceedings of the 2025 ACM International Workshop on Thermal Sensing and Computing},
pages = {40–46},
numpages = {7},
keywords = {LWIR, edge AI, large language models, thermal sensing},
location = {Hong Kong, China},
series = {HotSense '25}
}

@inproceedings{10.1145/3737901.3768365,
author = {Lee, Dong Yoon and Weakley, Alyssa and Wei, Hui and Brown, Blake and Carrion, Keyana and Pan, Shijia},
title = {RRAR: Robust Real-World Activity Recognition with Vibration by Scavenging Near-Surface Audio Online},
year = {2025},
isbn = {9798400719783},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3737901.3768365},
doi = {10.1145/3737901.3768365},
abstract = {One in four people dementia live alone, leading family members to take on caregiving roles from a distance. Many researchers have developed remote monitoring solutions to lessen caregiving needs; however, limitations remain including privacy preserving solutions, activity recognition, and model generalizability to new users and environments. Structural vibration sensor systems are unobtrusive solutions that have been proven to accurately monitor human information, such as identification and activity recognition, in controlled settings by sensing surface vibrations generated by activities. However, when deploying in an end user's home, current solutions require a substantial amount of labeled data for accurate activity recognition. Our scalable solution adapts synthesized data from near-surface acoustic audio to pre-train a model and allows fine tuning with very limited data in order to create a robust framework for daily routine tracking.},
booktitle = {Proceedings of the 3rd ACM International Workshop on Intelligent Acoustic Systems and Applications},
pages = {1–6},
numpages = {6},
keywords = {Activity Recognition, Applied Computing, Domain Adaptation, Machine Learning, Modality Transfer, Transfer Learning, Vibration Sensing},
location = {Hong Kong, China},
series = {IASA '25}
}

@inproceedings{10.1145/3742889.3768345,
author = {Zhang, Chongli and Cuan, Liyuan and Yang, Yufan and Qiao, Liang and Qiao, Xiuquan and Huang, Yakun},
title = {MonoFlex3D: Low-Latency Glasses-Free 3D Teleconference from a Single Camera},
year = {2025},
isbn = {9798400719974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3742889.3768345},
doi = {10.1145/3742889.3768345},
abstract = {The transformative potential of immersive 3D teleconference to replicate face-to-face interaction remains largely unrealized due to persistent barriers in usability and real-time performance. Existing systems often compromise on ease-of-use, require complex multi-sensor setups, struggle with the latency-fidelity trade-off in dynamic human reconstruction, or necessitate cumbersome wearable devices. This paper introduces MonoFlex3D, a novel real-time system designed to dismantle these barriers, enabling low-latency, glasses-free 3D teleconference using only a single commodity RGB camera. At its core, MonoFlex3D employs an avatar-driven approach with a compositional 3D Gaussian Splatting representation. This technique models the human body and face separately using parametric templates, with targeted refinement of the crucial facial region to capture fine-grained expressive details. Extensive evaluations of the MonoFlex3D prototype demonstrate superior real-time capabilities: it achieves high-fidelity avatar animation with approximately 10ms driving latency while maintaining an end-to-end system latency below 100ms at 30 frames per second. It also reduces 50.2\% end-to-end latency versus the simplest capture systems, and enhances visual quality by 9\% to 14.5\% over state-of-the-art alternatives, all while drastically simplifying capture requirements. MonoFlex3D thus offers a significant step towards truly accessible and practical immersive communication.},
booktitle = {Proceedings of the 3rd ACM Workshop on Mobile Immersive Computing, Networking, and Systems},
pages = {1–8},
numpages = {8},
keywords = {Compositional Gaussian Representation, Real-time Teleconference, Single RGB Camera},
location = {Hong Kong, China},
series = {ImmerCom '25}
}

@inproceedings{10.1145/3737901.3768366,
author = {Liu, Yang and Montanari, Alessandro},
title = {EarFusion: Quality-Aware Fusion of In-Ear Audio and Photoplethysmography for Heart Rate Monitoring},
year = {2025},
isbn = {9798400719783},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3737901.3768366},
doi = {10.1145/3737901.3768366},
abstract = {Wearable devices are increasingly adopted for continuous heart rate (HR) monitoring, yet existing solutions remain constrained by modality-specific vulnerabilities. Photoplethysmography (PPG), the most widely used technique, suffers from motion artifacts, skin-sensor variability, and ambient light, whereas in-ear audio sensing, an emerging alternative, is robust to many motion disturbances but easily corrupted by ambient noise and self-speech. These complementary failure modes motivate an integrated approach that can dynamically adapt to signal reliability. This paper introduces EarFusion, the first framework that integrates in-ear audio and PPG for HR estimation through quality-aware fusion. We design modality-specific Signal Quality Indices (SQIs) and incorporate them into a fusion algorithm that allocates trust across streams in real time. Experiments on the OmniBuds platform across diverse activities with synchronized ECG ground truth validate both the effectiveness and feasibility of the approach. EarFusion reduces HR estimation error compared to single-modality baselines, consistently achieves lower variance than either modality alone, and further benefits from SQI-driven integration relative to using a single quality measure.},
booktitle = {Proceedings of the 3rd ACM International Workshop on Intelligent Acoustic Systems and Applications},
pages = {31–36},
numpages = {6},
keywords = {Wearable, heart rate, in-ear audio, multimodal sensing},
location = {Hong Kong, China},
series = {IASA '25}
}

@inproceedings{10.1145/3737895.3768307,
author = {Loreti, Pierpaolo and De Luca, Massimiliano and Bracciale, Lorenzo and Catini, Alexandro and Mangione, Stefano and Tinnirello, Ilenia},
title = {An Energy-Harvesting Shield and Solar Testbed for IoT: LoRa Performance Insights and Power Measurements},
year = {2025},
isbn = {9798400719721},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3737895.3768307},
doi = {10.1145/3737895.3768307},
abstract = {Designing firmware for energy-harvesting (EH) IoT devices requires testing under realistic and repeatable conditions, yet most simulation tools abstract away critical physical constraints, and field testing is often impractical or non-reproducible. This paper presents a two-part platform to address this challenge: (i) a modular shield for the STM32 NUCLEO development boards that integrates a solar panel, energy harvesting power management, environmental and motion sensors, GPS, and flash memory; and (ii) an indoor testbed that reproduces real-world solar irradiance using programmable dimmable lamps driven by recorded sunlight traces. Together, these tools enable reproducible experimentation on energy-aware firmware, including runtime adaptation and energy budgeting strategies. Furthermore, we analyze real-world LoRa communication performance and highlight a lesser-known phenomenon where higher spreading factors, while theoretically robust, experience signal degradation due to synchronization loss. Our platform provides a practical foundation for long-term, realistic evaluation of EH-powered IoT systems.},
booktitle = {Proceedings of the ACM Workshop on Wireless Network Testbeds, Experimental Evaluation \&amp; Characterization},
pages = {105–112},
numpages = {8},
keywords = {energy harvesting, solar-powered IoT, LoRA communication, indoor testbed, wireless sensor networks},
location = {Kerry Hotel, Hong Kong, Hong Kong, China},
series = {WiNTECH '25}
}

@inproceedings{10.1145/3737895.3768295,
author = {Alghisi, Giovanni Angelo and Perin, Giovanni and Meneghello, Francesca and Gringoli, Francesco},
title = {Let It Beam: Enabling Selective and Secure CSI-based Sensing via Beamforming},
year = {2025},
isbn = {9798400719721},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3737895.3768295},
doi = {10.1145/3737895.3768295},
abstract = {Integrated sensing and communication (ISAC) strategies are key components of next-generation wireless networks, including Wi-Fi systems, enabling new services and supporting network management operations. The main idea behind this integration is that the channel state information (CSI), which is continuously estimated for communication purposes, can be leveraged for environmental sensing, especially with the aid of artificial intelligence (AI) algorithms. However, this capability comes with privacy concerns, as passive eavesdroppers, even when using commercial off-the-shelf (COTS) devices, can estimate the CSI and potentially infer sensitive information. In this paper, we propose a new technique to mitigate this risk, leveraging the potentialities of multiple-input multiple-output (MIMO) systems. We design an obfuscation and de-obfuscation system that conceals the real CSI from eavesdroppers, while enabling trusted devices to reverse the distortion and perform sensing normally. We implemented a prototype of our system through software-defined radios (SDRs) and evaluated the effectiveness of our proposed approach considering a device-free localization task. The results show that the obfuscation makes it unfeasible to perform sensing at unauthorized devices (the accuracy drops to about 20\%) while the de-obfuscation at legitimate devices allows reaching almost 100\% in sensing accuracy. To support reproducibility, we make the dataset and code publicly available.},
booktitle = {Proceedings of the ACM Workshop on Wireless Network Testbeds, Experimental Evaluation \&amp; Characterization},
pages = {9–16},
numpages = {8},
keywords = {integrated sensing and communication (ISAC), wi-fi sensing, channel state information (CSI), MIMO systems, beam-forming, physical layer security, device-free localization},
location = {Kerry Hotel, Hong Kong, Hong Kong, China},
series = {WiNTECH '25}
}

@article{10.1145/3774426,
author = {Qin, Hong and Debiao, He and Feng, Qi and Luo, Min},
title = {Secure and Dropout-Resilient Three-Party Clustering Based on Cloud-Edge-Client Collaboration},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1533-5399},
url = {https://doi.org/10.1145/3774426},
doi = {10.1145/3774426},
abstract = {Clustering algorithms, as the core technology in data analysis, can extract potential patterns and regularities from complex data. However, deploying k-means clustering on resource-limited devices remains a challenge. Despite the promise of cloud computing, outsourcing data to a remote cloud leads to high latency and privacy risks. Moreover, the stability and speed of cloud can be affected by the state of network and configuration, which leads to computation error. Therefore, we design a secure and dropout-resilient k-means clustering scheme based on cloud-edge-client collaboration architecture. In our scheme, cloud server simply generates multiplication triples in pre-processing phase and can be offline. In online phase, IoT devices secretly share the raw sensing data with three edge servers. Then edge servers accomplish the clustering task interactively. We propose four basic protocols based on vector space secret sharing, including Euclidean distance, comparison, minimum and division protocols. By applying these protocols, we construct a clustering scheme that can tolerate the exit of one edge server and corruption of two edge servers. Since edge servers are generally located in trusted environment, we allow them to reconstruct clustering result and provide low-latency and high-reliability service. We prove that the basic protocols and clustering scheme are secure against semi-honest adversary. We conduct the experiments on two realistic datasets, showing that our scheme has good efficiency and is suitable for practical application.},
note = {Just Accepted},
journal = {ACM Trans. Internet Technol.},
month = nov,
keywords = {IoT security, k-means clustering, cloud-edge-client collaboration, vector space secret sharing, semi-honest}
}

@inproceedings{10.1145/3765325.3765402,
author = {Qu, Na and Wang, Qinghui and Sun, Hongmei},
title = {Construction and Practice of a Virtual Reality-Based Teaching System for AGV Operation in Supply Chain Education},
year = {2025},
isbn = {9798400715846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3765325.3765402},
doi = {10.1145/3765325.3765402},
abstract = {As virtual reality technology becomes increasingly prevalent in higher education, a key challenge in logistics and supply chain management education lies in developing immersive teaching systems tailored to the demands of specialized courses. Based on the classroom practice in the School of Management of the University of Sheffield, UK, this study designs and implements a virtual reality teaching system for AGV operation tasks. Based on Kolb's experiential learning theory, the system constructs a three-phase teaching process of “cognition-operation-collaboration”, integrates path planning algorithms and behavioral assessment mechanisms, and realizes an immersive, task-driven logistics training environment. The experimental results show that the collaborative VR group is significantly better than the traditional teaching group in terms of task completion rate, path efficiency and response feedback, and the students’ feedback also shows a higher degree of participation and sense of identity. Combined with the current situation of logistics education in Chinese colleges and universities, this paper puts forward a set of system adaptation path suggestions to provide technical paths and implementation references for the popularization and application of virtual reality technology in logistics professional education.},
booktitle = {Proceedings of the 2025 3rd International Conference on Educational Knowledge and Informatization},
pages = {457–463},
numpages = {7},
keywords = {AGV Training, Experiential Learning Model, Immersive Learning, Supply Chain Education, Virtual Reality (VR) in Education},
location = {
},
series = {EKI '25}
}

@inproceedings{10.1145/3766918.3766920,
author = {Gao, Chen},
title = {Federated P-Tuning based Time-LLM for Hotel Booking Prediction},
year = {2025},
isbn = {9798400716027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3766918.3766920},
doi = {10.1145/3766918.3766920},
abstract = {With the rapid development of large language models (LLMs), their potential in time series prediction tasks is gradually being explored. LLMs possess strong feature representation capabilities and the ability to model complex relationships. They have shown significant advantages in areas such as text generation, classification, and time series prediction. However, pre-trained LLMs usually perform well on general tasks but are difficult to apply directly to specific tasks. Fine-tuning is required to adapt them to target domain characteristics. Meanwhile, hotel booking data is often distributed across multiple organizations. This data is privacy-sensitive and unevenly distributed, making traditional centralized training methods unsuitable for privacy protection and collaborative modeling. To address these issues, we propose FPTuning-LLM, a fine-tuning framework for large language models based on Federated Learning and P-Tuning. But time series data cannot be directly input into LLMs trained on discrete tokens. Therefore, we use Time-LLM to embed input sequences and convert them into representations suitable for LLMs. Prompt design is crucial for downstream task performance. We adopt P-Tuning to fine-tune Time-LLM, utilizing lightweight parameters to improve task adaptation. In a federated learning environment, we perform global fusion on the embedding layers of Time-LLM and the LSTM+MLP network in P-Tuning. To address the issue of non-independent and identically distributed (Non-IID) data, FedProx is used to constrain local training, enhancing model robustness and global convergence. Experimental results show that FPTuning-LLM effectively improves prediction accuracy while ensuring data privacy by leveraging private data from multiple parties.},
booktitle = {Proceedings of the 2025 International Conference on Generative Artificial Intelligence for Business},
pages = {7–11},
numpages = {5},
keywords = {Federated Learning, Hotel Booking Prediction, Large Language Models (LLMs), P-Tuning, Time Series Prediction},
location = {
},
series = {GAIB '25}
}

@inproceedings{10.1145/3766918.3766977,
author = {Wang, Yu and Jia, Yanrui and Shao, Min},
title = {Consumer Profiling Based on K-Means Clustering and Strategic Guidance for Green Consumption Willingness under the Perspective of Ecological Civilization},
year = {2025},
isbn = {9798400716027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3766918.3766977},
doi = {10.1145/3766918.3766977},
abstract = {In the context of ecological civilization construction, green consumption is of great significance to sustainable development. The generation mechanism and influencing factors of green consumption willingness are worth exploring in depth. Based on consumer value perception and value-belief-norm theory, this study obtained 356 valid samples through questionnaires, with the help of SPSSAU platform to explore the influence of perceived value and sustainable consumption on the willingness to consume green, and the moderating effect of price sensitivity. The results show that the functional, emotional, social, and green values all positively and significantly affect green consumption intention. Also price sensitivity negatively moderates the relationship between perceived value and consumption intention. And sustainable consumption outlook strengthens environmental protection beliefs and ethical norms, and becomes the core driving force of green consumption intention. To further explore consumer heterogeneity, k-means clustering was applied to seven variables. The optimal number of clusters was determined to be two, clearly separating consumers into a price-sensitive group with lower green intention and a high-value-perception group with stronger willingness to consume green. Based on these results, the study proposes the construction of a multi-dimensional value perception system, the implementation of differentiated price strategies, the promotion of sustainable consumption concept education and other guiding paths, to provide theoretical and practical support for the government and enterprises to promote the transformation of green consumption.},
booktitle = {Proceedings of the 2025 International Conference on Generative Artificial Intelligence for Business},
pages = {353–359},
numpages = {7},
keywords = {Ecological civilization, Green consumption, Guidance strategy, K-means clustering, Price sensitivity},
location = {
},
series = {GAIB '25}
}

@inproceedings{10.1145/3766918.3766964,
author = {Ji, Jinyuan and Zhang, Haoqiang and Cai, Liu},
title = {Literature Review of AI in Healthcare},
year = {2025},
isbn = {9798400716027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3766918.3766964},
doi = {10.1145/3766918.3766964},
abstract = {Artificial intelligence has become an important force driving changes in the medical field, from disease diagnosis to drug development, from personalized treatment to medical resource management. Artificial intelligence is gradually penetrating into every aspect of healthcare, bringing unprecedented opportunities for the development of the medical industry. Artificial intelligence, with its powerful data analysis capabilities and deep learning algorithms, can quickly and accurately analyze various medical imaging and medical record data such as X-rays and CT scans. In surgical treatment, artificial intelligence driven surgical robots have also emerged. Da Vinci surgical robots can achieve more precise and stable operations through robotic arms, reduce surgical risks, and improve surgical success rates. Through technologies such as big data analysis and machine learning, artificial intelligence can quickly screen potential drug targets, design more effective drug molecules, and accelerate the process of drug development. Establish strict data protection mechanisms for a large amount of sensitive information of patients. The development prospects of artificial intelligence in the medical field are broad, which will bring more efficient, accurate, and personalized medical services to the medical industry, improve patients' medical experience, and enhance human health.},
booktitle = {Proceedings of the 2025 International Conference on Generative Artificial Intelligence for Business},
pages = {272–278},
numpages = {7},
keywords = {Artificial intelligence, Medical robots, Brain computer interface, New drug development, Coping strategies, Multidisciplinary integration},
location = {
},
series = {GAIB '25}
}

@inproceedings{10.1145/3768740.3768771,
author = {Qiao, Jiageng},
title = {Neural Network-Based Expert System for Identification of Safety Hazards in Distribution Network Equipment},
year = {2025},
isbn = {9798400720987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3768740.3768771},
doi = {10.1145/3768740.3768771},
abstract = {The normal operation of the power system is an important guarantee for the development of the national economy. Large-scale failures within the power system can lead to significant economic losses and severely impact national defense security, social production, urban transportation, healthcare systems, educational institutions, and daily life of residents. Traditional methods for safety hazard identification in distribution network equipment often rely heavily on expert experience and manual analysis, which can be inefficient, subjective, and limited in handling complex or unseen fault patterns. To overcome these challenges, this paper proposes a backpropagation (BP) neural network-based expert system designed to automatically and accurately identify potential safety hazards in distribution network equipment. By leveraging the self-learning capability and powerful pattern recognition of neural networks, the system effectively diagnoses faults and interprets complex, non-linear relationships within operational data. The core of the method involves representing fault characteristics as feature vectors. These vectors are constructed from real-time sensor data and historical fault records, capturing key indicators of equipment anomalies. The collected feature vectors are fed into the neural network model, which has been trained on extensive datasets to recognize patterns associated with various types of faults. The output of the system provides the probability distribution across different potential causes of safety hazards, thereby assisting operators in making informed maintenance decisions. Experimental results demonstrate that the system exhibits strong real-time performance and high diagnostic accuracy. It significantly reduces reliance on manual expertise and mitigates the impact of knowledge and experience gaps in safety hazard identification. The proposed approach offers a scalable, reliable, and intelligent solution for enhancing the reliability and safety of modern power distribution networks.},
booktitle = {Proceedings of the 2025 International Conference on Simulation, Modeling and Big Data},
pages = {201–206},
numpages = {6},
keywords = {BP neural network, EMTP simulation, distribution network equipment, expert system, identification of safety hazards},
location = {
},
series = {SMBD '25}
}

@inproceedings{10.1145/3767624.3767651,
author = {Hou, Zherui and Xiong, Zibo and He, Yushan and Xiong, Ziliang and Pan, Haoyang and Wang, Canbo and Yang, Han and Tang, Huiying},
title = {Design and Implementation of an Intelligent Monitoring System for Vertical Farming Based on STM32F103RCT6},
year = {2025},
isbn = {9798400715907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3767624.3767651},
doi = {10.1145/3767624.3767651},
abstract = {As an emerging solution to reconcile urbanization and food security challenges, vertical farming still faces issues including fragmented functionality and lack of closed-loop control in existing research. This paper presents a full-stack intelligent monitoring and control system for vertical farms. The system employs STM32F103RCT6 as the core controller, integrating multi-source sensor networks (temperature/humidity, light intensity, CO2 concentration, etc.) with LoRa low-power communication modules to establish a closed-loop control framework featuring "environmental perception - data fusion - decision execution". It achieves precise control over LED spectrum adjustment, hydroponic nutrient cycling, and ventilation/thermal regulation. At the software layer, a hierarchical communication protocol maintains packet integrity rates exceeding 80\% within 98m range while keeping control command latency below 200ms. Through localized decision-making and coordinated device control, the system resolves dynamic optimization challenges of multi-dimensional parameters in vertical farming, delivering a highly reliable, low-latency integrated solution for high-density urban agriculture.},
booktitle = {Proceedings of the 2025 International Conference on Smart Agriculture and Artificial Intelligence},
pages = {184–191},
numpages = {8},
keywords = {LoRa communication, STM32F103RCT6, environmental monitoring, vertical farming},
location = {
},
series = {SAAI '25}
}

@inproceedings{10.1145/3767624.3767633,
author = {Cheng, Qianjunye and Du, Haini and Zhong, Xuan and Guo, Hanqing and Wang, Shen and Xie, Jiaming and Huang, Jingjing and Liu, Jianguo and Zhang, Jiaqin and Jia, Guoku},
title = {An Unmanned Surface Vehicle for Fish Health Assessment, Water Quality Monitoring, and Pollution Treatment Using YOLOv11},
year = {2025},
isbn = {9798400715907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3767624.3767633},
doi = {10.1145/3767624.3767633},
abstract = {This study proffers the development of an autonomous unmanned surface vehicle (USV) as a means of efficaciously monitoring the health of fish in aquatic environments. The vehicle has been equipped with state-of-the-art sensing and computer vision technologies. The system is based on an Arduino Mega 2560. This is the primary command centre. Vision processing is handled by a Raspberry Pi, and the USV is integrated with a high-resolution underwater camera. This, integrated with the YOLOv11 deep learning algorithm, facilitates real-time identification of piscine health. The system also has a variety of environmental sensors, including pH, temperature, total dissolved solids (TDS) and turbidity sensors, which monitor key water quality parameters affecting fish health. The amalgamation of environmental data with visual recognition facilitates the identification of piscine ailments and the dissemination of alerts for timely intervention. It is equipped with a GPS navigation system and can be operated via a web-based control panel, which is a rather splendid feature. The system enables water quality sampling and fish health monitoring, which is excellent. This is a pretty innovative solution, It's a great tool for keeping fish healthy and for automating the monitoring of the aquaculture environment. It makes fish farming more sustainable and efficient.},
booktitle = {Proceedings of the 2025 International Conference on Smart Agriculture and Artificial Intelligence},
pages = {52–62},
numpages = {11},
keywords = {Fish Health Assessment, Real-time Monitoring, Unmanned Surface Vehicle (USV), Water Quality Monitoring, YOLOv11},
location = {
},
series = {SAAI '25}
}

@inproceedings{10.1145/3767624.3767653,
author = {Xie, Yi and Fan, Xiaojiao and He, Yinuo},
title = {An Intelligent Monitoring System for Greenhouse Strawberries Based on Internet of Things Technology},
year = {2025},
isbn = {9798400715907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3767624.3767653},
doi = {10.1145/3767624.3767653},
abstract = {To deal with the problems like delayed monitoring, low regulation efficiency, and high wiring costs that exist in traditional strawberry cultivation, we came up with and put into practice an intelligent monitoring system which is based on Internet of Things (IoT) technology. This system has a four-layer architecture known as 'Perception-Processing-Network-Application'. The Perception Layer brings together various sensors. For instance, it has the DHT11 with a temperature/humidity accuracy of ±0.3°C/±2\%RH, the MQ-812 for gas detection, and the BH1750 which has a light intensity resolution of 1 lx. These sensors are used to collect environmental data. The Processing Layer, with the STM32F103C8T6 microcontroller at its core, makes use of a sliding average filtering algorithm to do the data preprocessing. The Network Layer takes advantage of the ESP8266 Wi-Fi module that has a power consumption of 0.8 W and the MQTT protocol to ensure reliable data transmission to the cloud. On the Application Layer, a visual management system and a mini-program have been developed on the OneNet platform. This allows for remote configuration, heatmap visualization, and growth trend prediction. The experimental results show that the system manages to keep the temperature/humidity monitoring errors within ±0.3°C/±2\%RH, has a light intensity resolution of 1 lx, the overall power consumption is only 0.8 W, the data upload success rate reaches 97.2\%, and the average network packet loss rate is 2.17\%. The system works in a stable and reliable way. It is low in cost, has great real-time performance, ensures secure communication, and is easy to operate. In effect, it meets the requirements for precise strawberry cultivation well. This solution offers a scalable and cost-effective way of deployment. It can also be extended to other high-value crops and can be adjusted to work with NB-IoT/LoRa technology.},
booktitle = {Proceedings of the 2025 International Conference on Smart Agriculture and Artificial Intelligence},
pages = {200–208},
numpages = {9},
keywords = {Edge Computing, Internet of Things, MQTT Protocol, OneNet Platform},
location = {
},
series = {SAAI '25}
}

@inproceedings{10.1145/3767624.3767639,
author = {Zhao, Mengqi and Leng, Yonggang and Zhou, Yansuo and Zhang, Yuyang and Wang, Yingyi and Wu, Zixing and Xu, Junjie},
title = {An Intelligent Control Method for Temperature and Humidity of Agricultural Glass Greenhouse Based on Fuzzy Neural Network},
year = {2025},
isbn = {9798400715907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3767624.3767639},
doi = {10.1145/3767624.3767639},
abstract = {The agricultural glass greenhouse is a powerful supplement to the traditional agricultural production mode, and a key carrier to cultivate new agricultural productivity and promote the high-quality development of agricultural modernization. Temperature and humidity are the decisive factors for the growth of crops in the greenhouse. How to dynamically adjust the growth environment of crops in the greenhouse based on the comprehensive optimum of temperature and humidity is the key to ensure the efficient and stable operation of the greenhouse. Therefore, an intelligent temperature-humidity control method based on fuzzy neural network was proposed in this paper. Internet of Things (IoT) for real-time temperature and humidity sensing was set up; The fuzzy neural network was used to intelligently control the temperature-humidity in the greenhouse, making up the deficiency that nonlinear system control needs a lot of complex mathematical models but still cannot be accurately controlled; The validity of this method was verified by Simulink, which proved that the method can provide a high quality environment for the growth of greenhouse crops.},
booktitle = {Proceedings of the 2025 International Conference on Smart Agriculture and Artificial Intelligence},
pages = {106–111},
numpages = {6},
keywords = {Agricultural glass greenhouse, Fuzzy neural network, Temperature-humidity intelligent control},
location = {
},
series = {SAAI '25}
}

@inproceedings{10.1145/3758871.3758904,
author = {Liu, Minghui and Zheng, Ruishen and Niu, Ruowen and Zuo, Yirui and Zhang, Weiwei},
title = {A Literature Review of Indoor Navigation: Examining Comparability and Ecological Validity between Virtual and Real Environments},
year = {2025},
isbn = {9798400713897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3758871.3758904},
doi = {10.1145/3758871.3758904},
abstract = {As urbanization accelerates, effectively assisting individuals in complex indoor environments has emerged as a pressing research topic. In recent years, the development of Virtual Reality (VR) technology has become a new tool that enables researchers to simulate human wayfinding behavior in controlled settings for deeper investigation. Since sensory perception and individual differences are factors that significantly influence wayfinding behavior in real environments, many researchers used VR technology to simulate human wayfinding behaviors from these aspects, exploring potential solutions to wayfinding challenges. However, the analogy of virtual and real environments in existing studies, as well as the applicability of their findings to real environments, remains uncertain. Therefore, we conducted a systematic literature review and compiled a comprehensive analysis table detailing the experimental factors, conditions, methodologies, and results from relevant studies. Based on these analogize, we categorized ecological validity into three types: Proven ecological validity, Extrapolated ecological validity, and Relevant ecological validity. This work provides researchers with a comparative perspective, helps them clearly understand the scope and limitations of research findings in virtual environments, and guides them in making informed decisions for future experimental designs. Furthermore, the categorization of ecological validity facilitates the reasonable application of VR technology in indoor wayfinding research and supports research findings to effectively translate into practical applications.},
booktitle = {Proceedings of the Twelfth International Symposium of Chinese CHI},
pages = {405–416},
numpages = {12},
keywords = {Indoor wayfinding, Analogy, Ecological Validity, VR},
location = {
},
series = {CHCHI '24}
}

@inproceedings{10.1145/3758871.3758966,
author = {Jiang, Mengqi and Liu, Hongwei and Guo, Jiawei and Hu, Hongci and Bai, Ziqian},
title = {Emotion Visualization and Emotion Recognition via Textile Strain-Sensing Network Clothes},
year = {2025},
isbn = {9798400713897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3758871.3758966},
doi = {10.1145/3758871.3758966},
abstract = {It has been widely recognized that there is a relationship between human emotions and body motions. Most body motion-based emotion recognition solutions rely on visual data from cameras or hardware like IMU and EMG sensors. However, existing solutions have privacy constraints and are generally limited to strict lab environments. This paper proposed a novel emotion recognition approach from textile-based strain-sensing network clothes, which can monitor emotion-related body activities. This work constructed a wearable strain-sensing network system for emotion recognition and emotion visualization, and the preliminary experiment on the upper body also supported the hypothesis that textile-based strain-sensing network clothes can be applied to emotion recognition. This project is dedicated to constructing wearable, portable, and flexible textile sensor network-integrated clothing and corresponding emotion analysis systems, innovating existing emotion recognition approaches beyond the lab environment. This work also discussed the reflection and future research directions based on the preliminary experiment result.},
booktitle = {Proceedings of the Twelfth International Symposium of Chinese CHI},
pages = {571–583},
numpages = {13},
keywords = {E-textiles, Emotion recognition, Emotion visualization, Affective computing, Sensing network, Wearables},
location = {
},
series = {CHCHI '24}
}

@inproceedings{10.1145/3758871.3758886,
author = {Ding, Ke and Chen, Ruxiao and Guan, Zezheng and Je, Seungwoo},
title = {ThermoShift: Enhancing VR Immersion with Thermal Motion Simulation},
year = {2025},
isbn = {9798400713897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3758871.3758886},
doi = {10.1145/3758871.3758886},
abstract = {Hand-based sensory interaction simulations could effectively enhance the user’s sense of immersion in Virtual Reality (VR). Previous research on haptic interactions focuses on sensations such as texture and weight perception simulation. Integrating thermal feedback into hand-based interactions remains a novel area of exploration. In this paper, we introduce ThermoShift, a wearable glove designed to deliver dynamic thermal illusions in the palm by leveraging the phenomenon of thermal referral, strengthening the immersive experience in VR. ThermoShift uses Peltier elements for thermal stimulation and vibration motors to create the sensation of moving heat, enabling users to experience various environmental effects in VR scenarios. Given that previous research demonstrated that combined thermal stimulation and pressure cues can create the illusion of thermal movement on the forearm, we conducted a user study to determine the optimal number and arrangement of pressure actuators to design a glove-type wearable device. Our study suggested that ThermoShift provides high resolution and dynamic thermal motion simulation to enhance the immersive experience, with potential applications that span intricate virtual spaces and daily wearable solutions for improved user experience.},
booktitle = {Proceedings of the Twelfth International Symposium of Chinese CHI},
pages = {197–205},
numpages = {9},
keywords = {Thermal feedback, Wearable devices, Haptic interaction, Thermal illusion, Virtual reality},
location = {
},
series = {CHCHI '24}
}

@inproceedings{10.1145/3758871.3758875,
author = {Liu, Jiacheng and Li, Yue and Zhang, Fan},
title = {Towards Modelling Distracted Human Driving: Sensor Fusion and Driver Status Monitoring in a VR Simulator},
year = {2025},
isbn = {9798400713897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3758871.3758875},
doi = {10.1145/3758871.3758875},
abstract = {Various driver monitoring systems have been deployed to understand human driving behaviours in complex scenarios, contributing to the development of automated vehicles that meet technical and legal requirements. However, commercial systems are often overpriced, and there is still limited understanding of how driving behaviour, distractions, and scenarios interact to influence decision-making and performance. This study addresses the gap by collecting behavioural and physiological data in different driving tasks and modelling human decision-making. In a between-subject design, participants were instructed to drive safely or aggressively through three simulated scenes, namely the crossroads, the T-junction, and the roundabout, under five distraction conditions: 1) no distraction, 2) audio-cognitive, 3) audio-action, 4) visual-cognitive, and 5) visual-action. Each participant completed forty-five trials, lasting 30-40 minutes. The driving scene was developed in Unreal Engine 4, using Microsoft AirSim. The experiment setup included a multi-sensor driver monitoring system, a driving simulator with wheel and pedals, and a VIVE Pro 2 VR display, to collect behavioural (e.g., head movements, steering) and physiological (e.g., heart rate, skin conductance) data. ANOVA was performed to explore behavioural patterns and physiological responses, including differences between safe and aggressive driving, distractions, and scenarios. Significant differences across conditions were revealed. Specifically, the throttle, steering, acceleration, the speed of the vehicle, the heart rate, and the head turning of participants in aggressive driving are significantly different from those in safe driving. Distraction conditions had a significant impact on the steering and head turning ranges. Our contributions include setting up a realistic driving simulation environment with affordable solutions and creating a human driving data collection pipeline for modelling driving performance. Future work will focus on improving data acquisition, modelling human decision-making, and integrating these models into the planning and control of automated vehicles to enhance AI transparency and public acceptance of autonomous driving.},
booktitle = {Proceedings of the Twelfth International Symposium of Chinese CHI},
pages = {31–46},
numpages = {16},
keywords = {Driver Monitoring System, Driving Simulator, Decision-making, Virtual Reality (VR), Distraction},
location = {
},
series = {CHCHI '24}
}

@article{10.1145/3758104,
author = {Lombardi, Maria and Maiettini, Elisa and Wykowska, Agnieszka and Natale, Lorenzo},
title = {Gaze Estimation Learning Architecture as Support to Affective, Social and Cognitive Studies in Natural Human–Robot Interaction},
year = {2025},
issue_date = {January 2026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
url = {https://doi.org/10.1145/3758104},
doi = {10.1145/3758104},
abstract = {Gaze is a crucial social cue in any interacting scenario and drives many mechanisms of social cognition (joint and shared attention, predicting human intention and coordinating tasks). Gaze is an indication of social and emotional functions affecting the way the emotions are perceived. Evidence shows that embodied humanoid robots endowed with social abilities can be seen as sophisticated stimuli to study several mechanisms of human social cognition while increasing engagement and ecological validity. In this context, building a robotic perception system to automatically estimate the human gaze only relying on robot’s sensors is still demanding. Main goal of the article is to propose a learning robotic architecture estimating the human gaze direction in table-top scenarios without any external hardware. Table-top tasks are largely used in experimental psychology because they are suitable to implement numerous face-to-face collaborative scenarios. Such an architecture can provide a valuable support in studies where external hardware might represent an obstacle to spontaneous human behaviour, especially in environments less controlled than the laboratory (e.g., in clinical settings). A novel dataset was also collected with the humanoid robot iCub, including images annotated from 24 participants in different gaze conditions.},
journal = {J. Hum.-Robot Interact.},
month = oct,
articleno = {16},
numpages = {22},
keywords = {gaze estimation, experimental psychology, non-invasive robotic setup, computer vision, human–robot interaction, humanoid robot}
}

@inproceedings{10.1145/3748825.3748955,
author = {Lu, Lanqian and Dan, Xiaoyu and Hu, Chunjing and Qian, Shun and Yang, Shaobin and Lin, Yating and Gao, Yaran and Pan, Keming and Wu, Jie and Zhang, Qi},
title = {Tomato Maintenance System Based on Improved YOLOv8},
year = {2025},
isbn = {9798400714337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3748825.3748955},
doi = {10.1145/3748825.3748955},
abstract = {Driven by the rural revitalization strategy, intelligent agriculture is developing at an unprecedented speed. To meet the demands of intelligent upgrading in agriculture, this paper innovatively proposes an intelligent vegetable maintenance system based on edge computing. This system integrates multiple high-precision sensors and the improved YOLO-BiFaster model, which can monitor the agricultural production environment in real time and accurately identify the growth status of vegetables. The modular design of the system enables multiple devices to operate in coordination and realizes remote monitoring and data visualization based on the cloud platform, greatly enhancing the intelligent level of agricultural management. Experiments show that the system has significant advantages in improving maintenance efficiency, reducing waste and enhancing accuracy, providing technical support for the development of smart agriculture.},
booktitle = {Proceedings of the 2025 2nd International Conference on Digital Society and Artificial Intelligence},
pages = {842–848},
numpages = {7},
keywords = {ESP32, Edge Computing, Irrigation Optimization, MaixCam, Smart Agriculture, YOLO-BiFaster},
location = {
},
series = {DSAI '25}
}

@inproceedings{10.1145/3755881.3755922,
author = {Sun, Bingkun and Ren, Jialin and Luo, Juntao and Shen, Liwei and Lu, Yongqiang and Chen, Qicai and Dong, Zhen and Peng, Xin},
title = {EnvGuard: Guaranteeing Environment-Centric Safety and Security Properties in Web of Things System},
year = {2025},
isbn = {9798400719264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3755881.3755922},
doi = {10.1145/3755881.3755922},
abstract = {Web of Things (WoT) technology standardizes the integration of various IoT devices deployed in daily environments, promoting the capability of applications to automatically sense and regulate the physical environment. Meanwhile, the complex nature of such a ubiquitous software system, where heterogeneous applications, user activities, and environment states collectively influence device behaviors, poses risks of unexpected or even hazardous safety and security violations caused by improper device operations. Existing works on WoT violation identification primarily focus on the sole analysis of software applications, however, lacking consideration of the multi-source violations stemming from the human-cyber-physical ternary spaces, as well as the intricate interplay between environment and devices. Furthermore, the investigation into users’ preferences for violation resolution remains unexplored. To address these limitations, we introduce EnvGuard, an environment-centric approach for customizing safety and security properties, identifying violations, and executing resolutions in the WoT environment. Our evaluation in two real-world WoT systems shows that EnvGuard outperforms previous state-of-the-art works, and confirms its usability, effectiveness, and runtime efficiency.},
booktitle = {Proceedings of the 16th International Conference on Internetware},
pages = {545–557},
numpages = {13},
keywords = {Web of Things, Safety and Security, Human–Cyber–Physical Systems, Environment-Centric Approaches},
location = {
},
series = {Internetware '25}
}

@inproceedings{10.1145/3746027.3755115,
author = {Zhu, Yitong and Liang, Zhuowen and Wu, Yiming and Li, Tangyao and Wang, Yuyang},
title = {Towards Consumer-Grade Cybersickness Prediction: Multi-Model Alignment for Real-Time Vision-Only Inference},
year = {2025},
isbn = {9798400720352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746027.3755115},
doi = {10.1145/3746027.3755115},
abstract = {Cybersickness remains a major obstacle to the widespread adoption of immersive virtual reality (VR), particularly in consumer-grade environments. While prior methods rely on invasive signals such as electroencephalography (EEG) for high predictive accuracy, these approaches require specialized hardware and are impractical for real-world applications. In this work, we propose a scalable, deployable framework for personalized cybersickness prediction leveraging only non-invasive signals readily available from commercial VR headsets, including head motion, eye tracking, and physiological responses. Our model employs a modality-specific graph neural network enhanced with a Difference Attention Module to extract temporal-spatial embeddings capturing dynamic changes across modalities. A cross-modal alignment module jointly trains the video encoder to learn personalized traits by aligning video features with sensor-derived representations. Consequently, the model accurately predicts individual cybersickness using only video input during inference. Experimental results show our model achieves 88.4\% accuracy, closely matching EEG-based approaches (89.16\%), while reducing deployment complexity. With an average inference latency of 90ms, our framework supports real-time applications, ideal for integration into consumer-grade VR platforms without compromising personalization or performance. The code will be relesed at https://github.com/U235-Aurora/PTGNN.},
booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia},
pages = {6859–6867},
numpages = {9},
keywords = {consumer-grade deployment, cross-modal alignment, cybersickness prediction, difference attention},
location = {Dublin, Ireland},
series = {MM '25}
}

@inproceedings{10.1145/3746027.3755767,
author = {Li, Xingchen and Zhang, Wuyang and You, Guoliang and Chu, Xiaomeng and Yu, Wenhao and Duan, Yifan and Xiao, Yuxuan and Zhang, Yanyong},
title = {CalibWorkflow: A General MLLM-Guided Workflow for Centimeter-Level Cross-Sensor Calibration},
year = {2025},
isbn = {9798400720352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746027.3755767},
doi = {10.1145/3746027.3755767},
abstract = {Extrinsic calibration is a fundamental step in sensor fusion systems. However, existing methods often lack generalization capabilities when facing diverse hardware configurations, sensor poses, and environmental conditions, hindering their large-scale deployment. To address this limitation, we propose a general extrinsic calibration method, CalibWorkflow. Our core innovation lies in positioning multimodal large language models (MLLMs) as ''visual guides'' for the calibration process, leveraging their powerful vision-language understanding capabilities to guide parameter search and refinement. This reliance on visual scene understanding, rather than specific geometric features or sensor characteristics, enables the method to generalize effectively across diverse hardware and environmental conditions. Specifically, CalibWorkflow employs a three-stage calibration pipeline: initial parameter search, coarse optimization, and fine optimization. First, it utilizes the MLLM to assess the visual consistency between the projected point cloud and the image, rapidly determining an initial range for the extrinsic parameters. Next, the MLLM serves as a differential evaluator, giving simple ''better'' or ''worse'' feedback on parameter changes to guide the search through the parameter space. Finally, the method refines the calibration by matching edge features and performing non-linear optimization. Extensive experiments are conducted across six diverse scenarios and four heterogeneous sensor combinations. CalibWorkflow achieves state-of-the-art sub-degree and centimeter-level accuracy on four datasets and demonstrates highly competitive performance on others. These results thoroughly validate the generalization and robustness when facing various scenarios. Codes will be available.},
booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia},
pages = {2178–2187},
numpages = {10},
keywords = {lidar-camera calibration, multi-modal large language model, visual guidance},
location = {Dublin, Ireland},
series = {MM '25}
}

@inproceedings{10.1145/3746027.3758171,
author = {Hendrix, Rutger and Patan\`{e}, Giovanni and Russo, Leonardo G. and Carnemolla, Simone and Proietto Salanitri, Federica and Bellitto, Giovanni and Spampinato, Concetto and Pennisi, Matteo},
title = {Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning},
year = {2025},
isbn = {9798400720352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746027.3758171},
doi = {10.1145/3746027.3758171},
abstract = {Foundation models have transformed multimedia analysis by enabling robust and transferable representations across diverse modalities and tasks. However, their static deployment conflicts with growing societal and regulatory demands-particularly the need to unlearn specific data upon request, as mandated by privacy frameworks such as the GDPR. Traditional unlearning approaches, including retraining, activation editing, or distillation, are often computationally expensive, fragile, and ill-suited for real-time or continuously evolving systems. In this paper, we propose a paradigm shift: rethinking unlearning not as a retroactive intervention but as a built-in capability. We introduce a prompt-based learning framework that unifies knowledge acquisition and removal within a single training phase. Rather than encoding information in model weights, our approach binds class-level semantics to dedicated prompt tokens. This design enables instant unlearning simply by removing the corresponding prompt-without retraining, model modification, or access to original data. Experiments demonstrate that our framework preserves predictive performance on retained classes while effectively erasing forgotten ones. Beyond utility, our method exhibits strong privacy and security guarantees: it is resistant to membership inference attacks, and prompt removal prevents any residual knowledge extraction, even under adversarial conditions. This ensures compliance with data protection principles and safeguards against unauthorized access to forgotten information, making the framework suitable for deployment in sensitive and regulated environments. Overall, by embedding removability into the architecture itself, this work establishes a new foundation for designing modular, scalable and ethically responsive AI models.},
booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia},
pages = {12446–12454},
numpages = {9},
keywords = {machine unlearning, parameter-efficient finetuning},
location = {Dublin, Ireland},
series = {MM '25}
}

@inproceedings{10.1145/3746027.3755341,
author = {Peng, Qucheng and Bai, Chen and Zhang, Guoxiang and Xu, Bo and Liu, Xiaotong and Zheng, Xiaoyin and Chen, Chen and Lu, Cheng},
title = {NavigScene: Bridging Local Perception and Global Navigation for Beyond-Visual-Range Autonomous Driving},
year = {2025},
isbn = {9798400720352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746027.3755341},
doi = {10.1145/3746027.3755341},
abstract = {Autonomous driving systems have made significant advances in Q&amp;A, perception, prediction, and planning based on local visual information, yet they struggle to incorporate broader navigational context that human drivers routinely utilize. We address this critical gap between local sensor data and global navigation information by proposing NavigScene, an auxiliary navigation-guided natural language dataset that simulates a human-like driving environment within autonomous driving systems. Moreover, we develop three complementary paradigms to leverage NavigScene: (1) Navigation-guided Reasoning, which enhances vision-language models by incorporating navigation context into the prompting approach; (2) Navigation-guided Preference Optimization, a reinforcement learning method that extends Direct Preference Optimization to improve vision-language model responses by establishing preferences for navigation-relevant summarized information; and (3) Navigation-guided Vision-Language-Action model, which integrates navigation guidance and vision-language models with conventional driving models through feature fusion. Extensive experiments demonstrate that our approaches significantly improve performance across perception, prediction, planning, and question-answering tasks by enabling reasoning capabilities beyond visual range and improving generalization to diverse driving scenarios. This work represents a significant step toward more comprehensive autonomous driving systems capable of navigating complex, unfamiliar environments with greater reliability and safety.},
booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia},
pages = {4193–4202},
numpages = {10},
keywords = {autonomous driving, reinforcement learning, vision-language model},
location = {Dublin, Ireland},
series = {MM '25}
}

@inproceedings{10.1145/3746027.3755822,
author = {Lin, Hongyang and Shao, Kuixiang and Xu, Peijun and Bu, Zhuoyang and Jiao, Yuyang and Tang, Ziyuan and Xiao, Chenxi and Yu, Jingyi},
title = {HandCraft: Tactile-Informed Hand-Object Dynamics Capture and Realistic Rendering},
year = {2025},
isbn = {9798400720352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746027.3755822},
doi = {10.1145/3746027.3755822},
abstract = {Creating digitalized hand-object interaction scenes plays a crucial role in recent advancements, enabling viewers to understand how human dexterity influences and shapes the world. In this paper, we present HandCraft, a framework designed to capture and render hand-object interactions with exceptional precision and realism. Our Gaussian models are built on the development of digital representations of hands, objects, and scenes, derived from data captured using multi-modal sensing systems. By combining motion capture with IMU-based data gloves equipped with tactile sensors, HandCraft ensures precise hand pose tracking and reliable contact fidelity. HandCraft includes a novel method that uses hand motions to solve the object occlusions, effectively reconstructing missing interaction details. For enhanced physical feasibility, HandCraft incorporates optimization techniques to resolve object penetration issues and enforce temporal consistency. Using these techniques, we introduce a high-quality dataset of hand-object interaction sequences, featuring complex and prolonged daily activities. This dataset demonstrates HandCraft's ability to capture and reproduce subtle, dynamic interactions in rich detail. HandCraft holds promises in creating realistic virtual environments and advancing world modeling in both graphics and robotics research.},
booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia},
pages = {2274–2283},
numpages = {10},
keywords = {hand-object interaction capture, neural 3d representation},
location = {Dublin, Ireland},
series = {MM '25}
}

@inproceedings{10.1145/3728423.3759401,
author = {Hayashi, Ryunosuke and Torimi, Kohei and Nagata, Rokuto and Ikeda, Kazuma and Sako, Ozora and Nakamura, Taichi and Tani, Masaki and Aoki, Yoshimitsu and Yoshioka, Kentaro},
title = {BasketLiDAR: The First LiDAR-Camera Multimodal Dataset for Professional Basketball MOT},
year = {2025},
isbn = {9798400711985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3728423.3759401},
doi = {10.1145/3728423.3759401},
abstract = {Real-time 3D trajectory player tracking in sports plays a crucial role in tactical analysis, performance evaluation, and enhancing spectator experience. Traditional systems rely on multi-camera setups, but are constrained by the inherently two-dimensional nature of video data and the need for complex 3D reconstruction processing, making real-time analysis challenging. Basketball, in particular, represents one of the most difficult scenarios in the MOT field, as ten players move rapidly and complexly within a confined court space, with frequent occlusions caused by intense physical contact. To address these challenges, this paper constructs BasketLiDAR, the first multimodal dataset in the sports MOT field that combines LiDAR point clouds with synchronized multi-view camera footage in a professional basketball environment, and proposes a novel MOT framework that simultaneously achieves improved tracking accuracy and reduced computational cost. The BasketLiDAR dataset contains a total of 4,445 frames and 3,105 player IDs, with fully synchronized IDs between three LiDAR sensors and three multi-view cameras. We recorded 5-on-5 and 3-on-3 game data from actual professional basketball players, providing complete 3D positional information and ID annotations for each player. Based on this dataset, we developed a novel MOT algorithm that leverages LiDAR's high-precision 3D spatial information. The proposed method consists of a real-time tracking pipeline using LiDAR alone and a multimodal tracking pipeline that fuses LiDAR and camera data. Experimental results demonstrate that our approach achieves real-time operation, which was difficult with conventional camera-only methods, while achieving superior tracking performance even under occlusion conditions. The dataset is available upon request at: https://sites.google.com/keio.jp/keio-csg/projects/basket-lidar},
booktitle = {Proceedings of the 8th International ACM Workshop on Multimedia Content Analysis in Sports},
pages = {78–86},
numpages = {9},
keywords = {lidar, sensor-fusion, sports dataset, sports mot},
location = {Dublin, Ireland},
series = {MMSports '25}
}

@inproceedings{10.1145/3746027.3755730,
author = {Tang, Zhipeng and Zhang, Sha and Deng, Jiajun and Wang, Chenjie and You, Guoliang and Huang, Yuting and Lin, Xinrui and Zhang, Yanyong},
title = {VLMPlanner: Integrating Visual Language Models with Motion Planning},
year = {2025},
isbn = {9798400720352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746027.3755730},
doi = {10.1145/3746027.3755730},
abstract = {Integrating large language models (LLMs) into autonomous driving motion planning has recently emerged as a promising direction, offering enhanced interpretability, better controllability, and improved generalization in rare and long-tail scenarios. However, existing methods often rely on abstracted perception or map-based inputs, missing crucial visual context, such as fine-grained road cues, accident aftermath, or unexpected obstacles, which are essential for robust decision-making in complex driving environments. To bridge this gap, we propose VLMPlanner, a hybrid framework that combines a learning-based real-time planner with a vision-language model (VLM) capable of reasoning over raw images. The VLM processes multi-view images to capture rich, detailed visual information and leverages its common-sense reasoning capabilities to guide the real-time planner in generating robust and safe trajectories. Furthermore, we develop the Context-Adaptive Inference Gate (CAI-Gate) mechanism that enables the VLM to mimic human driving behavior by dynamically adjusting its inference frequency based on scene complexity, thereby achieving an optimal balance between planning performance and computational efficiency. We evaluate our approach on the large-scale, challenging nuPlan benchmark, with comprehensive experimental results demonstrating superior planning performance in scenarios with intricate road conditions and dynamic elements.},
booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia},
pages = {5040–5049},
numpages = {10},
keywords = {autonomous driving, motion planning, vlm},
location = {Dublin, Ireland},
series = {MM '25}
}

@inproceedings{10.1145/3746027.3755700,
author = {Yang, Woo Yi and Wang, Jiarui and Wu, Sijing and Duan, Huiyu and Zhu, Yuxin and Yang, Liu and Fu, Kang and Zhai, Guangtao and Min, Xiongkuo},
title = {LMME3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs},
year = {2025},
isbn = {9798400720352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746027.3755700},
doi = {10.1145/3746027.3755700},
abstract = {The rapid advancement in generative artificial intelligence have enabled the creation of 3D human faces (HFs) for applications including media production, virtual reality, security, healthcare, and game development, etc. However, assessing the quality and realism of these AI-generated 3D human faces remains a significant challenge due to the subjective nature of human perception and innate perceptual sensitivity to facial features. To this end, we conduct a comprehensive study on the quality assessment of AI-generated 3D human faces. We first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of AI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS) collected across two dimensions, i.e., quality and authenticity, 2,000 distortion-aware saliency maps and distortion descriptions. Based on Gen3DHF, we propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating 3DHF capable of quality and authenticity score prediction, distortion-aware visual question answering, and distortion-aware saliency prediction. Experimental results show that LMME3DHF achieves state-of-the-art performance, surpassing existing methods in both accurately predicting quality scores for AI-generated 3D human faces and effectively identifying distortion-aware salient regions and distortion types, while maintaining strong alignment with human perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be released upon the publication.},
booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia},
pages = {8825–8834},
numpages = {10},
keywords = {ai-generated 3d human faces, large multimodal model (lmm), quality assessment},
location = {Dublin, Ireland},
series = {MM '25}
}

@inproceedings{10.1145/3746027.3755853,
author = {Ma, Tengyu and Ruan, Jiafa and Wang, Yuetong and Han, Guangchao and Liu, Zhu and Ma, Long and Liu, Risheng},
title = {Degradation-Aware One-Step Diffusion Model for Content-Sensitive Super-Resolution in the Dark},
year = {2025},
isbn = {9798400720352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746027.3755853},
doi = {10.1145/3746027.3755853},
abstract = {Diffusion-based super-resolution methods have achieved impressive results under normal lighting conditions. However, their performance in low-light scenarios faces fundamental limitations due to two inherent challenges. First, the characteristic noise patterns and complex degradation features in severely underexposed images create significant obstacles for diffusion models to establish reliable noise prediction mechanisms. Second, these methods often fail to establish effective coupling between the degradation priors of low-light observations and the reconstruction process, resulting in compromised detail recovery and unrealistic texture synthesis.To address these limitations, we propose Degradation-aware Adaptation with Representation Embedding (DARE) method, a novel one-step diffusion framework specifically designed for super-resolution in dark environments. DARE employs a degradation-aware low-rank adaptation strategy that dynamically adjusts model parameters conditioned on degradation-specific features, effectively addressing compound degradations such as low-light, blur, and noise. Furthermore, we introduce a content-sensitive representation embedding mechanism, integrating complementary spatial and frequency domain priors through a bilinear cross-attention module. This module explicitly captures second-order statistical correlations, enriching semantic understanding and detail recovery during the denoising process. Extensive experiments across diverse low-light scenarios demonstrate that DARE outperforms state-of-the-art methods in terms of both visual quality and perceptual accuracy. The code is available at https://github.com/csmty/DARE.},
booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia},
pages = {9016–9025},
numpages = {10},
keywords = {content representation embedding, degradation-aware lora, low-light image super-resolution, one-step diffusion},
location = {Dublin, Ireland},
series = {MM '25}
}

@inproceedings{10.1145/3746259.3760432,
author = {Patan\`{e}, Giovanni and Sorrenti, Amelia and Bellitto, Giovanni and Palazzo, Simone},
title = {Continual Learning Strategies for Personalized Mental Well-being Monitoring from Mobile Sensing Data},
year = {2025},
isbn = {9798400720413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746259.3760432},
doi = {10.1145/3746259.3760432},
abstract = {Automatic prediction of psychological stress using mobile sensing data is critical for developing personalized mental health monitoring tools. Traditional approaches typically train static models offline, limiting their applicability in real-world scenarios characterized by evolving user behaviors and data streams. To address these challenges, we formulate stress prediction as a continual learning (CL) regression task, where each user constitutes a distinct sequential task. Within this framework, we evaluate two adaptation strategies: (1) a replay-based approach, leveraging recurrent neural networks combined with an episodic memory buffer; and (2) a prompt-based method, where task-specific prompts are jointly trained with a transformer backbone and combined with the memory buffer to enable task-aware adaptation across users. Experiments conducted on a filtered subset of the StudentLife dataset, comprising multimodal behavioral and contextual data, demonstrate that prompt-based methods achieve competitive predictive performance. Our results underline the potential of continual learning paradigms to enable scalable and adaptive stress prediction systems.},
booktitle = {Proceedings of the International Workshop on Personalized Incremental Learning in Medicine},
pages = {9–17},
numpages = {9},
keywords = {stress prediction, continual learning, mobile sensing, prompt tuning},
location = {Ireland},
series = {PILM '25}
}

@inproceedings{10.1145/3728487.3759418,
author = {Anwar, Muhammad Shahid and Ali, Miram and Elwardy, Majed and Ahmad, Shabir and Ahmad, Pir Noman and Ahmad, Mubashir},
title = {Gamified Immersive Learning and User Experience Evaluation of Metaverse-Based Cultural Heritage Education},
year = {2025},
isbn = {9798400718441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3728487.3759418},
doi = {10.1145/3728487.3759418},
abstract = {The growing intersection of immersive technologies and cultural heritage presents an unprecedented opportunity to transform users' engagement with history and art. This research is motivated by the need to improve cultural experiences through the virtual recreation of historical sites. It employs a multi-user virtual reality (VR) environment with gamified elements to create more engaging, interactive, and educational platforms within the metaverse. This work focuses on Saudi Arabia's World Heritage Site to develop a virtual replica of Jabal Al-Ahmar using image processing and 3D modeling to ensure accuracy and authenticity in reconstructing its architectural and cultural elements. Through virtual recreation, users engage with these sites by unlocking interactive features and completing tasks that promote active learning and engagement with historical narratives. A subjective test with 46 participants compared experiences in both VR and non-VR conditions. User experience (UX) and sense of presence were evaluated using the User Experience Questionnaire (UEQ) and the Igroup Presence Questionnaire (IPQ). UEQ results showed that VR significantly outperformed non-VR in Stimulation and Novelty, while non-VR achieved higher Perspicuity scores. IPQ results revealed that VR participants reported a stronger sense of presence, higher involvement, and greater perceived realism compared to non-VR participants. Overall, the study demonstrates the potential of metaverse-based cultural heritage applications to combine education, preservation, and user-centered design for more impactful cultural experiences.},
booktitle = {Proceedings of the International Workshop on Intelligent Immersification in the Metaverse: AI-Driven Immersive Multimedia},
pages = {44–52},
numpages = {9},
keywords = {metaverse, immersive learning, education, cultural heritage, gamification, user experience (ux), multi-user, virtual reality (vr)},
location = {Ireland},
series = {I2M-MM '25}
}

@article{10.1145/3768580,
author = {Yuan, Wu and Yu, Hengyu and Ding, Lei and Hu, Xinrong and Zhang, Jian and Chen, Yanjiao and Zhang, Qian},
title = {MI-Ra: Towards Motion-robust Myocardial Infarction Detection Using Deep Wireless Sensing},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {6},
issn = {1550-4859},
url = {https://doi.org/10.1145/3768580},
doi = {10.1145/3768580},
abstract = {Early detection of myocardial infarction (MI) is essential for alleviating symptoms and improving daily activity performance. Researchers typically employ continuous segments of heartbeat signals (20–30 seconds), such as ECG signals, for MI detection, as MI often induces changes in heartbeat patterns. Current MI detection methods, like wearable sensors, may induce discomfort from prolonged wear, and Radio Frequency (RF) based approaches might fail to extract fine-grained heartbeat signals during vigorous movement. This article presents a reliable and motion-robust MI detection method based on RF signals. By developing a series of advanced signal processing algorithms, MI-Ra can capture fine-grained heartbeat signals during various daily activities. Our design is inspired by the fact that RF reflections caused by heartbeat signals are mixed with other motion-induced reflections in a nonlinear manner. We utilize the Taylor series expansion method to extract the linear component of these mixed non-linear signals and propose a novel Generative Adversarial Networks (GAN) method, named IQ-TransGAN, to separate the heartbeat signal. To enhance MI detection reliability, MI-Ra employs a multi-periodicity modeling method to extract refined signal representations from recovered heartbeat signals. We have recruited 50 volunteers with MI from Zhongnan Hospital of Wuhan, China, and 50 volunteers without MI, for comprehensive evaluations. The results demonstrate that MI-Ra achieves an average MI detection accuracy of 95.2\% when user is quasi-stationary. Even during user non-stationary conditions, MI-Ra maintains an average detection accuracy of 90.5\%. MI-Ra shows promise in paving way for smart home healthcare.},
journal = {ACM Trans. Sen. Netw.},
month = oct,
articleno = {54},
numpages = {25},
keywords = {Wireless sensing, motion-robust, myocardial infarction, heartbeat recovery}
}

@inproceedings{10.1145/3704413.3764445,
author = {Wang, Xun and Li, Zhuoran and Huang, Longbo},
title = {Beyond Static Populations: Efficient Delay-Constrained Scheduling for Dynamic Users via Deep Reinforcement Learning},
year = {2025},
isbn = {9798400713538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704413.3764445},
doi = {10.1145/3704413.3764445},
abstract = {Multi-user delay-constrained scheduling is a critical challenge in real-world applications such as embodied AI and modern network systems, where efficient resource allocation is required among users with diverse delay sensitivities. While deep reinforcement learning (DRL) has shown superior performance over traditional methods in complex environments, existing approaches typically assume a static user population. This limits their scalability, as user population changes necessitate costly retraining. To address this limitation, we propose Heterogeneous Embedding Multi-Agent Reinforcement Learning (HEMA), a novel multi-agent DRL algorithm that maps variable-shaped user features into a shared embedding space and employs a parameter-shared recurrent neural network to compute individual value functions. Following the centralized training with decentralized execution paradigm and a carefully designed online adaptation mechanism, HEMA efficiently adapts to dynamic user populations, adjusts its allocation strategy on the fly, and achieves high throughput under resource constraints. Extensive experiments show that HEMA outperforms the state-of-the-art DRL baseline by up to 32.1\% in average throughput under static population settings, and achieves up to 31.8\% higher steady-state throughput than traditional methods in dynamic environments, demonstrating strong effectiveness and better generalizability.},
booktitle = {Proceedings of the Twenty-Sixth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {151–160},
numpages = {10},
keywords = {delay-constrained scheduling, dynamic users, deep reinforcement learning, multi-agent},
location = {Rice University, Houston, TX, USA},
series = {MobiHoc '25}
}

@inproceedings{10.1145/3663547.3746383,
author = {Gamage, Bhanuka and McDowell, Nicola and Kovacic, Dijana and Holloway, Leona and Do, Thanh-Toan and Lowery, Arthur James and Price, Nicholas and Marriott, Kim},
title = {Smart Glasses for CVI: Co-Designing Extended Reality Solutions to Support Environmental Perception by People with Cerebral Visual Impairment},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3746383},
doi = {10.1145/3663547.3746383},
abstract = {Cerebral Visual Impairment (CVI) is the set to be the leading cause of vision impairment, yet remains underrepresented in assistive technology research. Unlike ocular conditions, CVI affects higher-order visual processing—impacting object recognition, facial perception, and attention in complex environments. This paper presents a co-design study with two adults with CVI investigating how smart glasses, i.e. head-mounted extended reality displays, can support understanding and interaction with the immediate environment. Guided by the Double Diamond design framework, we conducted a two-week diary study, two ideation workshops, and ten iterative development sessions using the Apple Vision Pro. Our findings demonstrate that smart glasses can meaningfully address key challenges in locating objects, reading text, recognising people, engaging in conversations, and managing sensory stress. With the rapid advancement of smart glasses and increasing recognition of CVI as a distinct form of vision impairment, this research addresses a timely and under-explored intersection of technology and need.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {2},
numpages = {24},
keywords = {cerebral visual impairment, assistive technology, co-design, double diamond, extended reality, augmented reality, apple vision pro},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3760544.3764132,
author = {Cavigliano, Chiara and Mura, Silvia and Vizziello, Anna and Savazzi, Pietro and Magarini, Maurizio},
title = {Galvanic Coupling Channel Characterization for Wearable Devices},
year = {2025},
isbn = {9798400721663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3760544.3764132},
doi = {10.1145/3760544.3764132},
abstract = {Galvanic Coupling (GC) Intra-Body Communication offers a promising solution for reliable, low-power data transmission in wearable medical devices. However, signal performance is highly sensitive to electrode material and skin-electrode interface properties. This study uses finite element modeling (FEM) to quantify the effects of electrode design, including material choice, conductive gel, and foam layers, on the GC channel frequency response (CFR), focusing on attenuation, phase delay, and group delay. Results show that incorporating gel and foam significantly enhances signal transmission by reducing attenuation, minimizing phase distortion, and stabilizing group delay across a broad frequency range. These improvements help mitigate performance disparities between Ag/AgCl and copper (Cu) electrodes, supporting the development of high-performance, energy-efficient intra-body networks for wearable healthcare applications.},
booktitle = {Proceedings of the 12th Annual ACM International Conference on Nanoscale Computing and Communication},
pages = {46–51},
numpages = {6},
keywords = {intrabody communications, galvanic coupling, COMSOL},
location = {University of Electronic Science and Technology of China, Chengdu, China},
series = {NANOCOM '25}
}

@inproceedings{10.1145/3760544.3764138,
author = {Lotter, Sebastian and Mohr, Elisabeth and Rutsch, Andrina and Brand, Lukas and Ronchi, Francesca and D\'{\i}az-Marug\'{a}n, Laura},
title = {Synthetic MC via Biological Transmitters: Therapeutic Modulation of the Gut-Brain Axis},
year = {2025},
isbn = {9798400721663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3760544.3764138},
doi = {10.1145/3760544.3764138},
abstract = {Synthetic molecular communication (SMC) is a key enabler for future healthcare systems in which Internet of Bio-Nano-Things (IoBNT) devices facilitate the continuous monitoring of a patient's biochemical signals. To close the loop between sensing and actuation in these systems, both the detection and the generation of in-body molecular communication (MC) signals is key. However, generating signals inside the human body, e.g., via synthetic nanodevices, still poses a major research challenge in SMC, due to technological obstacles as well as legal, safety, and ethical issues. In contrast to many existing studies, this paper considers an SMC system in which signals are generated indirectly via the modulation of a natural in-body MC system, namely the gut-brain axis (GBA). Therapeutic GBA modulation is already established as treatment for some neurological diseases, e.g., drug refractory epilepsy (DRE), and performed via the administration of nutritional supplements or specific diets with therapeutic effect. However, the molecular signaling pathways that mediate the effect of such treatments are mostly unknown. Consequently, existing treatments are standardized or designed heuristically and able to help only some patients while failing to help others. In this paper, we propose to leverage personal health data, e.g., data gathered by in-body IoBNT devices, to overcome this research gap and design more versatile and robust GBA modulation-based treatments as compared to the existing ones. To show the feasibility of our approach, we first define a catalog of theoretical requirements for therapeutic GBA modulation. Then, we propose a machine learning model to verify these requirements for practical scenarios when only limited data on the GBA modulation is available. By evaluating the proposed model on several published datasets, we confirm its excellent accuracy in identifying different modulators of the GBA. Finally, we utilize the proposed model to identify specific modulatory pathways that play an important role for therapeutic GBA modulation. The results presented in this paper may help to develop novel personalized GBA-based treatments, i.e., novel nutritional supplements and/or diets, to help patients that do not respond to existing standardized treatments.},
booktitle = {Proceedings of the 12th Annual ACM International Conference on Nanoscale Computing and Communication},
pages = {84–90},
numpages = {7},
keywords = {synthetic molecular communication, personalized healthcare, gut-brain axis, random forest, machine learning},
location = {University of Electronic Science and Technology of China, Chengdu, China},
series = {NANOCOM '25}
}

@inproceedings{10.1145/3663547.3746364,
author = {Kneitmix, Melanie Jo and Wobbrock, Jacob O.},
title = {From Screen Reading to “Scene Reading” in SceneVR: Touch-Based Interaction Techniques for Use in Virtual Reality by Blind and Low-Vision Users},
year = {2025},
isbn = {9798400706769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663547.3746364},
doi = {10.1145/3663547.3746364},
abstract = {To improve the accessibility of virtual reality (VR) for blind and low-vision (BLV) users, we introduce “scene reading,” a technique inspired by touch-based screen reading for use in virtual environments. Scene reading provides semantic information about virtual objects and their on-screen positions, organizing details into hierarchies that users can navigate for more granular exploration; it also uses spatial audio for nonvisual feedback. To design and evaluate our scene reading technique, we developed a system called SceneVR, which supports touch and gesture input, and spatial audio output. SceneVR streams the live view from a VR headset onto a phone or tablet, letting BLV users drag their finger across the touchscreen to identify objects and avatars, navigate, and gain a spatial understanding of the scene. We conducted a task-based usability study to evaluate our SceneVR controller, collecting data on task performance, user experience, interaction patterns, and subjective feedback. Our findings indicate that scene reading with the SceneVR controller effectively supports BLV users in exploring virtual environments, enabling them to discover objects, navigate object hierarchies, and build an understanding of their surroundings while also providing a sense of enjoyment and agency. However, our findings also reveal initial design implications, including minimizing cognitive load and effectively integrating scene reading labels and descriptions with other sensory feedback to create a cohesive experience.},
booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {3},
numpages = {18},
keywords = {Accessibility, blind and low-vision, virtual reality (VR), touchscreen-based interfaces, spatial audio.},
location = {
},
series = {ASSETS '25}
}

@inproceedings{10.1145/3760544.3764143,
author = {Yang, Qingqing and Chen, Zan and He, Peng and Sun, Yue and Chen, Yifan},
title = {Oxygen Gradient Simulation in Tumor Microenvironments: A Bio-realistic Model for Computational Nanobiosensing},
year = {2025},
isbn = {9798400721663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3760544.3764143},
doi = {10.1145/3760544.3764143},
abstract = {Cancer remains a leading global health challenge, with early detection being crucial to improving patient outcomes. The tumor microenvironment (TME), characterized by abnormal oxygen and pH distributions, plays a vital role in tumor progression and treatment response. Recent advances in nanotechnology have enabled the development of micro/nanorobots capable of real-time sensing and targeted delivery within the TME. However, the effectiveness of such systems depends on the accurate modeling of biological gradient fields (BGFs). In this study, we propose a simulation framework grounded in Computational Nanobiosensing (CONA) to integrate vascular morphology with oxygen gradient field analysis for a more comprehensive understanding of the TME. Using COMSOL Multiphysics, we constructed a two-dimensional model that couples laminar flow and mass transport to investigate oxygen distribution under conditions in the presence and absence of tumors. The simulation results reveal that tumors significantly alter local oxygen profiles, leading to steep concentration gradients and hypoxic regions. These findings provide both visual and quantitative insight into the TME and establish a theoretical foundation for designing oxygen gradient-guided nanoparticle delivery strategies. This work contributes to the advancement of intelligent navigation and precision therapy in complex tumor environments.},
booktitle = {Proceedings of the 12th Annual ACM International Conference on Nanoscale Computing and Communication},
pages = {111–115},
numpages = {5},
keywords = {tumor microenvironment (TME), biological gradient fields (BGFS), computational nanobiosensing (CONA)},
location = {University of Electronic Science and Technology of China, Chengdu, China},
series = {NANOCOM '25}
}

@article{10.1145/3767332,
author = {Li, Chunlin and Zhang, Shuai and Wu, Yaojuan and Jiang, Kun and Wu, Wenhao and Yuan, Shaochong and Wan, Shaohua},
title = {Improved TD3 Based Resource Allocation Optimization for Latency-sensitive Tasks in ISAC-aided VEC},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {6},
issn = {1550-4859},
url = {https://doi.org/10.1145/3767332},
doi = {10.1145/3767332},
abstract = {Vehicular edge computing (VEC) has emerged to address the increasing demands on wireless networks posed by massive data and diverse applications in intelligent vehicular services. However, challenges such as low spectrum utilization due to massive sensor deployment, signal degradation from high-speed mobility, and computational resource allocation issues hinder the real-time and secure operation of intelligent vehicles. Therefore, we propose a resource allocation optimization method for VEC based on Integrated Sensing and Communication (ISAC) and Orthogonal Time Frequency Space (OTFS) technologies. Specifically, OTFS is leveraged to multiplex roadside unit (RSU) radar resources, improving spectrum efficiency. We develop comprehensive models for communication, vehicle mobility, sensing, delay, energy consumption, and formulate a delay-minimization resource allocation problem. The problem is modeled as a Markov Decision Process and solved with an improved Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm, which incorporates prioritized experience sampling and dynamic parameter update to accelerate training and enhance agent-environment interaction. Extensive simulations are conducted in a VEC environment, where the proposed algorithm is compared with DDQN, MADDPG, and MRL-DDPG. The results demonstrate that our method effectively mitigates the impact of vehicle mobility on signal transmission and significantly reduces task completion delay compared with existing algorithms.},
journal = {ACM Trans. Sen. Netw.},
month = oct,
articleno = {52},
numpages = {27},
keywords = {Vehicular edge computing, resource allocation, integrated sensing and communication, twin delayed deep deterministic policy gradient (TD3)}
}

@inproceedings{10.1145/3725843.3756097,
author = {Kalyanapu, Joshua and Dizani, Farshad and Asher, Darsh and Ghanbari, Azam and Cammarota, Rosario and Aysu, Aydin and Ajorpaz, Samira Mirbagher},
title = {GateBleed: Exploiting On-Core Accelerator Power Gating for High Performance and Stealthy Attacks on AI},
year = {2025},
isbn = {9798400715730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3725843.3756097},
doi = {10.1145/3725843.3756097},
abstract = {As power consumption from AI training and inference continues to increase, AI accelerators are being integrated directly into the CPU. Intel’s Advanced Matrix Extensions (AMX) is one such example, debuting in the 4th Generation Intel Xeon Scalable CPU, attaining significant gains in the metrics of performance/watt and decreased memory offloading penalty. This paper discloses a timing side and covert channel, GateBleed, caused by the aggressive power gating utilized to keep the CPU within operating limits. This paper shows that the GateBleed side channel is a threat to AI privacy, as many ML models such as Transformers and CNNs make critical computationally-heavy decisions based on private values like confidence thresholds and routing logits. Timing delays from the selective powering down of AMX components mean that each matrix multiplication is a potential leakage point when executed on the AMX accelerator. This paper identifies over a dozen potential gadgets across popular ML libraries (Hugging Face, PyTorch, TensorFlow, etc.), revealing that they can leak sensitive and private information, including class labels and internal states. GateBleed poses a risk for local and remote timing inference, even under previous protective measures. GateBleed gadgets can also be used as a a generic high performance, stealthy magnifier for microarchitectural attacks to bypass timer resolution coarsening defenses and create robust and realistic side channels in noisy environments, such as remote attacks on networks with high traffic. This paper shows that when GateBleed gadget is used as a transmission channel for Spectre, it can leak arbitrary memory addresses of the victim with high performance (0.067 bps), and evade the state-of-the-art microarchitectural attack detectors for the first time.This paper implements an end-to-end membership inference attack with 81\% accuracy on a Transformer model optimized with Intel AMX and 99\% accuracy on an early-exit CNN classifier. GateBleed achieves 0.89 precision while leaking expert choice in a Transformer mixture-of-experts (MoE) with 100\% accuracy. These attacks do not rely on confidence scores or model outputs, but only on the execution time of attacker-controlled AMX instructions on the shared hardware accelerator with power gating. To the authors’ knowledge, this is the first side-channel attack on AI privacy that exploits hardware accelerator power optimizations. The paper also suggests effective mitigations and measures their trade-off between power consumption and performance.},
booktitle = {Proceedings of the 58th IEEE/ACM International Symposium on Microarchitecture},
pages = {308–325},
numpages = {18},
location = {
},
series = {MICRO '25}
}

@article{10.1145/3757533,
author = {Lee, Chanhee and Joung, Eunki and Koh, Youngji and Kim, Esther and Son, Sohwi and Kwon, Sunjung and Lee, Uichin},
title = { 'In That Small Space with Just the Two of Us': User Experiences with Cumpa in a Robotic Counseling Center},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {7},
url = {https://doi.org/10.1145/3757533},
doi = {10.1145/3757533},
abstract = {The growing demand for mental health support has highlighted the limitations of traditional counseling accessibility, increasing the usage of digital mental health interventions. There has been a rising interest in using robots to support mental health due to their benefits in engagement and rapport. Capitalizing on the opportunity of placemaking for designing a feasible robotic digital mental health intervention, our study explores the Robot Counseling Center (RCC) and its robotic counselor, Cumpa, designed to improve mental health accessibility and user engagement. A two-week field study with 20 participants evaluated RCC's impact on their mental health, engagement, and sense of place within a counseling environment. Results indicate that RCC positively influences emotional awareness and engagement. Our findings provide insights into the role of social robots in mental health interventions and offer design implications for developing robotic counseling centers as supportive, effective spaces, contributing to building better places and interactive systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {CSCW352},
numpages = {35},
keywords = {digital mental health intervention, human-robot interaction, social robot}
}

@article{10.1145/3757434,
author = {Kim, Heejun and Choi, Bogeum and Daly, Brian and Huh-Yoo, Jina},
title = {Understanding Community 'Likes' and Clinical Perspective in Mental Health Discourse: Insights from YouTube Comments on College Students' Mental Health},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {7},
url = {https://doi.org/10.1145/3757434},
doi = {10.1145/3757434},
abstract = {Anonymized, informal environments, such as social media, provide opportunities for individuals to naturally exchange information and receive or provide social support for stigmatized conditions, such as mental health. The sensitive nature of the content shared on these platforms requires automated moderation, which is often based on keyword detection. However, what is considered as support versus risk in these contexts can be controversial and more situated. To investigate how we might better define what is supportive versus harmful content, we examined 49,006 YouTube comments on videos about college students' mental health, using statistical tests and qualitative content analysis with a clinical psychologist. We studied (1) the association between community 'Likes' and both self-disclosure and related linguistic features and (2) when comments exhibiting features associated with community 'Likes' reflect perceived support versus harm in the context of mental health. We discuss the situatedness of how community-generated comments containing self-disclosure can be either supportive or potentially harmful from a clinical perspective. This work highlights the need for a paradigmatic change in developing automated moderation rules and assumptions toward social media environments for supporting mental health.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {CSCW253},
numpages = {26},
keywords = {CSCW, college students, emerging adults, engagement, harmful, mental health, online communities, self disclosure, social media, supportive, young adults}
}

@article{10.1145/3757555,
author = {Shi, Jiayue Melissa and Wang, Keran and Yoo, Dong Whi and Karkar, Ravi and Saha, Koustuv},
title = {Balancing Caregiving and Self-Care: Exploring Mental Health Needs of Alzheimer's and Dementia Caregivers},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {7},
url = {https://doi.org/10.1145/3757555},
doi = {10.1145/3757555},
abstract = {Alzheimer's Disease and Related Dementias (AD/ADRD) are progressive neurodegenerative conditions that impair memory, thought processes, and functioning. Family caregivers of individuals with AD/ADRD face significant mental health challenges due to long-term caregiving responsibilities. Yet, current support systems often overlook the evolving nature of their mental wellbeing needs. Our study examines caregivers' mental wellbeing concerns, focusing on the practices they adopt to manage the burden of caregiving and the technologies they use for support. Through semi-structured interviews with 25 family caregivers of individuals with AD/ADRD, we identified the key causes and effects of mental health challenges and developed a temporal mapping of how caregivers' mental wellbeing evolves across three distinct stages of the caregiving journey. Additionally, our participants shared insights into improvements for existing mental health technologies, emphasizing the need for accessible, scalable, and personalized solutions that adapt to caregivers' changing needs over time. These findings offer a foundation for designing dynamic, stage-sensitive interventions that holistically support caregivers' mental wellbeing, benefiting both caregivers and care recipients.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {CSCW374},
numpages = {36},
keywords = {caregiving, dementia, mental health, personal informatics, wellbeing}
}

@article{10.1145/3757614,
author = {Park, Sunyup and Nellore, Nidhi and Zimmer, Michael and Vitak, Jessica},
title = {In Search of "a Way to Level the Playing Field": Helping Incidental Users Navigate Privacy Risks in Smart Environments},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {7},
url = {https://doi.org/10.1145/3757614},
doi = {10.1145/3757614},
abstract = {Smart devices--including speakers, cameras, TVs, and sensors--are increasingly used to enhance comfort, efficiency, security, and entertainment in and outside the home. These devices collect significant audio, video, and environmental data, and that data may be shared with device owners, manufacturers, and/or third-parties. Social computing researchers have devoted significant attention to the privacy risks posed by these devices, although the focus has largely been on primary users who have access and control of devices and data. More recently, researchers have begun examining the privacy risks to incidental users--those who do not own or manage devices but still have their data collected due to their presence in a smart environment. In this paper, we present findings from 25 interviews with US-based adults who have encountered and/or used smart devices as incidental users in rental properties, as part of their work, at friends' and family's houses, and in shared living environments. We explore their reactions to these devices across different contexts, as well as their expectations for how devices should be disclosed and managed by primary users. We found a prevailing sense of resignation across our participants, as well behavioral and environmental modifications in response to smart devices. At the same time, participants were disinterested in gaining access and control to other people's devices, and they were hesitant or uncomfortable with the idea of initiating communication with device owners regarding their concern. Many wanted clearer and more prominent disclosures about device and data management that shifted responsibility from the incidental user to the device owner. Based on our findings, we argue for researchers to focus more closely on sociotechnical approaches to help incidental users navigate privacy risks in smart environments. Such approaches must account for interpersonal factors like trust, power dynamics, and social norms in addition to the many technical solutions that have been introduced in recent years.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {CSCW433},
numpages = {28},
keywords = {bystanders, incidental users, mitigation strategies, non-primary users, privacy, privacy negotiation, privacy resignation, smart home}
}

@article{10.1145/3757515,
author = {Robledo Yamamoto, Fujiko and Voida, Amy and Voida, Stephen},
title = {Towards Culturally Competent Design: A Mulitistakeholder Study of the Adoption and Use of Teletherapy within the Hispanic Community},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {7},
url = {https://doi.org/10.1145/3757515},
doi = {10.1145/3757515},
abstract = {There has been a call to develop more culturally sensitive designs for mental health technologies to ensure that the needs of diverse communities are met. We conducted an interview study with multiple stakeholders-clients from the Hispanic community (N=10), mental health therapists (N=10), and technology designers (N=5)-to identify benefits and challenges of accessing and using teletherapy for the Hispanic community. We identified four use and access barriers: connectivity and digital literacy barriers, safety barriers, technology exploitation barriers, and cultural and language barriers. We then discuss how these barriers could be addressed by using five elements derived from culturally competent frameworks from the National Association of Social Workers (NASW) and the American Psychological Association (APA). We offer design guidelines for developing culturally sensitive technologies based on each of the five elements of culturally competent mental health technologies.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {CSCW334},
numpages = {30},
keywords = {cultural competence, equity, mental health, online therapy, teletherapy}
}

@article{10.1145/3757396,
author = {Xu, Chunchen and Ge, Xiao},
title = {Reimagining Digital Well-being: A Theoretical Framework Based on the Psychology of Felt Structure and Illustrated through Creative Storytelling},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {7},
url = {https://doi.org/10.1145/3757396},
doi = {10.1145/3757396},
abstract = {Digital environments today often lack affordances that cultivate a sense of place and embodied being and acting. We develop a theoretical framework for embodied place-making based on people's psychological tendency to balance structure (clarity, predictability, and certainty) with unstructuredness (openness and possibility). This framework provides a unified theoretical foundation for examining continuity and discontinuity in social psychological processes and behaviors across digital, physical, and natural environments. We illustrate this framework through an original story (NewsWood ) that reconceptualizes digital news reading as an experience akin to wandering through woods. We hope to inspire radical reimagining of how future digital environments can foster well-being across individuals, communities, societies, and the natural world within their interconnected relationships.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {CSCW215},
numpages = {19},
keywords = {behavior, behavioral sciences, creative storytelling, digital and natural environments, embodied place-making, human-nature connection, meaning-making, psychology, social media design, well-being}
}

@inproceedings{10.1145/3750069.3750114,
author = {Deshmukh, Raghavendra},
title = {Toward Neurodivergent-Aware Productivity: A Systems andAI-Based Human-in-the-Loop Framework for ADHD-AffectedProfessionals},
year = {2025},
isbn = {9798400721021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3750069.3750114},
doi = {10.1145/3750069.3750114},
abstract = {Digital work environments in IT and knowledge-based sectors demand high levels of attention management, task juggling and self-regulation. For adults with Attention-Deficit/Hyperactivity Disorder (ADHD), these settings often amplify existing challenges such as time blindness, digital distraction, emotional reactivity and executive dysfunction. Such individuals prefer a low-touch, easy-to-use interventions to help them in their day-to-day activities. Conventional productivity tools fall short of supporting the cognitive variability and overload patterns experienced by neurodivergent professionals. This paper introduces a comprehensive framework that blends Systems Thinking, Human-in-the-Loop with Artificial Intelligence (AI), Machine Learning (ML) and privacy-first adaptive agents to support ADHD-affected users in managing digital work. At the heart of the solution is a voice-enabled productivity assistant that senses user behavior—tab usage, application focus, inactivity windows—using lightweight, on-device machine learning. These behavioral cues are analyzed in real-time to infer attention states and deliver adaptive nudges, reflective queries or accountability-based presence (body doubling) designed to co-regulate cognition without disruption. While technically grounded in AI/ML, the design is deeply influenced by Systems Thinking, viewing user attention as a product of dynamic feedback loops. This hybrid approach bridges behavioral sensing with cognitive inclusivity, offering a replicable model for adaptive, neurodivergent first decision support tools in high-distraction work environments.},
booktitle = {Proceedings of the 16th Biannual Conference of the Italian SIGCHI Chapter},
articleno = {5},
numpages = {6},
keywords = {Neurodivergence, ADHD, Artificial Intelligence, Machine Learning, AI Assistant, Body Double, Accountability Partner, Privacy First},
location = {
},
series = {CHItaly '25}
}

