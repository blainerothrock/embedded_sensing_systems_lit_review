@inproceedings{10.1145/3769102.3770619,
author = {Shastri, Hetvi and Hanafy, Walid and Wu, Li and Irwin, David and Srivastava, Mani and Shenoy, Prashant},
title = {LLM-Driven Auto Configuration for Transient IoT Device Collaboration},
year = {2025},
isbn = {9798400722387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3769102.3770619},
doi = {10.1145/3769102.3770619},
abstract = {Today's Internet of Things (IoT) has evolved from simple sensing and actuation devices to those with embedded processing and intelligent services, enabling rich collaborations between users and their devices. However, enabling such collaboration becomes challenging when transient devices need to interact with host devices in temporarily visited environments. In such cases, fine-grained access control policies are necessary to ensure secure interactions; however, manually implementing them is often impractical for non-expert users. Moreover, at run-time, the system must automatically configure the devices and enforce such fine-grained access control rules. Additionally, the system must address the heterogeneity of devices.In this paper, we present CollabIoT, a system that enables secure and seamless device collaboration in transient IoT environments. CollabIoT employs a Large language Model (LLM)-driven approach to convert users' high-level intents to fine-grained access control policies. To support secure and seamless device collaboration, CollabIoT adopts capability-based access control for authorization and uses lightweight proxies for policy enforcement, providing hardware-independent abstractions.We implement a prototype of CollabIoT's policy generation and auto configuration pipelines and evaluate its efficacy on an IoT testbed and in large-scale emulated environments. We show that our LLM-based policy generation pipeline is able to generate functional and correct policies with 100\% accuracy. At runtime, our evaluation shows that our system configures new devices in ~150 ms, and our proxy-based data plane incurs network overheads of up to 2 ms and access control overheads up to 0.3 ms.},
booktitle = {Proceedings of the Tenth ACM/IEEE Symposium on Edge Computing},
articleno = {7},
numpages = {17},
keywords = {IoT, LLMs, access control, device collaboration},
location = {the Hilton Arlington National Landing, Arlington, VA, USA},
series = {SEC '25}
}

@article{10.1145/3777452,
author = {Ono, Taiga and Sugawara, Takeshi and Sakuma, Jun and Mori, Tatsuya},
title = {Adversarial Beats: Feasibility Study of Spoofed Arrhythmia in Automated Electrocardiogram Diagnosis},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2378-962X},
url = {https://doi.org/10.1145/3777452},
doi = {10.1145/3777452},
abstract = {This study aims to assess the feasibility of applying adversarial examples to attack cardiac diagnosis systems powered by machine learning algorithms. To achieve this, we introduce “adversarial beats,” which are adversarial perturbations that are tailored specifically against classification systems designed to diagnose electrocardiograms (ECGs). We first formulated an algorithm to generate adversarial examples for multiple neural network models for ECG classification and studied their attack success rates. Next, to evaluate their feasibility in a physical environment, we mounted a hardware attack by designing a malicious signal generator that injects adversarial beats into ECG sensor readings using commercial off-the-shelf hardware. To the best of our knowledge, our research is the first to evaluate the proficiency of adversarial examples for ECGs in a physical setup. Our real-world experiments demonstrate that, against an automated ECG diagnosis apparatus, our attack method can fake the presence of potential signs of cardiomyopathy with approximately 42.1\% chance of success and the attacker can repeat the attack until a fraudulent insurance claim or other health care fraud is established. Based on the comprehensive feasibility study of attacks using adversarial beats, we conclude that the attacks have a sufficient chance to succeed such that an attacker may be incentivized to fake the presence of cardiomyopathy, potentially leading to unnecessary medication prescriptions and fraudulent medical insurance claims.},
note = {Just Accepted},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = dec,
keywords = {adversarial examples, hardware, machine learning, ECG}
}

@inproceedings{10.1145/3769102.3774436,
author = {Farhady Ghalaty, Nahid and Bhargav-Spantzel, Abhilasha},
title = {RAI Overblock Dataset (ROD): A GAN-Based Synthetic Data Generation Approach for Evaluating Overblocking in Responsible AI Systems},
year = {2025},
isbn = {9798400722387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3769102.3774436},
doi = {10.1145/3769102.3774436},
abstract = {Recent deployments of Responsible AI (RAI) systems in high-stakes domains like healthcare and law enforcement have raised concerns about overblocking, where valid inputs are wrongly flagged due to varying levels of RAI checks required for different scenarios. This paper introduces RAI Overblock Dataset (ROD), a novel framework that leverages Generative Adversarial Networks (GANs) to generate synthetic datasets for quantifying overblocking in RAI systems before deployment. We leverage GANs to create realistic data points for evaluating blocking rates of an RAI system before its deployment. Unlike existing methods that rely on real user data and require noise injection or complex privacy-preserving techniques, our approach does not require access to sensitive real user data when evaluating the overblocking issue in RAI systems. This helps better protect user privacy during the RAI system development. By applying fairness metrics across sensitive attributes—such as patient demographics or law enforcement contexts—our approach enables a systematic analysis of RAI decision boundaries to identify and mitigate unintended biases. Our findings highlight how GAN-generated data can be effectively used to improve fairness and accuracy in AI moderation systems, particularly in environments where minimizing bias and ensuring reliability are critical.},
booktitle = {Proceedings of the Tenth ACM/IEEE Symposium on Edge Computing},
articleno = {80},
numpages = {5},
keywords = {privacy preservation, responsible AI (RAI), AI bias, synthetic data generation, equitable AI deployment},
location = {the Hilton Arlington National Landing, Arlington, VA, USA},
series = {SEC '25}
}

@inproceedings{10.1145/3769102.3774894,
author = {Hasan, Md Mehedi and Biswas, Bishakha Rani and Hou, Xueyu and Guan, Yongjie},
title = {ForestRAG-AR: An AR and RAG Framework for Context-Aware Silviculture Assistance},
year = {2025},
isbn = {9798400722387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3769102.3774894},
doi = {10.1145/3769102.3774894},
abstract = {Silvicultural decision making in forestry relies heavily on technical manuals and expert knowledge that are difficult to access in the field. Meanwhile, advances in wearable augmented reality (AR) and retrieval-augmented generation (RAG) offer new opportunities to deliver context-aware, data-driven guidance directly within natural environments. This paper presents ForestRAG-AR, an integrated AR and RAG framework that provides site-specific silvicultural recommendations grounded in authoritative forestry documents. The system combines (i) a perception and context extraction module that interprets local forest conditions, including tree species, stand density, and terrain slope, through on-device sensing, (ii) a RAG-based knowledge backend that adapts a large language model to forestry by embedding regional silviculture manuals and best-management-practice guides, and (iii) an AR interface that visualizes thinning and pruning guidance with natural-language explanations and verifiable citations. A domain-adapted cross-encoder reranker fuses environmental context with text passages to improve retrieval precision and factual grounding. Experiments on 50 forestry queries demonstrate that ForestRAG-AR improves nDCG@10 by 7.8 points and citation precision to 0.96 while maintaining sub-500 ms end-to-end latency.},
booktitle = {Proceedings of the Tenth ACM/IEEE Symposium on Edge Computing},
articleno = {64},
numpages = {6},
location = {the Hilton Arlington National Landing, Arlington, VA, USA},
series = {SEC '25}
}

@article{10.1145/3779214,
author = {Sharma, Himanshu and Padha, Devanand and Singh, Yashwant},
title = {ChitraVivran: Real-Time Attention-Based Hindi Image Captioning with Boosted Contextual Descriptors},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3779214},
doi = {10.1145/3779214},
abstract = {Image captioning is the ability to generate concise natural language descriptions of given images. It integrates computer vision and natural language processing, two cutting-edge artificial intelligence disciplines. Image captioning is nowadays widely used in vision assistance, healthcare, remote sensing, and security. While English image captioning has advanced across domains, Hindi image captioning remains underdeveloped, lacking features like autonomy, ensemble extraction, hybrid attention, and vision-boosted decoding. Additionally, integrating Hindi image captioning into vision aid tools is infeasible due to the lack of real-time and multi captioning ability. This research introduces ChitraVivran, a novel real-time, end-to-end Hindi image captioning framework. Our framework employs an ensemble visual feature extraction module to generate boosted contextual descriptors, enriching the fusion of visual and semantic embeddings. A dataset named PASCAL 1K-Hindi has also been manually created by translating the PASCAL 1K-English image captioning dataset into Hindi. Various pipeline configurations, confining ensemble feature extractors, attention mechanisms, and decoders, have also been developed and tested for Hindi image captioning. To enhance the applicability of Hindi image captioning in vision aid tools, our framework also incorporates real-time captioning and customized multi-captioning support. Experimental analysis on the Flickr 8K-Hindi and our newly developed PASCAL 1K-Hindi dataset indicates that ChitraVivran produces improved quantitative (Bilingual Evaluation Understudy-3(26.73\%), Bilingual Evaluation Understudy-4(16.82\%)) and qualitative results against baselines. Our framework demonstrates high performance in real-time captioning.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec
}

@inproceedings{10.1145/3769102.3770628,
author = {Mo, Fan and Malekzadeh, Mohammad and Chatterjee, Soumyajit and Kawsar, Fahim and Mathur, Akhil},
title = {Enhancing Efficiency in Multidevice Federated Learning through Data Selection},
year = {2025},
isbn = {9798400722387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3769102.3770628},
doi = {10.1145/3769102.3770628},
abstract = {Ubiquitous wearable and mobile devices provide access to a diverse set of data. However, the mobility demand for our devices naturally imposes constraints on their computational and communication capabilities. A solution is to locally learn knowledge from data captured by ubiquitous devices, rather than to store and transmit the data in its original form. In this paper, we develop a federated learning framework, called Centaur, to incorporate on-device data selection at the edge, which allows partition-based training of a deep neural nets through collaboration between constrained and resourceful devices within the multidevice ecosystem of the same user. We benchmark on five neural net architecture and six datasets that include image data and wearable sensor time series. On average, Centaur achieves ~19\% higher classification accuracy and ~58\% lower federated training latency, compared to the baseline. We also evaluate Centaur when dealing with imbalanced non-iid data, client participation heterogeneity, and different mobility patterns. To encourage further research in this area, we release our code at github.com/nokia-bell-labs/data-centric-federated-learning.},
booktitle = {Proceedings of the Tenth ACM/IEEE Symposium on Edge Computing},
articleno = {2},
numpages = {14},
keywords = {federated learning, constrained devices, data selection, partition-based training, edge intelligence, on-device AI},
location = {the Hilton Arlington National Landing, Arlington, VA, USA},
series = {SEC '25}
}

@inproceedings{10.1145/3770177.3770327,
author = {Hou, Bingxue and Zhang, Beichen and Zhang, Chengjin and Huang, Yongyi and Hu, Haowei},
title = {Analysis of indicators affecting illegal wildlife trade based on mathematical model},
year = {2025},
isbn = {9798400720109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3770177.3770327},
doi = {10.1145/3770177.3770327},
abstract = {To combat illegal wildlife trade, our team developed a project called the Ecological Defenders Program. An index system comprising eight branch goals was devised. To identify the ideal implementation partner, the United States Fish and Wildlife Service (USFWS) was selected through the application of an entropy weight-based Technique for Order Performance by Similarity to the Ideal Solution (TOPSIS) comprehensive evaluation model. Furthermore, Spearman correlation analysis was employed to assess the correlation between the primary tasks of the USFWS, thereby enhancing the persuasiveness of the project's adoption. The ARIMA time-series prediction model was used to analyze the lack of customer power and the need for additional imported resources. Finally, linear regression fitting was used to further analyze the impact of customer adoption of the project on combating illegal wildlife trade, and sensitivity analysis was performed.},
booktitle = {Proceedings of the 2025 International Conference on Economic Management and Big Data Application},
pages = {912–917},
numpages = {6},
keywords = {ARIMA forecasting, Spearman correlation analysis, United States Fish and Wildlife Service (USFWS), entropy-based TOPSIS model, illegal wildlife trade, resource mobilisation},
location = {
},
series = {ICEMBDA '25}
}

@inproceedings{10.1145/3769102.3770620,
author = {Song, Guanqun and Li, Yan and Zhu, Ting},
title = {A Metal Sensing and Biometric-based Tracking System},
year = {2025},
isbn = {9798400722387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3769102.3770620},
doi = {10.1145/3769102.3770620},
abstract = {Smart buildings are supposed to be able to send alerts and localize threats. Despite traditional smart security devices such as fire alarms, entrance guards, and cameras, modern smart buildings also need to identify and track hostiles who hide potentially harmful metal objects under their clothes. We introduce Magneto, the first metal-sensing and biometric-based tracking system that makes use of existing power cables and WiFi infrastructures. Magneto tracks and identifies individuals' gait signatures while simultaneously sensing and discriminating metal objects. By leveraging existing power line infrastructure and WiFi networks, Magneto turns normal buildings into automated secure smart buildings. We built a prototype that fused both magnetic and RF sensing networks and evaluated it with individuals carrying 10 different metal objects. To show the robustness of our system, the volunteers have also sat in a metal wheelchair in the process of evaluation. Our extensive evaluation in a real-life environment shows that Magneto achieves a metal detection accuracy of 91.4\% and a localization accuracy above 97\%.},
booktitle = {Proceedings of the Tenth ACM/IEEE Symposium on Edge Computing},
articleno = {23},
numpages = {17},
keywords = {metal sensing, biometric-based tracking},
location = {the Hilton Arlington National Landing, Arlington, VA, USA},
series = {SEC '25}
}

@article{10.1145/3770689,
author = {Li, Zhenghao and Lu, Taiting and Liu, Runze and Yuan, Shengming and Gosalia, Jigar and Bowlus, Christine and Proctor, David N. and Pawelczyk, James A. and Jin, Yincheng and Gowda, Mahanth},
title = {FuSenseRing: An Open-Source Platform for Robust Cuffless Blood Pressure Monitoring via Multimodal Sensor Fusion and Temperature-Adaptive Attention},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770689},
doi = {10.1145/3770689},
abstract = {Reliable, frequent blood pressure (BP) monitoring outside clinical settings is crucial for cardiovascular health management, yet current wearable solutions often fail to deliver clinically accurate measurements in real-world conditions. A key limitation stems from the common reliance, particularly in convenient ring form factors, on photoplethysmography (PPG) sensing alone. PPG signals acquired from the finger are highly susceptible to physiological and environmental noise; notably, finger skin temperature fluctuations induce errors exceeding 10 mmHg due to vascular changes, while sensor-skin contact pressure further distorts the signal. This inherent sensitivity of PPG, when uncompensated, prevents many devices from consistently meeting clinical accuracy standards. We introduce FuSenseRing, an open-source smart ring engineered to overcome these issues through synergistic hardware-software co-design. FuSenseRing integrates a multimodal sensor suite (PPG, ECG, skin temperature, contact force) on a custom flexible PCB within a low-cost form factor, approximately $94.9 per unit at retail (batch of 5) and approximately $28.72 per unit at wholesale (batch of 500), while remaining lightweight (4.6 g) to ensure optimal signal acquisition during BP measurements. Our core algorithmic innovation is a Temperature-Adaptive Attention (TAA) mechanism that dynamically fuses PPG/ECG features and recalibrates their contributions using real-time temperature context to specifically mitigate thermal drift. Efficient personalization via transfer learning requires only 5-10 normal-temperature calibration samples. Through rigorous evaluations involving total 60 participants across diverse laboratory-controlled temperatures (17 °C to 40 °C skin temp) and challenging real-world ambulatory conditions (-10 °C to 24.4 °C ambient temp), we demonstrate FuSenseRing's effectiveness. Our system maintains high accuracy across the full temperature range (Overall SBP/DBP ME±STD: -0.09 ± 7.67 / 0.42 ± 5.74 mmHg), satisfying the stringent accuracy criteria of the FDA-recognized AAMI/ISO 81060-2 standard. Furthermore, FuSenseRing significantly outperformed three popular commercial BP rings in the real-world study.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {192},
numpages = {34},
keywords = {wearable sensors, blood pressure monitoring, sensor fusion, open-source hardware}
}

@inproceedings{10.1145/3737906.3767096,
author = {Wang, Hao and Wang, Xin and Wei, Tao and Zhong, Yu and Zhang, Qian and Wang, Lu},
title = {Enabling Real-Time Monitoring of Wound Exudate Status through On-Tag RF Computing},
year = {2025},
isbn = {9798400719837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3737906.3767096},
doi = {10.1145/3737906.3767096},
abstract = {RF computing represents an emerging paradigm that computes by directly manipulating physical-layer RF signal properties. Capitalizing on this, we present a battery-free wireless system for real-time pH monitoring in diabetic foot ulcers (DFUs). Our core innovation lies in hardware co-design that implements on-tag RF computing. This is achieved by integrating pH-sensitive hydrogel as a functional antenna component to directly modulate backscattered signals. This enables direct chemical-to-RF transduction: pH shifts instantaneously modulate antenna impedance, thereby altering the phase of backscattered signals. To ensure robustness, we use a differential twin-tag architecture to suppress environmental interference, which enables precise pH extraction. By embedding sensing within passive RF hardware, we deliver a low-cost solution advancing mobile health through RF computing principles.},
booktitle = {Proceedings of the Second International Workshop on Radio Frequency (RF) Computing},
pages = {1–5},
numpages = {5},
keywords = {RFID, pH Monitoring, Wearable Sensors, Diabetic Foot Ulcers, Passive Sensing},
location = {
},
series = {RFCom '25}
}

@article{10.1145/3771581,
author = {Bucaria, Vincenzo Alessio and Longo, Francesco and Merlino, Giovanni and Restuccia, Francesco},
title = {µ-VF: Enabling Virtualization of Embedded FPGAs},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
url = {https://doi.org/10.1145/3771581},
doi = {10.1145/3771581},
abstract = {Despite growing interest in virtualization of Field-Programmable Gate Arrays (FPGAs), existing approaches predominantly target datacenter-class FPGAs, which heavily rely on external (powerful) servers for hypervisor execution and resource management. This significantly limits their suitability for edge environments where autonomy, energy efficiency, and direct low-latency access to physical Input/Output (I/O) are critical. To address this goal, this paper introduces µ-VF, a lightweight virtualization framework specifically designed to enable robust multi-tenancy on embedded FPGAs operating autonomously at the network edge. µ-VF embeds all virtualization logic entirely onboard the FPGA unit, eliminating the need for any off-chip infrastructure and thus significantly reducing overall system power consumption. Each tenant operates within a secure and isolated container on the on-chip Processing System (PS), coupled with exclusive access to a dedicated Programmable Logic (PL) region. Additionally, µ-VF fully virtualizes external General-Purpose Input/Output (GPIO) directly within the PL fabric, thus enabling independent, concurrent and latency-sensitive access to shared peripherals. We have implemented a prototype of µ-VF with a Zynq UltraScale+ ZCU102 board with PL operating at 100 MHz. Experimental results demonstrate that the hardware virtualization layer utilizes less than 10\% of the FPGA's logic resources, with 85\% available for tenant applications compared to 50\% in prior work. Moreover, µ-VF adds 2.93\% to Memory-Mapped I/O (MMIO) access latency compared to native execution for single-tenant operation, increasing to 6.5\% with four concurrent tenants. Memory throughput measurements show 1.8\% overhead for write operations and negligible impact on read operations, with aggregate throughput 17.1\% higher than previous frameworks. Hardware-based GPIO remapping completes in 20 nanoseconds.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = dec,
articleno = {66},
numpages = {26},
keywords = {containers, edge computing, fpga virtualization, hypervisor, i/o virtualization, iaas, iot, multi-tenancy, resource isolation, soc-fpga}
}

@article{10.1145/3770646,
author = {Bonelli, Eleonora and Secco, Matteo and Gianotti, Mattia and Di Marco, Davide and Garzotto, Franca and Colombo, Alessandro},
title = {Bridging Climbing and Interactive Smart Spaces for Children},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770646},
doi = {10.1145/3770646},
abstract = {Interactive Smart Spaces (ISSs)-digitally enhanced physical environments that respond to users' movements and gestures with multisensory stimuli-have been proposed as promising platforms to enhance children's social and cognitive skills. However, most existing approaches have focused on low-intensity physical activities. Evidence from exergame research and studies in medicine, psychology, neuroscience, and sports sciences suggests that incorporating medium-to-high physical activity can further improve cognitive and social outcomes.; AB@This paper investigates how such physical intensity can be integrated into ISSs to potentially enhance user engagement, social interaction, and inclusion. We introduce a novel approach that extends traditional ISS interactions with Augmented Climbing, where a climbing wall serves as a large interactive surface, with sensorized handholds and footholds acting as interaction affordances. Climbing was chosen for its established physical, cognitive, and social benefits. We present the enabling technologies, describe an interactive game experience designed for primary school children (both typically developing and atypically developing), and discuss the co-design process and strategies to support accessibility and inclusion. Finally, we report on two empirical studies (N= 12 and N = 113), whose findings indicate that integrating Augmented Climbing increases perceived task difficulty but also enhances verbal communication between players-an indicator of deeper social engagement.; AB@The physical and technological infrastructure of ISSs, when integrated with Augmented Climbing, serves as a flexible research tool for exploring a wide range of child-experience research topics and for advancing our understanding of how interactive experiences that combine multisensory stimuli and interactions with varying levels of physical intensity can support cognitive development, social interaction, and inclusion.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {162},
numpages = {24},
keywords = {Interactive Smart Space, Sensorized Climbing Wall, Augmented Climbing, Children, Cooperative Play}
}

@article{10.1145/3770632,
author = {Sahu, Nilesh Kumar and Gupta, Snehil and Lone, Haroon R.},
title = {MAD: A Multimodal Physiological and Self-Reported Dataset for Anxiety Research from a Low-to-Middle-Income Country},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770632},
doi = {10.1145/3770632},
abstract = {Wearable sensors provide a promising approach for monitoring physiological parameters, offering valuable insights into mental health conditions, including Social Anxiety Disorder (SAD). Early detection of SAD using physiological data can facilitate timely interventions, yet the development of robust anxiety detection models requires high-quality datasets. While real-world studies provide ecological validity, controlled studies ensure structured, high-quality data with minimal missing values, making them ideal for developing generalized and personalized anxiety detection models.; AB@Existing publicly available datasets related to anxiety research are limited to developed nations and focus on one or two anxiety-inducing activities. However, cultural differences significantly influence how anxiety is experienced and expressed, highlighting the need for datasets from diverse populations. This work presents MAD, a novel dataset collected in a low-to-middle-income country that addresses this gap. Our study involved participants engaging in three anxiety-inducing activities—speech, group discussion, and interview—each structured into three phases: anticipation, performance, and reflection. Physiological data were collected using wearable sensors, including electrocardiogram, electrodermal activity, and photoplethysmography, along with self-reported anxiety levels.; AB@Our dataset (N = 97) is unique in its inclusion of multiple anxiety-inducing activities, comprehensive phase-wise assessment, and representation of an underrepresented population. It provides a valuable resource for developing generalizable anxiety detection models, designing personalized interventions, and studying cultural variations in anxiety responses. By making MAD available, we aim to facilitate future research in machine learning-based mental health analysis, cross-cultural studies, and privacy-preserving anxiety detection approaches.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {206},
numpages = {34},
keywords = {Social anxiety disorder, Anxiety-provoking activities, ECG, PPG, EDA, Heart rate variability}
}

@article{10.1145/3770677,
author = {Xu, Xiangyu and Cui, Minghao and Ding, Ding and Ling, Zhen},
title = {HandiSense: Left/Right Operating Hand Recognition for Multi-finger Touchscreen Interactions},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770677},
doi = {10.1145/3770677},
abstract = {Touch-based interfaces dominate today's mobile and embedded devices, creating a need to accurately distinguish the operating hand (left vs. right) behind touch inputs for personalized, context-aware interaction. We present HandiSense, a purely software solution that determines the operating hand using only the raw multi-touch data exposed by standard capacitive-touchscreen APIs—no cameras, wearables, or low-level sensor access required. HandiSense captures distinctive geometric and kinematic signatures by constructing spatial and temporal triangles from concurrent touch points and their trajectories. A modular classification framework is first trained on two-finger gestures and then extended to handle up to five fingers through masked feature fusion. Evaluated on over 70,000 multi-finger samples from diverse users, devices, and scenarios, HandiSense achieves over 96\% accuracy and maintains stable performance across varying screen sizes and interaction styles. By offering a calibration-free, cross-device, and wholly software-based path to hand-aware interaction, HandiSense broadens the design space for personalized and context-adaptive touch interfaces.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {227},
numpages = {24},
keywords = {Touchscreen interactions, Multi-Touch gestures, Hand recognition, Model aggregation}
}

@article{10.1145/3770695,
author = {Shahi, Soroush and Shahabi, Farzad and Naboulsi, Rama and Fernandes, Glenn and Katsaggelos, Aggelos K and Alshurafa, Nabil},
title = {THOR: Thermal-Guided Hand-Object Reasoning via Adaptive Vision Sampling},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770695},
doi = {10.1145/3770695},
abstract = {Wearable cameras are increasingly used as an observational and interventional tool for human behaviors by providing detailed visual data of hand-related activities. This data can be leveraged to facilitate memory recall for logging of behavior or timely interventions aimed at improving health. However, continuous processing of RGB images from these cameras consumes significant power impacting battery lifetime, generates a large volume of unnecessary video data for post-processing, raises privacy concerns, and requires substantial computational resources for real-time analysis. We introduce THOR, a real-time adaptive spatio-temporal RGB frame sampling method that leverages thermal sensing to capture hand-object patches and classify them in real time. We use low-resolution thermal camera data to identify moments when a person switches from one hand-related activity to another and adjust the RGB frame sampling rate by increasing it during activity transitions and reducing it during periods of sustained activity (when the system has enough information to identify the activity). Additionally, we use the thermal cues from the hand to localize the region of interest (i.e., the hand-object interaction) in each RGB frame, allowing the system to crop and process only the necessary part of the image for activity recognition. We develop a wearable device to validate our method through an in-the-wild study with 14 participants and over 30 activities, and further evaluate it on Ego4D (923 participants across 9 countries, totaling 3,670 hours of video). Our results show that using only 3\% of the original RGB video data, our method captures all the activity segments, and achieves a hand-related activity recognition F1-score (95\%) comparable to using the entire RGB video (94\%). Our work provides a more practical path for the longitudinal use of wearable cameras to monitor hand-related activities and health-risk behaviors in real time.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {207},
numpages = {25},
keywords = {wearable, multimodal, hand-object, HAR, computer vision}
}

@article{10.1145/3770650,
author = {Lin, Hongnan and He, Zhenxuan and Pan, Ziao and Xu, Zihan and Shen, Jia and Zhang, Hechuan and Luo, Tianren and Li, Meng and Han, Teng and Tian, Feng},
title = {TactiFold: Electrotactile Aids for Visually Impaired Individuals in Origami},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770650},
doi = {10.1145/3770650},
abstract = {Origami offers creative and therapeutic benefits, but adapting it for visually impaired people is challenging due to its reliance on visual instructions and intricate details. Traditional aids like braille instructions and tactile diagrams are helpful but costly and limited in adaptability. In response, we developed TactiFold, an innovative wearable device using electrotactile technology to provide dynamic tactile feedback for origami folding. TactiFold features a high-density electrode array that renders fine-grained tactile textures on users' fingertips, guiding them through different folding steps. Five electrotactile symbols were designed and implemented, taking account of the simplicity and clarity of electrotactile aids, to represent key folding operations types and the corresponding positions. Two empirical studies with 14 visually impaired participants confirmed that TactiFold reliably rendered the tactile sensations that were well perceived, and by providing clear, guided feedback, TactiFold accompanied with necessary audio instructions enabled visually impaired people to get started with origami at high precision. This work demonstrates the flexibility and viability of electrotactile rendering in adaptations for accessibility.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {195},
numpages = {29},
keywords = {Origami, Visually impaired, Electrotactile, Wearable tactile aids}
}

@article{10.1145/3770635,
author = {Dasari, Ananyananda and Revanur, Ambareesh and Gadepally, Sai Shweta and Houston-Suluku, Nathaniel and Kamara, Sheku and Jeni, Laszlo A. and Tucker, Conrad},
title = {REVIT: A Dataset and Benchmark for Remote Vital Sign Estimation in Real-World Settings},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770635},
doi = {10.1145/3770635},
abstract = {Video-based physiological signal estimation is crucial for non-invasive and contactless patient monitoring in scenarios where traditional sensor-based methods are impractical, such as telemedicine and large-scale population health studies. The development and efficacy of such approaches is critically influenced by the availability of representative human subject datasets that reflect real-world scenarios. However, existing datasets predominantly consist of recordings collected in controlled laboratory settings with limited diversity in environmental conditions and demographic representation. Such constraints hinder the generalizability and robustness of video-based physiological signal estimation approaches in real-world scenarios (due to real-world scenarios demonstrating a challenging distribution of environmental diversity as compared to laboratory conditions). To this end, we present the REmote VITals (REVIT) dataset, comprising of 1553 human subject videos and associated ground truth physiological sign measurements (such as heart rate) collected in multiple outside-the-lab settings for the remote estimation of human physiological signs. The dataset is uniquely designed to capture a wide range of environmental diversity (such as variable lighting and background) encountered in outside-the-lab settings, thereby enabling the development of more robust physiological signal estimation approaches. Through statistical hypothesis testing, we demonstrate that utilizing the REVIT dataset results in a significant improvement in the robustness of existing video-based physiological signal estimation approaches. Furthermore, this work highlights the impact of existing data distributions toward controlled laboratory conditions, underscoring their limitations in real world applicability.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {169},
numpages = {40},
keywords = {Dataset, Physiological signals, Remote estimation, Mobile phone}
}

@inproceedings{10.1145/3768633.3770130,
author = {Bikkina, Bhaskar Ruthvik and Guda, Srija and Yekkali, Abhinav Sai and Deshkar, Atharva and Jain, Tushya and Chakraborty, Dipanjan},
title = {Learnings from a VR-Based Fire Safety Training for Students and Security Staff in an Indian University},
year = {2025},
isbn = {9798400718489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3768633.3770130},
doi = {10.1145/3768633.3770130},
abstract = {We present a Virtual Reality (VR)–based training module for fire extinguisher use, developed for students and security staff in a university setting. The simulation offers an immersive, repeatable alternative to conventional fire safety drills, which are often costly and logistically challenging. Through a mixed-methods study with 32 participants, we examine user performance, learning outcomes, and perceptions of realism and usability. While some first-time users faced minor interaction challenges, all participants completed the task and reported increased confidence. Our findings highlight the potential of VR for scalable, context-sensitive emergency preparedness, particularly in resource-constrained institutional environments.},
booktitle = {Proceedings of the 16th International Conference of Human-Computer Interaction (HCI) Design \&amp; Research},
pages = {162–177},
numpages = {16},
keywords = {virtual reality, fire safety training, emergency preparedness},
location = {
},
series = {IndiaHCI '25}
}

@inproceedings{10.1145/3737906.3767098,
author = {Shang, Fei and Du, Haohua and Yan, Dawei and Yang, Panlong},
title = {RINN: One Sample Radio Frequency Imaging based on Physics Informed Neural Network},
year = {2025},
isbn = {9798400719837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3737906.3767098},
doi = {10.1145/3737906.3767098},
abstract = {Due to its ability to work in non-line-of-sight and low-light environments, radio frequency (RF) imaging technology is expected to bring new possibilities for embodied intelligence and multimodal sensing. However, widely used RF devices (such as Wi-Fi) often struggle to provide high-precision electromagnetic measurements and large-scale datasets, hindering the application of RF imaging technology. In this paper, we combine the ideas of PINN to design the RINN network, using physical constraints instead of true value comparison constraints and adapting it with the characteristics of ubiquitous RF signals, allowing the RINN network to achieve RF imaging using only one sample without phase and with amplitude noise. Our numerical evaluation results show that compared with 5 classic algorithms based on phase data for imaging results, RINN's imaging results based on phaseless data are good, with indicators such as RRMSE performing similarly well. RINN provides new possibilities for the universal development of radio frequency imaging technology.},
booktitle = {Proceedings of the Second International Workshop on Radio Frequency (RF) Computing},
pages = {12–17},
numpages = {6},
keywords = {Wireless sensing, RF imaging, PINN, One sample learning},
location = {
},
series = {RFCom '25}
}

@article{10.1145/3770686,
author = {Meng, Xuanqi and Ge, Weiping and Tian, Yichen and Tong, Xinyu and Liu, Xiulong and Xie, Xin and Qu, Wenyu},
title = {MetaTrack: Enabling Wi-Fi Device Free Tracking in Complex Scenarios},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770686},
doi = {10.1145/3770686},
abstract = {In recent years, Wi-Fi-based human activity sensing has emerged as a prominent research domain, attracting considerable attention across both academic and industrial communities. While the Fresnel zone model has served as a fundamental theoretical framework for Wi-Fi sensing systems, its explanatory power proves inadequate when applied to complex real-world environments. Although numerous studies have attempted to establish theoretical models for cross-link and non-line-of-sight (NLOS) scenarios, a unified theoretical framework for device-free tracking in complex environments remains lacking. To fill this gap, we introduce MetaTrack, which is designed to achieve device-free tracking in complex scenarios in the form of sensing-informed lightweight neural network modules. In this paper, our contribution is twofold: 1) We propose Tracking Heat Zone (THZ), a novel representation that describes wireless signal distributions in diverse scenarios, enabling adaptive modeling of arbitrary environments. 2) We design a standardized sensing-informed modular pipeline that effectively translates THZ representations into lightweight neural network. Such a customizable solution is not only easily deployed, but also flexible to adapt to different kinds of complex scenarios. Experiments in several environment settings show that MetaTrack achieves high-precision tracking not only in ideal scenarios with a median error of 0.48m, but also in complex environments with a median error 0.52m, demonstrating its technical strength where it matters most.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {200},
numpages = {26},
keywords = {Wi-Fi Sensing, Device-free Tracking, Theoretical Model}
}

@article{10.1145/3770666,
author = {Fu, Yibo and Shen, Vivian and Riera Naranjo, V\'{\i}ctor and Deng, Bolei and Adams, Alex and Hester, Josiah},
title = {SoundOff: Low-cost Passive Ultrasound Tags for Non-invasive and Non-Intrusive Smart Home Sensing},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770666},
doi = {10.1145/3770666},
abstract = {Understanding in real time how we move, operate, and interact within living and working spaces is crucial to applications in smart buildings, elder care, and the automation industry. Myriads of systems have used complex electronics to capture this information, yet they often rely on independent sensors or devices that require active power and circuitry, also raising privacy concerns when using cameras or microphones. These solutions are often difficult to setup, have incompatible ecosystems, and require extra cost when upgrading already existing devices. In this paper, we propose SoundOff, a system that deploys ultra-low-cost, easily manufacturable, passive-ultrasound-emitting tags in any indoor environment (i.e., door knobs, toilet lids, cabinets, faucets, and windows), where the movement of this furniture causes a unique ultrasonic emission identifiable by a wearable device worn by users. These tags generate ultrasound signals during everyday interaction, above the range of human hearing, making them non-intrusive and non-invasive. This electronics-free, zero-infrastructure solution provides a scalable way to instrument spaces. Through a series of performance evaluations, we show that the ultrasonic emissions generated by SoundOff with different geometrical designs are not only robust to various environmental noises but also easily distinguishable from each other. Through physics-based modeling, we demonstrate how to systematically generate thousands of designs with unique and easily distinguishable ultrasonic emissions, enabling a wide range of interactions that can be mapped to custom automation systurce the work, including a geometric modeling pipeline and fabrication guide that enables design exploration, as well as an easily modifiable recognition system, allowing SoundOff tags to be replicated and disseminated throughout any indoor environment.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {174},
numpages = {32},
keywords = {Smart Environments, Acoustic Tags, Passive Ultrasound, Fabrication}
}

@article{10.1145/3770705,
author = {Kalanadhabhatta, Manasa and Rahman, Tauhidur and Grabell, Adam S. and Ganesan, Deepak},
title = {Tandem: At-Home Behavior Assessment Using Multimodal Signals from the Parent-Child Dyad},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770705},
doi = {10.1145/3770705},
abstract = {The quality of parent-child interactions at an early age has been linked to children's social-emotional development, executive function, and risk for behavior problems. As such, parent-child interactions in naturalistic settings could present a unique opportunity to screen for at-risk behavior in young children, enabling timely and targeted interventions. In this work, we validate the feasibility of using structured at-home play sessions, completed via the Tandem smartphone app, to enable highly accurate and scalable behavioral assessments. We demonstrate that audio and physiological signals recorded during the play session can be used to capture key markers of parent-child interaction dynamics, which are more indicative of at-risk behavior compared to features from each individual alone. We propose novel audio-based dyadic interaction features that significantly outperform conventional speech features at predicting risk for behavior problems, achieving an F1 score of 0.87. Furthermore, we show that dyadic physiological synchrony features, extracted from privacy-preserving wearable sensor data, can classify at-risk behavior with an F1 score of 0.91. Tandem thus sets the stage for automated at-home behavior assessment tools for young children that balance screening accuracy with practical deployment considerations.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {182},
numpages = {25},
keywords = {mental health sensing, behavioral problems, parent-child interaction, audio, wearables}
}

@article{10.1145/3770652,
author = {Yan, Hua and Tan, Heng and Ding, Yi and Zhou, Pengfei and Namboodiri, Vinod and Yang, Yu},
title = {Large Language Model-guided Semantic Alignment for Human Activity Recognition},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770652},
doi = {10.1145/3770652},
abstract = {Human Activity Recognition (HAR) using Inertial Measurement Unit (IMU) sensors is critical for applications in healthcare, safety, and industrial production. However, variations in activity patterns, device types, and sensor placements create distribution gaps across datasets, reducing the performance of HAR models. To address this, we propose LanHAR, a novel system that leverages Large Language Models (LLMs) to generate semantic interpretations of sensor readings and activity labels for cross-dataset HAR. This approach not only mitigates cross-dataset heterogeneity but also enhances the recognition of new activities. LanHAR employs an iterative re-generation method to produce high-quality semantic interpretations with LLMs and a two-stage training framework that bridges the semantic interpretations of sensor readings and activity labels. This ultimately leads to a lightweight sensor encoder suitable for mobile deployment, enabling any sensor reading to be mapped into the semantic interpretation space. Experiments on five public datasets demonstrate that our approach significantly outperforms state-of-the-art methods in both cross-dataset HAR and new activity recognition. The source code is publicly available at https://github.com/DASHLab/LanHAR.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {230},
numpages = {25},
keywords = {Human activity recognition, Natural language processing, Large language models}
}

@article{10.1145/3770631,
author = {Zhao, Yukun and Song, Xinyuan and Huang, Huajian and Braud, Tristan},
title = {From Motion to Localization: Cross-view Optimization with Stationary Event and RGB Cameras for Enhanced Pose Estimation},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770631},
doi = {10.1145/3770631},
abstract = {Applications such as Augmented Reality (AR) require accurate device positioning to minimize alignment errors. While visual positioning techniques offer high accuracy, their performance can degrade due to environmental changes like lighting variations and object movements. This paper introduces a new approach to visual positioning, relying on a stationary joint event/RGB sensing platform to track scene dynamics in real-time. This platform is at the core of a localization pipeline to predict the pose of user devices. First, a cross-modal object tracker matches dynamic objects between RGB and event images captured by the platform. These objects contribute to building a dynamic map, combined with the initial static 3D Structure from Motion (SfM) model to form a global feature map. Finally, a cross-view pose optimizer estimates pose uncertainties between modalities to refine and improve localization accuracy. To validate our approach, we collect a large-scale dataset over three scenes to account for typical AR scenarios where dynamics can affect the quality of visual positioning. We contribute this dataset to the community for future research on scene dynamics. Our approach shows significant improvement over existing methods, reducing translation and rotation errors by 12.9\% and 13.4\%, respectively, for weekly data over 4 weeks, and by 38.5\% and 16.2\% for monthly data over 4 months, compared to HLoc (SP+SG). It also reduces performance degradation by up to 50\% after only 4 weeks.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {243},
numpages = {25},
keywords = {Augmented Reality, Event Camera, Visual Positioning}
}

@article{10.1145/3770656,
author = {Sheng, Biyun and Sun, Shuqi and Cai, Hui and Dai, Chen and Xiao, Fu},
title = {From Coarse to Fine: Fast and Effortless Facial Landmark Detection via mmWave Signals},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770656},
doi = {10.1145/3770656},
abstract = {Facial landmarks provide essential representations of facial states and movements, serving as the foundation for numerous face-related tasks. However, traditional optical device-based facial landmark detection (FLD) solutions suffer from limitations in low-light conditions, occlusion sensitivity, and privacy concerns. In this paper, we propose an efficient two-stage facial landmark detection system, CF-FLD, which utilizes millimeter-wave (mmWave) radar signals to reconstruct human faces. Specifically, CF-FLD is composed of coarse-grained affine transformation (CAT) and fine-grained offset transformation (FOT). To characterize large-scale and rigid facial movements caused by head poses and joint motions, CAT defines sparse and representative triangle constraints within and across different facial parts for affine transformation. Based on CAT results, FOT is presented to progressively obtain offset shifts from subtle and non-rigid facial deformations. Instead of resource-intensive area detection or search, FOT designs a multi-level region partition strategy, in which region-wise hybrid network and region-aware attention are constructed to hierarchically refine facial landmarks. Comprehensive evaluations on data collected from 20 participants in real-world environments demonstrate that CF-FLD can accurately localize facial landmarks during facial motion, achieving a mean absolute error (MAE) of 2.02 mm and a normalized mean error (NME) of 3.37\% at a low cost.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {208},
numpages = {25},
keywords = {Facial Landmark Detection, Coarse-grained Affine Transformation, Fine-grained Offset Transformation, Region-wise Hybrid Network, Region-Aware Attention}
}

@inproceedings{10.1145/3764926.3771950,
author = {Casari, Martina and Po, Laura},
title = {Scalable Urban Air Quality Monitoring: Generalizable Calibration Models for Cross-City Sensor Deployment},
year = {2025},
isbn = {9798400721892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3764926.3771950},
doi = {10.1145/3764926.3771950},
abstract = {The large-scale deployment of low-cost air quality sensors is hindered by the need for site-specific calibration against reference-grade monitoring stations. This study addresses that challenge by exploring spatially generalizable calibration methods, leveraging a multi-national dataset of 34 sensors deployed across 10 urban locations in 6 different countries. We systematically evaluate three machine learning models, LightGBM, LSTM, and Transformer, under three calibration strategies: traditional individual dataset baselines, multi-location generalization, and a novel Principal Component Analysis (PCA)-based spatial similarity method specifically designed for air quality applications.Our findings show that LightGBM, when combined with PCA-based spatial selection, delivers the best cross-location performance, achieving a mean absolute error of 6.26 μg/m3. This demonstrates that selecting calibration data from environmentally similar urban contexts significantly enhances model transferability across regions. Furthermore, our analysis of varying amounts of local training data indicates that new sensor deployments can achieve effective calibration without requiring prolonged site-specific data collection. This finding supports the development of scalable, AI-driven urban air quality monitoring systems with reduced setup time and lower deployment barriers.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Advances in Urban-AI},
pages = {49–58},
numpages = {10},
keywords = {Urban AI, Spatiotemporal reasoning, Heterogeneous sensor integration, Cross-city generalization, Air quality monitoring, Sensor calibration, Spatial transferability, Smart cities},
location = {
},
series = {UrbanAI '25}
}

@inproceedings{10.1145/3764922.3771202,
author = {Alkaee Taleghan, Samira and Karimzadeh, Morteza and Barrett, Andrew P. and Meier, Walter N and Banaei-Kashani, Farnoush},
title = {Ice-FMBench A Foundation Model Benchmark for Sea Ice Type Segmentation},
year = {2025},
isbn = {9798400721854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3764922.3771202},
doi = {10.1145/3764922.3771202},
abstract = {Accurate segmentation and mapping of sea ice types is crucial for safe polar navigation, offshore operations, climate monitoring, and ecosystem analysis. While deep learning has demonstrated strong potential for automating sea ice type segmentation, its success often relies on access to extensive expert-labeled datasets, which is both resource-intensive and time-consuming to create. However, foundation models (FMs), recently developed through self-supervised training on large-scale datasets, have demonstrated impressive performance across a range of remote sensing downstream tasks. Nevertheless, their applicability to sea ice type segmentation based on Synthetic Aperture Radar (SAR) imagery remains uncertain due to the unique challenges posed by sea ice—such as intricate geophysical patterns, pronounced seasonal variability, and SAR-specific artifacts like banding, scalloping, and heterogeneous backscatter—as well as the fact that SAR data in polar regions are often acquired using specialized sensor modes that differ markedly from those used to collect FM training data at lower latitudes, limiting their direct transferability to polar environments. To address this gap, we contribute: (1) Ice-FMBench, a comprehensive benchmark framework for evaluation of the state-of-the-art remote sensing FMs on the sea ice type segmentation task using Sentinel-1 SAR imagery, where Ice-FMBench is composed of a widely used standardized dataset, diverse evaluation metrics, and a representative set of selected remote sensing FM models most suitable for sea ice type segmentation, with the ability to include new models side-by-side the existing models; (2) an extensive comparative evaluation of the representative FMs using Ice-FMBench, with additional case studies to assess performance of the top-performing model in terms of transferability across temporal and spatial domains and sensitivity to training dataset size; and (3) a multi-teacher knowledge distillation approach to address lack of spatiotemporal transferability of the existing FMs by transferring insight from spatially and temporally specialized expert models into a single, efficient student model.},
booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Polar Data Science},
articleno = {1},
numpages = {10},
keywords = {Sea Ice Type Segmentation, Benchmarking, Foundation Models, Knowledge Distillation, Synthetic Aperture Radar (SAR)},
location = {
},
series = {PoIDS '25}
}

@inproceedings{10.1145/3737903.3768570,
author = {Li, Shusheng and Chen, Liang and Bo, Yang},
title = {FreqLT: Frequency-Domain Linear Transformation for Privacy-Preserving and Efficient Machine Learning on Wearable Sensor Signals},
year = {2025},
isbn = {9798400719806},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3737903.3768570},
doi = {10.1145/3737903.3768570},
abstract = {The proliferation of wearable computing devices has enabled unprecedented opportunities for personalized health monitoring and human-computer interaction. However, the continuous stream of highly sensitive physiological and kinematic data they generate poses significant privacy and security risks, especially when processed on untrusted cloud-based Machine Learning as a Service (MLaaS) platforms. Existing privacy-preserving techniques present a stark tradeoff: cryptographic methods like Homomorphic Encryption offer robust security but are computationally prohibitive for resource-constrained wearables, while simpler perturbation-based methods degrade model performance and offer weak privacy guarantees. To address this challenge, we propose FreqLT, a novel and lightweight framework for Privacy-Preserving and Efficient Machine Learning on wearable devices. The core insight of FreqLT is that the discriminative information for most sensor-based ML tasks resides in the frequency domain. It is a learnable, invertible linear transformation directly to the frequency-domain representation of the sensor data, effectively obscuring the original signal's time-domain characteristics and sensitive statistical properties. Our experiments on different datasets show that FreqLT provides superior privacy protection compared to other methods.},
booktitle = {Proceedings of the 3rd ACM Workshop on Smart Wearable Systems and Applications},
pages = {7–12},
numpages = {6},
keywords = {Wearable computing, privacy and security, linear transformation, machine learning},
location = {
},
series = {SmartWear '25}
}

@inproceedings{10.1145/3737906.3767103,
author = {Cui, Jin and Guo, Hao and Li, Yifan and Heng, Liwang and Liu, Huaxu},
title = {Neural Network-Based Optimization of Flexible Hydrogel Antennas},
year = {2025},
isbn = {9798400719837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3737906.3767103},
doi = {10.1145/3737906.3767103},
abstract = {The optimization of antennas is critical to the sensory performance of wearable devices, especially for passive sensing. However, the current optimization methods typically do not impose constraints on antenna geometries and frequency bands because they target multi-band applications. As a result, these methods are not suitable for human-sensing applications that require single-band operation and have geometric constraints. In this paper, a Neural Network-based Multi-parameter co-Optimization (NNMO) is proposed for optimizing flexible antenna design. NNMO integrates the back propagation neural network and genetic algorithm to ultimately derive the values of key parameters that affect antenna performance. Taking Nested-Slot Suspended-Patch and Eye Movement Sensing antennas as examples, the NNMO-optimized ones demonstrate better sensing performance (signal strength is one and a half times greater) and longer sensing distance (increasing 78\%) than baseline. Simulations and implementations are highlighted that NNMO can be used to optimize the flexible antenna design to achieve better impedance matching.},
booktitle = {Proceedings of the Second International Workshop on Radio Frequency (RF) Computing},
pages = {33–38},
numpages = {6},
keywords = {Flexible antenna, Impedance matching, Co-optimization, Passive sensing},
location = {
},
series = {RFCom '25}
}

@article{10.1145/3770669,
author = {Su, Jie and Ge, Fengtong and Wen, Zhenyu and Li, Taotao and Bai, Yang and Zhou, Yejian and Zhang, Xiaoqin},
title = {IMUZero: Zero-Shot Human Activity Recognition by Language-Based Cross Modality Fusion},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770669},
doi = {10.1145/3770669},
abstract = {Wearable-based human activity recognition (HAR) typically uses motion sensor data, such as inertial measurement unit (IMU) signals, to identify human movements. While effective in controlled scenarios, traditional HAR models are trained on a fixed set of activities and fail to generalize to new or unseen actions. This limitation motivates the use of zero-shot learning (ZSL), which aims to recognize unseen activities without direct training examples. Existing ZSL methods often rely on projecting seen and unseen classes into a shared latent space using external semantic information, such as visual or textual data. However, visual data are commonly unavailable in wearable settings, and text-based semantics from activity labels or coarse descriptions lack the detail needed for accurate recognition. Recent work explores large language models (LLMs) to provide prior knowledge through question-answering mechanisms. While promising, these approaches do not use raw sensor data directly and often miss important contextual signals. We propose IMUZero, a ZSL framework that fuses sensor signals with LLM-generated semantic attributes. Our method uses LLMs to produce fine-grained, decomposable activity attributes without additional LLM-based training, preserving sensor context. We also introduce a channel shuffle order constraint that models axial bias to improve generalization. Experiments on four public datasets show that our method outperforms existing ZSL approaches that rely on learned semantic embeddings. We release the code at https://github.com/Was-Lab/IMUZero.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {211},
numpages = {28},
keywords = {Human Activity Recognition, Zero Shot Learning, Large Language Model, Multi-modality}
}

@article{10.1145/3770637,
author = {Waghmare, Anandghan and Varghese, Sanjay and Zhang, Zhihan and Chatterjee, Ishan and Tekriwal, Ritesh and Patel, Shwetak},
title = {Z-Band: On-world Interactions Using Wrist-based Electrical Impedance Sensing},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770637},
doi = {10.1145/3770637},
abstract = {We introduce Z-Band, a wearable device that fuses bio-impedance and inertial sensing to enable robust stateful touch detection on various everyday, non-instrumented surfaces. Stateful touch detection, achieved by injecting wide-band, swept-frequency RF signals through the wrist and analyzing their reflections and transmissions, facilitates sensitive touch detection even on challenging dielectric surfaces. We build upon this foundation to support a range of gestures, including swipes and taps, and enable touch pressure sensing. User studies demonstrate over 95\% touch detection accuracy in stationary scenarios and over 90\% in motion without requiring per-user calibration. This approach offers a seamless, natural way to interact with wearables and receive haptic feedback through touch gestures on any surface, making possible a new generation of interactive experiences.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {216},
numpages = {45},
keywords = {Wearable input devices, AR/VR, RF sensing, Sensor fusion, Machine Learning, Interaction Techniques}
}

@article{10.1145/3770706,
author = {Tao, Yujie and Li, Jingjin and Ye, Libby and Zhang, Andrew and Bailenson, Jeremy N. and Follmer, Sean},
title = {Audio Augmentation of Manual Interactions to Support Mindfulness},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770706},
doi = {10.1145/3770706},
abstract = {Mindfulness is the state of maintaining attention to the present moment with curiosity and openness. Existing mobile technologies to support mindfulness focus on formal practices such as meditation, requiring dedicated space and time. However, everyday mindfulness—a more flexible form of practice woven into routine activities such as washing hands or cooking—remains under-supported. To address this gap, we introduce a wearable device that adopts a sensory-driven approach to foster two key components of mindfulness, attention and curiosity, in everyday contexts. Our device amplifies sounds produced by the user's hand interactions to make them more salient, such as the sounds of hands rubbing together or fingertips sliding across surfaces. By playing back the amplified sounds to the user in real time, the device leads to a fresh perspective on mundane interactions. We conducted a preregistered in-lab study with 60 participants to evaluate the device in an everyday task: object exploration. We found that audio augmentation enhanced self-reported state mindfulness, directing user attention to auditory properties of objects that would otherwise be overlooked. Behaviorally, audio augmentation caused participants to interact with objects for a longer duration than participants who did not experience audio augmentation. We also found that participants exhibited more trial-and-error exploratory behavior patterns with audio augmentation than without, suggesting increased curiosity.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {214},
numpages = {27},
keywords = {Sensory Augmentation, Mindfulness}
}

@article{10.1145/3770682,
author = {Yoon, Sang Ho and Kwak, DongKyu and Seo, Kyungjin and Kim, Rachel},
title = {Moving-Press: Pressure-based Moving Phantom Sensation for Immersive VR Hand Interaction},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770682},
doi = {10.1145/3770682},
abstract = {We propose a haptic interface, Moving-Press, enabling a pressure-based moving phantom sensation on the user's hand. The moving phantom sensation refers to an illusory perception of a continuous tactile stimulus traversing between two discrete stimulation points. While this phenomenon has been studied in the context of vibrotactile feedback, it remains underexplored in the domain of pressure feedback. As integrating phantom sensation offers a promising approach to simplifying typically bulky pressure devices, employing this phenomenon enables more compact pressure rendering systems. Therefore, we investigated whether pressure movement could be induced using two fixed-positioned actuators by leveraging pressure-based phantom sensation. Our initial perception study validated the occurrence of pressure-based phantom sensations on the hand. Then, we examined how varying the indentation depth of each actuator influences the perceived location, and how the phantom sensation intensity differs from a single pressure stimulus. Finally, we proposed a rack-and-pinion-driven wearable haptic device and conducted a user study to identify an appropriate haptic rendering profile for producing natural pressure movement. The results demonstrated that temporal modulation of indentation depth using the proposed device successfully evokes pressure-based moving phantom sensation.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {185},
numpages = {24},
keywords = {pressure feedback, tactile illusion, wearables, haptics}
}

@article{10.1145/3778168,
author = {Shibli, Fatima and Tufekci, Burak and Tunc, Cihan and Laidig, Robin},
title = {Leveraging LLM Decision-Making in the Internet of Drone Things (IoDT) Ecosystem},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3778168},
doi = {10.1145/3778168},
abstract = {The Internet of Drone Things (IoDT) advances autonomous drone operations by integrating live sensor inputs with environmental and situational awareness and intelligent decision-making capabilities. The full capabilities of IoDT remain limited by the difficulties of dynamic task assignment and path optimization, along with adaptive decision-making, when operating in complex environments such as disaster relief and smart agriculture. Traditional task-scheduling techniques have difficulty adapting to real-time changes caused by dynamic constraints such as weather variations, battery limitations, and drone malfunctions. We present an LLM-based task scheduling framework that uses Large Language Models (LLMs) to improve task prioritization performance and path planning accuracy while minimizing operational failures. We combine heuristic algorithms (A*, Dijkstra) with decision-making processes driven by LLMs to allow drones to adapt to environmental changes while optimizing efficiency and resource consumption. Integrating LLM technology into IoDT operations results in up to 95\% task completion rates and improves the scenario completion time by up to 42\%, while adding reasonable computational overhead. Our framework demonstrates improved task adaptability, battery efficiency, and stronger system resilience against non-LLM baselines during disaster relief and package delivery operations. Our research shows that LLM-based IoDT task management has transformative potential, leading to the development of more innovative and autonomous drone ecosystems.},
note = {Just Accepted},
journal = {ACM J. Auton. Transport. Syst.},
month = dec,
keywords = {Drones, Internet of Things, Internet of Drone Things, IoDT, Large Language Model, LLM, Simulator, Lightweight Simulation, Algorithm&nbsp;Development}
}

@inproceedings{10.1145/3771882.3771909,
author = {Di Serio, Arianna and Patern\`{o}, Fabio},
title = {Human Control of Privacy and Security Aspects in IoT Settings},
year = {2025},
isbn = {9798400720154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3771882.3771909},
doi = {10.1145/3771882.3771909},
abstract = {The digital technological evolution is driving exponential growth in connected sensors and objects within smart environments such as homes, workplaces, and social spaces. While automations bring new opportunities, they also introduce risks and challenges. It is essential to provide users with End-User Development (EUD) approaches and tools that let them create, modify, and control automations. In this perspective, a key challenge is enabling non-professional developers to understand and manage security and privacy risks. This paper presents a Systematic Literature Review analysing the intersection of EUD, the Internet of Things, and security and privacy issues, with particular attention to automations and trigger-action programming. We examined the research to identify major risks in smart environments, their sources, and consequences. We analysed approaches and tools designed to mitigate these risks, their user presentation, validation methods, user control levels, and application domains. Our findings offer an overview of current solutions and highlight opportunities for future research into tools that help non-professional developers understand IoT security, usability, and privacy.},
booktitle = {Proceedings of the 24th International Conference on Mobile and Ubiquitous Multimedia},
pages = {236–262},
numpages = {27},
keywords = {End-User Development, Internet of Things, Trigger-action programming, Usability and Security},
location = {
},
series = {MUM '25}
}

@inproceedings{10.1145/3764687.3764720,
author = {Watanabe, Kento and Nakamura, Satoshi},
title = {Can We Prevent "Text Neck" Using Only a Smartphone? Real-Time Neck Angle Estimation and a Serious Game as a Case Study},
year = {2025},
isbn = {9798400720161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3764687.3764720},
doi = {10.1145/3764687.3764720},
abstract = {Prolonged smartphone use often results in forward head posture, commonly known as “text neck,” which places excessive strain on the cervical spine and may lead to various health problems. However, users typically find it difficult to monitor and correct their posture without external support. To address this challenge, we propose a method for real-time neck angle estimation using only a smartphone’s front-facing camera and built-in sensors. Our approach extracts posture-related features such as the normalized distance from the nose to the neck base, interocular distance, smartphone tilt, and facial orientation. Regression analysis using Random Forest and Extra Trees models indicates that neck angle can be estimated with moderate accuracy (for example, R2 ≈ 0.6 and MAE ≈ 8.5°), although performance at extreme angles remains limited. To evaluate the practical utility of this system, we developed a serious game titled Look Up and Tap! that provides real-time visual feedback, including screen contrast adjustment and button highlighting, based on the estimated neck angle. A user study with 10 participants showed that this feedback significantly improved users’ awareness of their posture and encouraged more upright neck positions. These findings suggest that neck angle estimation using only a smartphone is feasible and can help promote posture awareness in everyday settings. The serious game serves as one example of how this approach can be applied in real-world scenarios.},
booktitle = {Proceedings of the 37th Australian Conference on Human-Computer Interaction},
pages = {356–370},
numpages = {15},
keywords = {posture correction, forward head posture, text neck, neck angle estimation, neck angle, biofeedback serious games, smartphone},
location = {
},
series = {OzCHI '25}
}

@inproceedings{10.1145/3766557.3766563,
author = {Zhang, Wenxi and Yang, Yanliang and Yao, Jie and Lu, Lan and Ablimit, Kadirya and Gao, Yi},
title = {First aid education under digital empowerment: intelligent curriculum design based on AI and virtual simulation},
year = {2025},
isbn = {9798400721168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3766557.3766563},
doi = {10.1145/3766557.3766563},
abstract = {As one of the core qualities of modern medical personnel training, the teaching paradigm of first aid competence is undergoing systematic changes with the deep integration of digital technology. Traditional first aid teaching faces problems such as insufficient reduction of clinical situation, imbalance of input-output ratio of teaching resources, and homogenization of evaluation dimension. The empowerment of digital technology provides an innovative path to solve these dilemmas - building a high-fidelity training environment through virtual simulation, real-time interaction and feedback with the help of intelligent devices, and accurate teaching and evaluation based on data analysis, so as to promote the transformation of first-aid education to the direction of intelligence, personalization, and precision. In this study, a set of intelligent first aid education system is constructed with deep learning (DL), virtual reality (VR) and augmented reality (AR) technologies as the core. The system adopts deep reinforcement learning (DRL) algorithm to optimize the dynamic response logic of the virtual simulation scene, collects physiological data in real time through multimodal sensor network for injury feature extraction, and combines AR technology to construct a three-dimensional interactive first aid environment. Through virtual simulation technology scenario simulation, intelligent simulator real-time feedback, artificial intelligence assisted assessment, combined with case studies and teamwork exercises and other teaching methods, to build an immersive, personalized first aid skills training mode, so as to comprehensively improve the clinical first aid resilience and teamwork level of medical students, and to promote the development of medical education in the direction of intelligence and precision.},
booktitle = {Proceedings of the 2025 International Conference on Educational Technology and Artificial Intelligence},
pages = {24–28},
numpages = {5},
keywords = {AI empowerment, Digital learning, First aid education, Intelligent education technology},
location = {
},
series = {ETAIC '25}
}

@article{10.1145/3757920,
author = {Zhao, Liangtian and Xu, Xiangmin and Pei, Shanshan and Chen, Siyu and Hu, Xiyuan and Xie, Qiwei},
title = {Road Surface State Change Detection Based on Binocular Vision for Autonomous Driving System},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3757920},
doi = {10.1145/3757920},
abstract = {Road surface condition monitoring is crucial for enhancing transportation safety and efficiency, with applications in autonomous driving and urban infrastructure management. Existing methods often rely on single-camera setups or manual inspections, which are either insufficient for real-time monitoring or labor-intensive. This system focuses on two critical factors: road slope and surface damage, both significantly impacting driving safety and experience, highlighting the need for timely detection. To ensure accuracy and robustness, the system employs a binocular camera for detailed road environment insights and integrates urban sensing techniques. Its hardware deployment processes stereo vision data on embedded platforms, ensuring compatibility with urban IoT networks. This approach surpasses single-camera systems in detecting road surface variations. The research motivation stems from the pressing need to enhance road safety and driving conditions in urban areas. By analyzing binocular camera data and urban sensing technologies, the system offers real-time road condition analysis for effective decision-making. Regarding results, the system showed robust performance in detecting both road slope and surface damage. Slope detection achieved high accuracy with minimal error, and road damage detection reached an overall accuracy of 84\%. The system remained stable across diverse conditions, including adverse weather and varying lighting.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = nov,
articleno = {29},
numpages = {22},
keywords = {Binocular Vision, Pavement Slope Detection, Pavement Damage Grade Detection, Urban Sensing}
}

@inproceedings{10.1145/3759972.3760142,
author = {Zhou, Yiting},
title = {Early-Life Environmental Pollutants and Neurodevelopmental Risks: Unraveling Exposure Pathways, Mechanisms, and Preventive Strategies},
year = {2025},
isbn = {9798400718656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3759972.3760142},
doi = {10.1145/3759972.3760142},
abstract = {Exposure to environmental pollutants in early life represents a main modifiable risk factor for neurodevelopmental disorders of infants and children including ASD/ADHD. In this review, we integrate evidence from 112 studies (2015-2025) on pathways involving maternal-fetal transfer, inhalation of airborne pollutants, household contact and neuro-disrupting mechanisms underlying childhood neurodevelopment. Early-life exposure to lead, fine particulate matter, and phthalates disrupts the blood-brain barrier, triggering neuroinflammatory responses and oxidative damage by disrupting thyroxine action, dopamine regulation and epigenetics; potentiated each other's effects, further amplifying the hazards of cognizance or behavioral deficits. Despite progress made, conflicts regarding low dose results and socio-economic status are ubiquitous. Decreasing plastic use, upholding pollution-control acts, promoting labeling-endocrine disruptor free products could attenuate such risks. Further works should combine multiple omics approaches and adopt longitudinal methods to uncover sensitive pathways through which pollutants act via geno-epigenetic actions, thereby tailoring evidentiary-based policies to foster early-life neural development.},
booktitle = {Proceedings of the 2025 International Conference on Health Informatization and Data Analytics},
pages = {11–16},
numpages = {6},
keywords = {Attention-deficit/hyperactivity disorder (ADHD), Autism spectrum disorder (ASD), Blood-brain barrier dysfunction, Environmental pollutants, Maternal-fetal transfer, Neurodevelopmental disorders, Oxidative stress},
location = {
},
series = {HIDA '25}
}

@inproceedings{10.1145/3742763.3760702,
author = {Ying, Yao and Huang, Minchuan and Chen, Shubin},
title = {Design and Implementation of STM32 Embedded System},
year = {2025},
isbn = {9798400721120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3742763.3760702},
doi = {10.1145/3742763.3760702},
abstract = {This STM32 model combines a machine learning algorithm with Things sensing technology, significantly improving prediction accuracy. The PyCharm integrated development environment and MySQL relational database were used to realize the model architecture and build a multidimensional feature data set. Experimental results show that the model has good generalization performance in sample testing. An automatic questionnaire crawler system based on Python was developed to improve the efficiency of questionnaire data collection, and a structural equation model for cognitive feature analysis was constructed to enhance the cognition of STM32. Based on the STM32 microcontroller architecture, a multi-node intelligent environment monitoring system model is implemented and designed. The temperature and humidity sensor is integrated with the wireless communication module to improve the time accuracy of monitoring data.},
booktitle = {Proceedings of the 2025 13th International Conference on Computer and Communications Management},
pages = {62–69},
numpages = {8},
keywords = {Cognitive behavioral analysis, Embedded systems, Integrated development environment, STM32 architecture},
location = {
},
series = {ICCCM '25}
}

@inproceedings{10.1145/3762249.3762309,
author = {Wu, You and Qin, Ying and Su, Xin and Lin, Yuxiu},
title = {Transformer-Based Risk Monitoring for Anti-Money Laundering with Transaction Graph Integration},
year = {2025},
isbn = {9798400713491},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3762249.3762309},
doi = {10.1145/3762249.3762309},
abstract = {This study addresses the growing complexity of transaction behaviors and the highly concealed nature of money laundering paths in current financial AML scenarios. It proposes a Transformer-based risk monitoring model for anti-money laundering. The approach is grounded in transaction sequence modeling and integrates the structural information of transaction graphs. A context-aware classifier is introduced to enable accurate identification and risk scoring of high-risk accounts. The model first applies feature embedding and positional encoding to each transaction. It then uses multiple Transformer layers to capture long-range behavioral dependencies. At the same time, it incorporates account interaction information from the graph structure. This enhances the model's ability to detect abnormal transaction chains across accounts. At the output stage, a classifier that fuses sequential semantics with graph context is used to determine the overall money laundering risk of each account. Multiple experiments were conducted on the publicly available Elliptic dataset. Results show that the proposed method outperforms existing mainstream models on evaluation metrics such as AUC, F1-Score, and Accuracy. It also demonstrates stronger discriminative power and greater stability in identifying high-risk accounts. Further analysis of model depth sensitivity and case-based verification supports the model's effectiveness in real-world complex transaction environments. The proposed method offers a more adaptable technical solution for financial institutions dealing with large-scale suspicious behavior detection tasks.},
booktitle = {Proceedings of the 2025 2nd International Conference on Digital Economy, Blockchain and Artificial Intelligence},
pages = {388–393},
numpages = {6},
keywords = {Anti-money laundering, Transformer model, financial behavior analysis, transaction graph modeling},
location = {
},
series = {DEBAI '25}
}

@inproceedings{10.1145/3759972.3760191,
author = {Han, Jiongxu and Lei, Yu and Qiu, Qian and Wu, Xinglan and Chen, Xuedan and Wu, Kaixin and Zhu, Jintian},
title = {Exploration of an Intelligent ICU Pressure Ulcer Skin Assessment System Based on a Dynamic Reward Adaptive Framework},
year = {2025},
isbn = {9798400718656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3759972.3760191},
doi = {10.1145/3759972.3760191},
abstract = {Large Language Models appear to be increasingly applied in clinical care assistance, and what seems to emerge from this understanding is that the integration of intelligent assessment systems can substantially enhance the efficiency and quality of nursing care. However, for specialized nursing tasks such as ICU pressure ulcers, establishing an effective intelligent assessment system tends to present challenges due to what might be characterized as the complexity of assessment, reliance on individual experience, and the unique characteristics of the environment and diseases. What this paper innovatively attempts to develop is a dual-path intelligent training system that seems to leverage a dynamic reward mechanism-driven dual-path training framework. This system appears to utilize an adaptive detector to classify limited samples and employs what seems to be a training auxiliary module to generate synthetic data, thereby ostensibly augmenting the training dataset and establishing what appears to represent a sustainable and evolving intelligent nursing assistance system.In the CP-LFW dataset benchmark test, our method appears to suggest better performance in occlusion image recognition accuracy compared to existing models. Specifically, what the data seems to indicate is that our model achieves an accuracy of approximately 89.7\%, seemingly outperforming the 76.2\% accuracy of the SenseTime security face recognition system and the 82.1\% accuracy of the foreign competitor, the MIT EmoPainNet pain recognition model. What the experiments tend to point toward is that the dynamic reward adaptive framework appears to achieve better recognition of low-quality images through what might be described as a dynamic feedback mechanism between genuine and fake samples. Within this broader analytical framework, compared to traditional multimodal reasoning, this framework seems to exhibit higher applicability and interpretability, presumably better meeting the clinical care needs of ICU pressure ulcers.},
booktitle = {Proceedings of the 2025 International Conference on Health Informatization and Data Analytics},
pages = {316–321},
numpages = {6},
keywords = {Dynamic reward adaptive framework, Intelligent assessment system, Nursing, Pressure ulcer skin},
location = {
},
series = {HIDA '25}
}

@article{10.1145/3725221,
author = {Liu, Qi and Wang, Zhilu and Zhou, Xiaokang and Zhang, Yonghong and Liu, Xiaodong and Lin, Haiyang},
title = {GSFL: A Privacy-Preserving Grouping-Split Federated Learning Approach in Resource-Constrained Edge Computing Scenarios},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3725221},
doi = {10.1145/3725221},
abstract = {The advancement of mobile multimedia communications, 5G, and Internet of Things (IoT) has led to the widespread use of edge devices, including sensors, smartphones, and wearables. This has generated in a large amount of distributed data, leading to new prospects for deep learning. However, this data is confined within data silos and contains sensitive information, making it difficult to be processed in a centralized manner, particularly under stringent data privacy regulations. Federated learning (FL) offers a solution by enabling collaborative learning while ensuring privacy. Nonetheless, data and device heterogeneity complicate FL implementation. This research presents a specialized FL algorithm for heterogeneous edge computing. It integrates a lightweight grouping strategy for homogeneous devices, a scheduling algorithm within groups, and a Split Learning (SL) approach. These contributions enhance model accuracy and training speed, alleviate the burden on resource-constrained devices, and strengthen privacy. Experimental results demonstrate that the GSFL outperforms FedAvg and SplitFed by 6.53\texttimes{} and 1.18\texttimes{}. Under experimental conditions with  (alpha=0.05) , representing a highly heterogeneous data distribution typical of extreme Non-IID scenarios, GSFL showed better accuracy compared to FedAvg by 10.64\%, HACCS by 4.53\%, and Cluster-HSFL by 1.16\%. GSFL effectively balances privacy protection and computational efficiency for real-world applications in mobile multimedia communications.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = nov,
articleno = {32},
numpages = {29},
keywords = {Federated learning, Split learning, Non-IID, Clustering}
}

@inproceedings{10.1145/3759972.3760173,
author = {Liu, Haotian},
title = {A BERT-Based Empirical Analysis of Medical Aesthetic Health Risks — A Study on the "Rednote" Platform},
year = {2025},
isbn = {9798400718656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3759972.3760173},
doi = {10.1145/3759972.3760173},
abstract = {In this study, we investigated consumers' views on and potential health risks of four popular aesthetic plastic surgeries on the Rednote platform: Photorejuvenation, Picosecond Laser Treatment, Facial Slimming Injection, and Hydrodermabrasion Injection. Through systematic data collection and analysis, a comprehensive framework has been developed that enhances content assessment by combining various deep learning methods. Our method integrates three neural network architectures, text-CNN, DPCNN and TextRNN, to carry out fine-grained sensitivity analysis, and classifies user feedback into three categories: positive, neutral and negative. These multi-model methods capture localized patterns and sequential dependencies in the user-generated content. The study further established two levels of risk assessment system. After initial characteristic extraction and risk indication under expert guidance, conduct predictive analysis with professional classifier based on BERT. This dual analysis effectively identifies potential health issues in consumer discussions. The model shows an effective ability to detect different types of risk information and provides useful support for consumer awareness and platform security measures.},
booktitle = {Proceedings of the 2025 International Conference on Health Informatization and Data Analytics},
pages = {220–224},
numpages = {5},
keywords = {BERT, Health Risk, Medical Aesthetics, Neutral networks},
location = {
},
series = {HIDA '25}
}

@inproceedings{10.1145/3759023.3759114,
author = {Phiri, Hazael and Maphosa, Vusumuzi and Moyo, Sibonile and Sebata, Allan},
title = {A Review of AI and IoT Integration Frameworks for Sustainable Soil Health Management},
year = {2025},
isbn = {9798400714276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3759023.3759114},
doi = {10.1145/3759023.3759114},
abstract = {The rising global population and increasing food demand, exacerbated by extreme climate changes in Sub-Saharan Africa, have accelerated soil degradation on arable lands critical for crop production. This has prompted calls for continental and national interventions to mitigate the trend. Among the proposed solutions are digitally enabled tools leveraging Artificial Intelligence (AI) and the Internet of Things (IoT), which have demonstrated significant success in developed regions. Recent advancements—such as cheaper, more accurate sensors, Edge AI, lightweight machine learning (ML) models running on edge devices, and open-source microcontrollers with enhanced computational capabilities—present promising opportunities for Sub-Saharan Africa, where small-scale farmers dominate agriculture. This study conducts a systematic literature review to evaluate AI and IoT integration frameworks for sustainable soil health management, examining the types of tools deployed and their contributions. The review analyses peer-reviewed publications from 2014 to 2024, sourced from Scopus, Google Scholar, the ACM Digital Library, and AJOL-CGspace. Findings indicate that while a layered approach framework is commonly adopted, it is often modified to suit specific contextual needs. However, many tools remain in the experimental stage, with proof-of-concept demonstrations but limited quantifiable evidence of their impact on sustainable soil health management. A key limitation of this study is the scarcity of long-term, large-scale field trials in Sub-Saharan Africa, restricting the generalisability of the findings to real-world farming conditions. Addressing this gap, calls for partnerships between researchers, governments, and local farming communities to implement pilot projects that test and refine these technologies in diverse agroecological zones across the region.},
booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems},
articleno = {25},
numpages = {9},
keywords = {IoT, AIoT framework, Machine learning, Soil health, Sustainable soil management},
location = {
},
series = {icABCD '25}
}

@inproceedings{10.1145/3765712.3765717,
author = {El Montaser, Soumia and Bailly, Gilles and Haliyo, Sinan and Gueorguiev, David},
title = {Vibrotactile Augmentation of Tangible Objects for 2-Dimensional Multi-Texture Rendering},
year = {2025},
isbn = {9798400715228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3765712.3765717},
doi = {10.1145/3765712.3765717},
abstract = {In virtual reality environments, using props that simulate objects or textures enables high quality haptic feedback without relying on bulky devices, thereby enhancing user immersion. However, covering the full range of tactile sensations soon requires a large number of different props, making this method complex. We propose augmenting tangible textures of varying hardness by applying vibrations to dynamically modulate their perceived roughness, thus leveraging both the material’s mechanical properties and the perceptual potential of vibration. Our results, obtained via hierarchical clustering of frequency–amplitude combinations of transverse sinusoidal vibrations, show that for each tested physical texture, modulation of perceived roughness can produce at least three distinct virtual textures. This approach allows the generation of a diverse range of tactile sensations from a limited set of physical textures, simplifying their integration into virtual environments through the concurrent variation of two essential tactile properties.},
booktitle = {Proceedings of the 36th Conference on l'Interaction Humain-Machine},
articleno = {5},
numpages = {15},
keywords = {3D printing texture, active haptic feedback, tangible object, texture rendering, vibrotactile feedback, cluster, texture imprim\'{e}e 3D, retour haptique actif, objet tangible, textures multiples, retour vibrotactile, cluster},
location = {
},
series = {IHM '25}
}

@article{10.1145/3748821,
author = {Bhardwaj, Ankit and Balashankar, Ananth and Iyer, Shiva and Soans, Nita and Sudarshan, Anant and Pande, Rohini and Subramanian, Lakshminarayanan},
title = {Comprehensive Monitoring of Air Pollution Hotspots Using Sparse Sensor Networks},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
url = {https://doi.org/10.1145/3748821},
doi = {10.1145/3748821},
abstract = {Urban air pollution hotspots pose significant health risks, yet their detection and analysis remain limited by the sparsity of public sensor networks. This article addresses this challenge by combining predictive modeling and mechanistic approaches to comprehensively monitor pollution hotspots. We enhanced New Delhi’s existing sensor network with 28 low-cost sensors, collecting PM2.5 data over 30 months from May 1, 2018, to Nov 1, 2020. Applying established definitions of hotspots to this data, we found the existence of an additional 189 hidden monthly hotspots in addition to confirming the 660 detected by the public network. Using predictive techniques like Space-Time Kriging, we identified monthly hotspots with 95\% precision and 88\% recall with 50\% sensor failure rate, and with 98\% precision and 95\% recall with 50\% missing sensors. The projections of our predictive model were further compiled into policy recommendations for public authorities. Additionally, we developed a Gaussian Plume Dispersion Model to understand the mechanistic underpinnings of hotspot formation, incorporating an emissions inventory derived from local sources. Our mechanistic model is able to explain 65\% of observed transient hotspots. Our findings underscore the importance of integrating data-driven predictive models with physics-based mechanistic models for scalable and robust air pollution management in resource-constrained settings.},
journal = {ACM J. Comput. Sustain. Soc.},
month = nov,
articleno = {28},
numpages = {36},
keywords = {Air pollution hotspots, pollution field interpolation, space-time kriging, dispersion modeling}
}

@inproceedings{10.1145/3719027.3765090,
author = {Xu, Shuo and Xu, Jiming and Xue, Pengfei and Wang, Xinyao and Ju, Lei and Zhang, Wei},
title = {Co-Prime: A Co-design Framework for Privacy Preserving Machine Learning on FPGA},
year = {2025},
isbn = {9798400715259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719027.3765090},
doi = {10.1145/3719027.3765090},
abstract = {In enormous privacy-sensitive machine learning application domains with collaborative data acquisition from multiple participants, secure multi-party computation (MPC) becomes a promising solution for privacy-preserving machine learning (PPML). Secret sharing protocols is a prevalent MPC strategy, where frequent data distribution and recombination are applied to uphold the confidentiality of participants' data. A key challenge for practical deployment of secret sharing protocols in PPML is the massive and unbalanced computation and communication workloads occurred in various linear and non-linear stages of machine learning. The imbalance could be further amplified when powerful hardware accelerators are designed to reduce the computation latency. In this work, we propose Co-Prime, an FPGA-based 3PC framework for efficient PPML without assistance from a secure third party. Co-Prime integrates protocol and hardware co-optimizations to mitigate the communication bottlenecks in secret sharing schemes. Particularly, Co-Prime proposes a novel protocol conversion technique that seamlessly converts data formats to adaptively adopt preferred protocols in various stages of PPML. Accelerator-friendly MPC primitives and system-level design space exploration schemes are designed to achieve latency hiding through overlapping computation and network communication. Finally, it enables direct interaction with data streams via network communication modules on FPGAs to further reduce the network communication overhead. Experimental results demonstrate significant performance improvements over existing privacy-preserving machine learning frameworks, with 2-18x speedup in inference latency across various LAN/WAN environments and neural network models.},
booktitle = {Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security},
pages = {3591–3604},
numpages = {14},
keywords = {fpga., privacy-preserving machine learning, secret sharing},
location = {Taipei, Taiwan},
series = {CCS '25}
}

@inproceedings{10.1145/3719027.3744823,
author = {Lu, Hongyi and Deng, Yunjie and Mertoguno, Sukarno and Wang, Shuai and Zhang, Fengwei},
title = {MOLE: Breaking GPU TEE with GPU-Embedded MCU},
year = {2025},
isbn = {9798400715259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719027.3744823},
doi = {10.1145/3719027.3744823},
abstract = {Graphics Processing Units (GPUs) are extensively used for applications such as machine learning, scientific computing, and graphics rendering. To protect sensitive data processed by GPUs, Trusted Execution Environments (TEEs) for GPUs have been proposed. GPU TEEs, built with hardware-based isolation primitives, can defend against high-privilege attackers like OS kernels. However, in this paper, we present MOLE, a novel attack that compromises the security of GPU TEEs on Arm Mali GPUs by exploiting the GPU-embedded Microcontroller Unit (MCU). By injecting malicious firmware into the MCU, an attacker can bypass GPU TEEs' security guarantees. We evaluated MOLE with state-of-the-art GPU TEE proposals under multiple real-world attack scenarios, such as in-GPU AES encryption and object detection tasks. Our evaluation shows that MOLE can successfully extract sensitive data or manipulate the computation results of GPU TEEs. We responsibly disclosed our findings to the authors of the affected GPU TEE proposals and received acknowledgments from all of them. Moreover, our findings prompted Arm to enhance the security of its GPU firmware supply chains.},
booktitle = {Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security},
pages = {693–707},
numpages = {15},
keywords = {GPU security, firmware attack, trusted execution environment},
location = {Taipei, Taiwan},
series = {CCS '25}
}

@inproceedings{10.1145/3719027.3765174,
author = {Ding, Ruyi and Xu, Tianhong and Shen, Xinyi and Ding, Aidong Adam and Fei, Yunsi},
title = {MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs},
year = {2025},
isbn = {9798400715259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719027.3765174},
doi = {10.1145/3719027.3765174},
abstract = {The transformer architecture has become a cornerstone of modern AI, fueling remarkable progress across applications in natural language processing, computer vision, and multi-modal learning. As these models continue to scale explosively for performance, implementation efficiency remains a critical challenge. Mixture-of-Experts (MoE) architectures, selectively activating specialized subnetworks (experts), offer a unique balance between model accuracy and computational cost. However, the adaptive routing in MoE architectures—where input tokens are dynamically directed to specialized experts based on their semantic meaning—inadvertently opens up a new attack surface for privacy breaches. These input-dependent activation patterns leave distinctive temporal and spatial traces in hardware execution, which adversaries could exploit to deduce sensitive user data. In this work, we propose MoEcho (MoE-Echo), discovering a side-channel analysis-based attack surface that compromises user privacy on MoE-based systems. Specifically, in MoEcho, we introduce four novel architectural side-channels on different computing platforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and Performance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting these vulnerabilities, we propose four attacks that effectively breach user privacy in large-language models (LLMs) and vision-language models (VLMs) based on MoE architectures: Prompt Inference Attack, Response Reconstruction Attack, Visual Inference Attack, and Visual Reconstruction Attack. We evaluate MoEcho on four open-source MoE-based models at different scales, with a specific focus on the DeepSeek architecture. Our end-to-end experiments on both CPU- and GPU-deployed MoE models demonstrate a 99.8\% success rate in inferring the patient's private inputs in healthcare records and 92.8\% in reconstructing LLM responses. MoEcho is the first run-time architecture-level security analysis of the popular MoE structure common in modern transformers, highlighting a serious security and privacy threat and calling for effective and timely safeguards when harnessing MoE-based models for developing efficient large-scale AI services.},
booktitle = {Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2159–2173},
numpages = {15},
keywords = {large language models, mixture-of-experts, side-channel attacks},
location = {Taipei, Taiwan},
series = {CCS '25}
}

