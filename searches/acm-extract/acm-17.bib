@inproceedings{10.1145/3678935.3678971,
author = {Hulleck, Abdul Aziz and Abdullah, Muhammad and Alkhalaileh, Abdelsalam and Liu, Tao and Mohan, Dhanya and Katmah, Rateb and Khalaf, Kinda and Rich, Marwan El},
title = {Impact of Upper Body Mass Scaling on Musculoskeletal Model Predictions during Gait},
year = {2024},
isbn = {9798400717628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678935.3678971},
doi = {10.1145/3678935.3678971},
abstract = {Utilizing musculoskeletal modeling through an inverse dynamics approach for gait assessment offers a non-invasive method to compute internal joint kinetics and ground reaction forces and moments solely from kinematic data, reducing reliance on cumbersome equipment. The effectiveness of these models relies on the scaling approach adopted to tailor the model to individual subject data. While constant percentage-based, also called uniform scaling-based, has traditionally been used, recently developed upper body shape-based mass distribution approach which accounts for inter-subject inherent mass distribution variation within the same body mass index category, has demonstrated sensitivity of muscle forces and joint kinetics during static posture to segmental masses and centers of mass variation. This study investigates the influence of upper body mass distribution on internal and external kinetics computed using a full body musculoskeletal model during level walking in normal-weight healthy individuals. The findings reveal that variations in segmental masses and centers of mass resulting from different mass scaling approaches significantly alters ground reaction force prediction, especially the vertical component, followed by the medio-lateral and antero-posterior components. Joint reaction forces also show sensitivity to variations in personalized mass distribution, particularly the vertical component at the hip, knee, and ankle joints, followed by the medio-lateral and antero-posterior components. These results emphasize the importance of caution when employing subject-specific upper body musculoskeletal models with uniform mass scaling for gait kinetics assessment.},
booktitle = {Proceedings of the 2024 14th International Conference on Biomedical Engineering and Technology},
pages = {140–145},
numpages = {6},
keywords = {Gait Analysis, Musculoskeletal Modeling, Personalization, Upper Body Mass Distribution},
location = {Seoul, Republic of Korea},
series = {ICBET '24}
}

@inproceedings{10.1145/3695080.3695172,
author = {Yu, Zheng-Jun},
title = {Research on Robot Motion Attitude Recognition Models Based on Computer Vision and Re-Unet Networks},
year = {2024},
isbn = {9798400710223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3695080.3695172},
doi = {10.1145/3695080.3695172},
abstract = {With the continuous development of robotics technology and the expansion of application fields, the motion attitude recognition of robots has become an important research direction. Traditional rule- and sensor-based methods have problems such as low recognition accuracy and high influence by environment, so computer vision techniques and deep learning algorithms need to be introduced to improve the accuracy and robustness of recognition. The aim of this study is to construct an efficient robot motion pose recognition model by combining computer vision techniques and Re-Unet network to support the motion control of robots in different application scenarios.},
booktitle = {Proceedings of the 2024 International Conference on Cloud Computing and Big Data},
pages = {535–539},
numpages = {5},
location = {Dali, China},
series = {ICCBD '24}
}

@inproceedings{10.1145/3686490.3686538,
author = {Yu, Yijia and Huang, Wenhao and Zhang, Chen},
title = {A New Centralized License Management Framework based on Internet of Things (IoT-CLMF)},
year = {2024},
isbn = {9798400717192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686490.3686538},
doi = {10.1145/3686490.3686538},
abstract = {With the widespread adoption and maturity of Internet of Things (IoT) technology, government services are undergoing unprecedented changes. More and more government agencies are integrating IoT technology into their daily services, aiming to improve work efficiency, optimize service processes, and provide more convenient and efficient services to the public. IoT technology has been widely used in various fields such as urban management, intelligent manufacturing, public safety, environmental protection, etc., providing the government with a more intelligent and convenient way to provide services. This paper explores the application of IoT technology in the context of the Unified Authentication Center for Government Services, focusing on the role of IoT technology in streamlining the process, improving efficiency and enhancing service quality, and proposes a new centralized license management framework based on IoT devices, fusion of transmission machine, license cabinet, card printer and other hardware devices, through RFID, sensor network and other technologies uploaded in the network big data platform. The new framework can help to build a centralized management system for licenses efficiently, effectively solve the pain points of license management at this stage, and provide new ideas for the development of license management. At last, in actual application scenarios, the validity of the proposed framework was validated.},
booktitle = {Proceedings of the 2024 7th International Conference on Signal Processing and Machine Learning},
pages = {328–334},
numpages = {7},
keywords = {Government Services, Internet of Things (IoT), IoT-CLMF, RFID},
location = {Qingdao, China},
series = {SPML '24}
}

@inproceedings{10.1145/3654777.3676333,
author = {Jain, Gaurav and Hindi, Basel and Zhang, Zihao and Srinivasula, Koushik and Xie, Mingyu and Ghasemi, Mahshid and Weiner, Daniel and Paris, Sophie Ana and Xu, Xin Yi Therese and Malcolm, Michael and Turkcan, Mehmet Kerem and Ghaderi, Javad and Kostic, Zoran and Zussman, Gil and Smith, Brian A.},
title = {StreetNav: Leveraging Street Cameras to Support Precise Outdoor Navigation for Blind Pedestrians},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676333},
doi = {10.1145/3654777.3676333},
abstract = {Blind and low-vision (BLV) people rely on GPS-based systems for outdoor navigation. GPS’s inaccuracy, however, causes them to veer off track, run into obstacles, and struggle to reach precise destinations. While prior work has made precise navigation possible indoors via hardware installations, enabling this outdoors remains a challenge. Interestingly, many outdoor environments are already instrumented with hardware such as street cameras. In this work, we explore the idea of repurposing existing street cameras for outdoor navigation. Our community-driven approach considers both technical and sociotechnical concerns through engagements with various stakeholders: BLV users, residents, business owners, and Community Board leadership. The resulting system, StreetNav, processes a camera’s video feed using computer vision and gives BLV pedestrians real-time navigation assistance. Our evaluations show that StreetNav guides users more precisely than GPS, but its technical performance is sensitive to environmental occlusions and distance from the camera. We discuss future implications for deploying such systems at scale.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {139},
numpages = {21},
keywords = {Visual impairments, computer vision, outdoor navigation, street camera},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{10.1145/3688636.3688660,
author = {Zhao, Jing and Jia, Congsheng and Ni, Lei and Fang, Yuanmeng and Tang, Liang and Yang, Jiechao},
title = {Design and Implementation of Information Sensing System based on Internet of Things in Smart Pasture},
year = {2024},
isbn = {9798400717109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3688636.3688660},
doi = {10.1145/3688636.3688660},
abstract = {Under the current sustainable development strategy, grassland management faces the challenges of overgrazing and livestock germplasm degradation. Due to the lack of scientific awareness of livestock raising, the carrying capacity of grasslands has declined. To solve the problems in the process of breeding and management in pastoral areas, such as the difficulty of accurate counting for huge livestock, insufficient detection for breeding environment, and the limited decision support, this paper proposed an information sensing system (ISS) based on the Internet of Things (IoTs), including the hardware design and the software algorithm test. The proposed system can identify and count animals through the OpenMV camera module, and in addition the temperature and humidity sensors are used to detect the temperature and humidity of the cowshed. What's more, the RFID is used to identify the information of the animal. Finally, the system simulation and testing show that the recognition accuracy of the proposed system can arrive up to 90\% with the OpenMV camera module, and the accuracy of the counting algorithm for the identified animals is 96\%. The error range of temperature detection and humidity detection for the environment is controlled within 0.3℃ and 2\%RH, respectively.},
booktitle = {Proceedings of the 2024 12th International Conference on Communications and Broadband Networking},
pages = {208–213},
numpages = {6},
location = {Nyingchi, China},
series = {ICCBN '24}
}

@inproceedings{10.1145/3654777.3676466,
author = {Cheng, Liqi and Jia, Hanze and Yu, Lingyun and Wu, Yihong and Ye, Shuainan and Deng, Dazhen and Zhang, Hui and Xie, Xiao and Wu, Yingcai},
title = {VisCourt: In-Situ Guidance for Interactive Tactic Training in Mixed Reality},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676466},
doi = {10.1145/3654777.3676466},
abstract = {In team sports like basketball, understanding and executing tactics—coordinated plans of movements among players—are crucial yet complex, requiring extensive practice. These tactics require players to develop a keen sense of spatial and situational awareness. Traditional coaching methods, which mainly rely on basketball tactic boards and video instruction, often fail to bridge the gap between theoretical learning and the real-world application of tactics, due to shifts in view perspectives and a lack of direct experience with tactical scenarios. To address this challenge, we introduce VisCourt, a Mixed Reality (MR) tactic training system, in collaboration with a professional basketball team. To set up the MR training environment, we employed semi-automatic methods to simulate realistic 3D tactical scenarios and iteratively designed visual in-situ guidance. This approach enables full-body engagement in interactive training sessions on an actual basketball court and provides immediate feedback, significantly enhancing the learning experience. A user study with athletes and enthusiasts shows the effectiveness and satisfaction with VisCourt in basketball training and offers insights for the design of future SportsXR training systems.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {32},
numpages = {14},
keywords = {Immersive Training, In-Situ Visualization, Mixed Reality, SportsXR},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{10.1145/3654777.3676443,
author = {Jiang, Yijing and Kleinau, Julia and Eckroth, Till Max and Hoggan, Eve and Mueller, Stefanie and Wessely, Michael},
title = {MouthIO: Fabricating Customizable Oral User Interfaces with Integrated Sensing and Actuation},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676443},
doi = {10.1145/3654777.3676443},
abstract = {This paper introduces MouthIO, the first customizable intraoral user interface that can be equipped with various sensors and output components. MouthIO consists of an SLA-printed brace that houses a flexible PCB within a bite-proof enclosure positioned between the molar teeth and inner cheeks. Our MouthIO design and fabrication technique enables makers to customize the oral user interfaces in both form and function at low cost. All parts in contact with the oral cavity are made of bio-compatible materials to ensure safety, while the design takes into account both comfort and portability. We demonstrate MouthIO through three application examples ranging from beverage consumption monitoring, health monitoring, to assistive technology. Results from our full-day user study indicate high wearability and social acceptance levels, while our technical evaluation demonstrates the device’s ability to withstand adult bite forces.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {1},
numpages = {16},
keywords = {Fabrication, Flexible Circuits, Oral Interface, Wearable Computing},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{10.1145/3654777.3676403,
author = {Park, Junyong and Yang, Saelyne and Jo, Sungho},
title = {Silent Impact: Tracking Tennis Shots from the Passive Arm},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676403},
doi = {10.1145/3654777.3676403},
abstract = {Wearable technology has transformed sports analytics, offering new dimensions in enhancing player experience. Yet, many solutions involve cumbersome setups that inhibit natural motion. In tennis, existing products require sensors on the racket or dominant arm, causing distractions and discomfort. We propose Silent Impact, a novel and user-friendly system that analyzes tennis shots using a sensor placed on the passive arm. Collecting Inertial Measurement Unit sensor data from 20 recreational tennis players, we developed neural networks that exclusively utilize passive arm data to detect and classify six shots, achieving a classification accuracy of 88.2\% and a detection F1 score of 86.0\%, comparable to the dominant arm. These models were then incorporated into an end-to-end prototype, which records passive arm motion through a smartwatch and displays a summary of shots on a mobile app. User study (N=10) showed that participants felt less burdened physically and mentally using Silent Impact on the passive arm. Overall, our research establishes the passive arm as an effective, comfortable alternative for tennis shot analysis, advancing user-friendly sports analytics.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {115},
numpages = {15},
keywords = {Action recognition, IMU, motion analysis, sports, tennis},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{10.1145/3654777.3676329,
author = {Waghmare, Anandghan and Chatterjee, Ishan and Iyer, Vikram and Patel, Shwetak},
title = {WatchLink: Enhancing Smartwatches with Sensor Add-Ons via ECG Interface},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676329},
doi = {10.1145/3654777.3676329},
abstract = {We introduce a low-power communication method that lets smartwatches leverage existing electrocardiogram (ECG) hardware as a data communication interface. Our unique approach enables the connection of external, inexpensive, and low-power "add-on" sensors to the smartwatch, expanding its functionalities. These sensors cater to specialized user needs beyond those offered by pre-built sensor suites, at a fraction of the cost and power of traditional communication protocols, including Bluetooth Low Energy. To demonstrate the feasibility of our approach, we conduct a series of exploratory and evaluative tests to characterize the ECG interface as a communication channel on commercial smartwatches. We design a simple transmission scheme using commodity components, demonstrating cost and power benefits. Further, we build and test a suite of add-on sensors, including UV light, body temperature, buttons, and breath alcohol, all of which achieved testing objectives at low material cost and power usage. This research paves the way for personalized and user-centric wearables by offering a cost-effective solution to expand their functionalities.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {14},
numpages = {13},
keywords = {ECG Sensing, Low-power devices, User-centric wearables, Wearable sensor add-ons},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{10.1145/3654777.3676448,
author = {Roy, Sutirtha and Chowdhury, Moshfiq-Us-Saleheen and Noim, Jurjaan Onayza and Pandey, Richa and Nittala, Aditya Shekhar},
title = {HoloChemie - Sustainable Fabrication of Soft Biochemical Holographic Devices for Ubiquitous Sensing},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676448},
doi = {10.1145/3654777.3676448},
abstract = {Sustainable fabrication approaches and biomaterials are increasingly being used in HCI to fabricate interactive devices. However, the majority of the work has focused on integrating electronics. This paper takes a sustainable approach to exploring the fabrication of biochemical sensing devices. Firstly, we contribute a set of biochemical formulations for biological and environmental sensing with bio-sourced and environment-friendly substrate materials. Our formulations are based on a combination of enzymes derived from bacteria and fungi, plant extracts and commercially available chemicals to sense both liquid and gaseous analytes: glucose, lactic acid, pH levels and carbon dioxide. Our novel holographic sensing scheme allows for detecting the presence of analytes and enables quantitative estimation of the analyte levels. We present a set of application scenarios that demonstrate the versatility of our approach and discuss the sustainability aspects, its limitations, and the implications for bio-chemical systems in HCI.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {111},
numpages = {19},
keywords = {Biochemical devices Sensing, Epidermal Devices, Physiological Sensing, Wearables},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{10.1145/3654777.3676373,
author = {Takahashi, Akifumi and Tanaka, Yudai and Tamhane, Archit and Shen, Alan and Teng, Shan-Yuan and Lopes, Pedro},
title = {Can a Smartwatch Move Your Fingers? Compact and Practical Electrical Muscle Stimulation in a Smartwatch},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676373},
doi = {10.1145/3654777.3676373},
abstract = {Smartwatches gained popularity in the mainstream, making them into today’s de-facto wearables. Despite advancements in sensing, haptics on smartwatches is still restricted to tactile feedback (e.g., vibration). Most smartwatch-sized actuators cannot render strong force-feedback. Simultaneously, electrical muscle stimulation (EMS) promises compact force-feedback but, to actuate fingers requires users to wear many electrodes on their forearms. While forearm electrodes provide good accuracy, they detract EMS from being a practical force-feedback interface. To address this, we propose moving the electrodes to the wrist—conveniently packing them in the backside of a smartwatch. In our first study, we found that by cross-sectionally stimulating the wrist in 1,728 trials, we can actuate thumb extension, index extension \&amp; flexion, middle flexion, pinky flexion, and wrist flexion. Following, we engineered a compact EMS that integrates directly into a smartwatch’s wristband (with a custom stimulator, electrodes, demultiplexers, and communication). In our second study, we found that participants could calibrate our device by themselves &lt;Formula format="inline"&gt;&lt;TexMath&gt;&lt;?TeX $sim 50 \%$?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 1&lt;/AltText&gt;&lt;File name="uist24-51-inline1" type="svg"/&gt;&lt;/Formula&gt; faster than with conventional EMS. Furthermore, all participants preferred the experience of this device, especially for its social acceptability \&amp; practicality. We believe that our approach opens new applications for smartwatch-based interactions, such as haptic assistance during everyday tasks.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {2},
numpages = {15},
keywords = {Electrical muscle stimulation, Force feedback, Smartwatch},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{10.1145/3654777.3676328,
author = {Lu, Yu and Ding, Dian and Pan, Hao and Li, Yijie and Zhou, Juntao and Fu, Yongjian and Zhang, Yongzhao and Chen, Yi-Chao and Xue, Guangtao},
title = {HandPad: Make Your Hand an On-the-go Writing Pad via Human Capacitance},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676328},
doi = {10.1145/3654777.3676328},
abstract = {The convenient text input system is a pain point for devices such as AR glasses, and it is difficult for existing solutions to balance portability and efficiency. This paper introduces HandPad, the system that turns the hand into an on-the-go touchscreen, which realizes interaction on the hand via human capacitance. HandPad achieves keystroke and handwriting inputs for letters, numbers, and Chinese characters, reducing the dependency on capacitive or pressure sensor arrays. Specifically, the system verifies the feasibility of touch point localization on the hand using the human capacitance model and proposes a handwriting recognition system based on Bi-LSTM and ResNet. The transfer learning-based system only needs a small amount of training data to build a handwriting recognition model for the target user. Experiments in real environments verify the feasibility of HandPad for keystroke (accuracy of 100\%) and handwriting recognition for letters (accuracy of 99.1\%), numbers (accuracy of 97.6\%) and Chinese characters (accuracy of 97.9\%).},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {4},
numpages = {16},
keywords = {Handwriting Input, Human Capacitance, Key Stroke},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{10.1145/3654777.3676440,
author = {Zhang, Zining and Li, Jiasheng and Yan, Zeyu and Nishida, Jun and Peng, Huaishu},
title = {JetUnit: Rendering Diverse Force Feedback in Virtual Reality Using Water Jets},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676440},
doi = {10.1145/3654777.3676440},
abstract = {We propose JetUnit, a water-based VR haptic system designed to produce force feedback with a wide spectrum of intensities and frequencies through water jets. The key challenge in designing this system lies in optimizing parameters to enable the haptic device to generate force feedback that closely replicates the most intense force produced by direct water jets while ensuring the user remains dry. In this paper, we present the key design parameters of the JetUnit wearable device determined through a set of quantitative experiments and a perception study. We further conducted a user study to assess the impact of integrating our haptic solutions into virtual reality experiences. The results revealed that, by adhering to the design principles of JetUnit, the water-based haptic system is capable of delivering diverse force feedback sensations, significantly enhancing the immersive experience in virtual reality.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {136},
numpages = {15},
keywords = {VR, force feedback, haptics, water jets},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{10.1145/3686490.3686532,
author = {Liu, Jiayi and Huo, Xingying and Wang, Xuan},
title = {Positioning service based on inertial sensor in edge intelligence environment},
year = {2024},
isbn = {9798400717192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686490.3686532},
doi = {10.1145/3686490.3686532},
abstract = {In the Internet of Things (IoT) space, the ability to determine a user's location is critical to improving everyday convenience through services such as navigation and tracking. How to accomplish the task of obtaining user location information with low cost and high efficiency is the core problem of this research. While the Global Positioning System (GPS) can easily obtain location information, it cannot be applied near obstacles or underground buildings. With the development of inertial sensor technology and edge computing, a safe positioning method based on inertial sensor in edge computing environment is proposed in this paper, and the corresponding indoor positioning system is designed and developed. The orbit thrust algorithm is used to realize the continuous positioning of inertial sensor. The acceleration and heading information can be obtained by using the data returned by the acceleration sensor and the direction sensor, and the required results can be obtained by analyzing and calculating on this basis, but there are many uncertainties in the calculation. The constant motion model is used to locate the system. The experiment proves that the system is feasible and can realize more accurate continuous positioning.},
booktitle = {Proceedings of the 2024 7th International Conference on Signal Processing and Machine Learning},
pages = {282–288},
numpages = {7},
keywords = {Edge Computing, Inertial sensors, Safe positioning},
location = {Qingdao, China},
series = {SPML '24}
}

@inproceedings{10.1145/3654777.3676461,
author = {Xu, Vasco and Gao, Chenfeng and Hoffmann, Henry and Ahuja, Karan},
title = {MobilePoser: Real-Time Full-Body Pose Estimation and 3D Human Translation from IMUs in Mobile Consumer Devices},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676461},
doi = {10.1145/3654777.3676461},
abstract = {There has been a continued trend towards minimizing instrumentation for full-body motion capture, going from specialized rooms and equipment, to arrays of worn sensors and recently sparse inertial pose capture methods. However, as these techniques migrate towards lower-fidelity IMUs on ubiquitous commodity devices, like phones, watches, and earbuds, challenges arise including compromised online performance, temporal consistency, and loss of global translation due to sensor noise and drift. Addressing these challenges, we introduce MobilePoser, a real-time system for full-body pose and global translation estimation using any available subset of IMUs already present in these consumer devices. MobilePoser employs a multi-stage deep neural network for kinematic pose estimation followed by a physics-based motion optimizer, achieving state-of-the-art accuracy while remaining lightweight. We conclude with a series of demonstrative applications to illustrate the unique potential of MobilePoser across a variety of fields, such as health and wellness, gaming, and indoor navigation to name a few.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {70},
numpages = {11},
keywords = {Motion capture, inertial measurement units, mobile devices, sensors},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{10.1145/3654777.3676344,
author = {Zhao, Junyi and Preechayasomboon, Pornthep and Christensen, Tyler and Memar, Amirhossein H. and Shen, Zhenzhen and Colonnese, Nicholas and Khbeis, Michael and Zhu, Mengjia},
title = {TouchpadAnyWear: Textile-Integrated Tactile Sensors for Multimodal High Spatial-Resolution Touch Inputs with Motion Artifacts Tolerance},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676344},
doi = {10.1145/3654777.3676344},
abstract = {This paper presents TouchpadAnyWear, a novel family of textile-integrated force sensors capable of multi-modal touch input, encompassing micro-gesture detection, two-dimensional (2D) continuous input, and force-sensitive strokes. This thin (&lt;1.5&nbsp;mm) and conformal device features high spatial resolution sensing and motion artifact tolerance through its unique capacitive sensor architecture. The sensor consists of a knitted textile compressive core, sandwiched by stretchable silver electrodes, and conductive textile shielding layers on both sides. With a high-density sensor pixel array (25/cm2), TouchpadAnyWear can detect touch input locations and sizes with millimeter-scale spatial resolution and a wide range of force inputs (0.05&nbsp;N to 20&nbsp;N). The incorporation of miniature polymer domes, referred to as “poly-islands”, onto the knitted textile locally stiffens the sensing areas, thereby reducing motion artifacts during deformation. These poly-islands also provide passive tactile feedback to users, allowing for eyes-free localization of the active sensing pixels. Design choices and sensor performance are evaluated using in-depth mechanical characterization. Demonstrations include an 8-by-8 grid sensor as a miniature high-resolution touchpad and a T-shaped sensor for thumb-to-finger micro-gesture input. User evaluations validate the effectiveness and usability of TouchpadAnyWear in daily interaction contexts, such as tapping, forceful pressing, swiping, 2D cursor control, and 2D stroke-based gestures. This paper further discusses potential applications and explorations for TouchpadAnyWear in wearable smart devices, gaming, and augmented reality devices.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {66},
numpages = {14},
keywords = {Capacitive, Fabrication, Gesture Recognition, High Resolution, Motion Artifacts, Multimodal, Printing, Soft Wearable, Tactile Display, Textile, Touch Sensor},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{10.1145/3654777.3676418,
author = {Wu, Erwin and Khirodkar, Rawal and Koike, Hideki and Kitani, Kris},
title = {SolePoser: Full Body Pose Estimation using a Single Pair of Insole Sensor},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676418},
doi = {10.1145/3654777.3676418},
abstract = {We propose SolePoser, a real-time 3D pose estimation system that leverages only a single pair of insole sensors. Unlike conventional methods relying on fixed cameras or bulky wearable sensors, our approach offers minimal and natural setup requirements. The proposed system utilizes pressure and IMU sensors embedded in insoles to capture the body weight’s pressure distribution at the feet and its 6 DoF acceleration. This information is used to estimate the 3D full-body joint position by a two-stream transformer network. A novel double-cycle consistency loss and a cross-attention module are further introduced to learn the relationship between 3D foot positions and their pressure distributions. We also introduced two different datasets of sports and daily exercises, offering 908k frames across eight different activities. Our experiments show that our method’s performance is on par with top-performing approaches, which utilize more IMUs and even outperform third-person-view camera-based methods in certain scenarios.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {67},
numpages = {9},
keywords = {Insole sensor, foot pressure, motion capture, pose estimation},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{10.1145/3686490.3686495,
author = {Zhu, Yan and Liang, Quanquan and Cui, Kaile},
title = {Fresnel Zone Theory Based Non-Line-of-Sight Respiration Behavior Detection Using USRP},
year = {2024},
isbn = {9798400717192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686490.3686495},
doi = {10.1145/3686490.3686495},
abstract = {Respiration is one of the most important metabolic activities of the living system, and the detection of respiratory status is an important method for the prevention and diagnosis of human diseases. In this paper, a real-time breath detection system based on a universal software radio peripheral (USRP) is designed. With the support of Fresnel zone theory, the amplitude information of OFDM generated multi-subcarriers is collected to detect breathing. We built a respiratory detection system in a real environment, using the hampel function to process the outliers of the collected channel state information (CSI) data after wavelet filtering for noise reduction and smoothing of CSI data, and importing the above pre-processed data into the feature extraction and machine learning algorithms for amplitude map classification. It was found that the accuracy of SVM classification using a combination of both HOG and GLCM features could reach more than 97.5\% when judging a respiratory condition alone. However, the accuracy of the method is not satisfactory for classifying more than two cases. In this regard, we can modify the feature extraction method and the classification algorithm. The KNN accuracy is up to 91.67\% and Tree accuracy to 95\% based on LBP features; for HOG features, the KNN accuracy reaches up to 91.67\% and NB accuracy reaches up to 96.67\%; under Gabor features, the KNN accuracy comes to 94.17\% and NB accuracy to 95\%. Our work shows that real-time sensing of normal breathing as well as interval breathing by measuring CSI is feasible in real-world settings.},
booktitle = {Proceedings of the 2024 7th International Conference on Signal Processing and Machine Learning},
pages = {34–41},
numpages = {8},
keywords = {Fresnel zone theory, USRP, amplitude information, machine learning, respiration detection},
location = {Qingdao, China},
series = {SPML '24}
}

@article{10.1145/3696003,
author = {Zhu, Shuai and Voigt, Thiemo and Rahimian, Fatemeh and Ko, Jeonggil},
title = {On-device Training: A First Overview on Existing Systems},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {6},
issn = {1550-4859},
url = {https://doi.org/10.1145/3696003},
doi = {10.1145/3696003},
abstract = {The recent breakthroughs in machine learning (ML) and deep learning (DL) have catalyzed the design and development of various intelligent systems over wide application domains. While most existing machine learning models require large memory and computing power, efforts have been made to deploy some models on resource-constrained devices as well. A majority of the early application systems focused on exploiting the inference capabilities of ML and DL models, where data captured from different mobile and embedded sensing components are processed through these models for application goals such as classification and segmentation. More recently, the concept of exploiting the mobile and embedded computing resources for ML/DL model training has gained attention, as such capabilities allow (i) the training of models via local data without the need to share data over wireless links, thus enabling privacy-preserving computation by design, (ii) model personalization and environment adaptation, and (iii) deployment of accurate models in remote and hardly accessible locations without stable internet connectivity. This work summarizes and analyzes state-of-the-art systems research that allows such on-device model training capabilities and provides a survey of on-device training from a systems perspective.},
journal = {ACM Trans. Sen. Netw.},
month = oct,
articleno = {118},
numpages = {39},
keywords = {Machine learning, IoT devices, on-device training}
}

@inproceedings{10.1145/3675094.3678365,
author = {Dotch, Emani},
title = {AudioBuddy: An Assistive Technology for People with Noise Sensitivity and Their Care Networks},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678365},
doi = {10.1145/3675094.3678365},
abstract = {Noise sensitivity is prevalent in both neurodivergent and neurotypical people, making self-regulation challenging and impacting their quality of life. Little work has been done in the wearable and ubiquitous computing space to support these particular experiences. My research explores how people with noise sensitivity (PWNS) and those around them manage and regulate their own and other's reactions to noise. Leveraging wearable and mobile devices, this work explores the management and regulation behaviors of PWNS and their care networks. It describes the design and deployment process of a novel application that utilizes wearable sensor technology to sense physiological and environmental signals, facilitate awareness and information sharing, and present users with strategies for managing and regulating noise sensitivity experiences. This research strives to contribute an understanding of noise sensitivity experiences, inclusive design practices, and a technological solution to support people with noise sensitivity.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {267–271},
numpages = {5},
keywords = {assistive technology, collaborative technology, noise sensitivity, wearable and sensing technology},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3678484,
author = {Brilka, Michael and Van Laerhoven, Kristof},
title = {Historiographer: An Efficient Long Term Recording of Real Time Data on Wearable Microcontrollers},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678484},
doi = {10.1145/3675094.3678484},
abstract = {Data collection is a core principle in the scientific and medical environment. To record study participants in daily life situations wearables can be used. These should be small enough to not disrupt the lifestyle of the participants, while delivering sensor data in an accurate and efficient way. This ensures a long recording time for these battery-powered devices. Current purchasable wearable devices, would lend themselves well for wearable studies. Simpler devices have many drawbacks: Low sampling rate, for energy efficiency and little support are some drawbacks. More advanced devices have a high-frequent sampling rate of sensor data. These however, have a higher price and a limited support time. Our work introduces an open-source app for cost-effective, high-frequent, and long-term recording of sensor data. We based the development on the Bangle.js 2, which is a prevalent open-source smartwatch. The code has been optimised for efficiency, using sensor-specific properties to store sensor data in a compressed, loss-less, and time-stamped form to the local NAND-storage. We show in our experiments that we have the ability to record PPG-data at 50 Hertz for at least half a day. With other configurations we can record multiple sensors with a high-frequent update interval for a full day.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {934–938},
numpages = {5},
keywords = {long term recording, open-source smartwatch, wearable},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3678420,
author = {Zhang, Tianyi and Teng, Songyan and Jia, Hong and D'Alfonso, Simon},
title = {Leveraging LLMs to Predict Affective States via Smartphone Sensor Features},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678420},
doi = {10.1145/3675094.3678420},
abstract = {As mental health issues for young adults present a pressing public health concern, daily digital mood monitoring for early detection has become an important prospect. An active research area, digital phenotyping, involves collecting and analysing data from personal digital devices such as smartphones (usage and sensors) and wearables to infer behaviours and mental health. Whilst this data is standardly analysed using statistical and machine learning approaches, the emergence of large language models (LLMs) offers a new approach to make sense of smartphone sensing data. Despite their effectiveness across various domains, LLMs remain relatively unexplored in digital mental health, particularly in integrating mobile sensor data. Our study aims to bridge this gap by employing LLMs to predict affect outcomes based on smartphone sensing data from university students. We demonstrate the efficacy of zero-shot and few-shot embedding LLMs in inferring general wellbeing. Our findings reveal that LLMs can make promising predictions of affect measures using solely smartphone sensing data. This research sheds light on the potential of LLMs for affective state prediction, emphasizing the intricate link between smartphone behavioral patterns and affective states. To our knowledge, this is the first work to leverage LLMs for affective state prediction and digital phenotyping tasks.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {709–716},
numpages = {8},
keywords = {digital phenotyping, large language models, mental wellbeing, mobile sensing, ubiquitous computing},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3678479,
author = {Kamminga, Jacob and van der Duim, Rob and ten Hove, Erik},
title = {M-MOVE-IT: Multimodal Machine Observation and Video-Enhanced Integration Tool for Data Annotation},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678479},
doi = {10.1145/3675094.3678479},
abstract = {M-MOVE-IT is an open-source framework that simplifies data acquisition, annotation, and AI training for wearable technology. It addresses the challenges of synchronizing video and IMU data, making it easier to develop AI models for healthcare, sports, wildlife monitoring, anti-poaching, and livestock management applications. The framework automates and streamlines managing sensors, subjects, and deployments, synchronizing data, and annotating activities. M-MOVE-IT uses the real-time clocks of sensors and an offset annotation step to achieve precise synchronization, automatically parses sensor metadata, and generates annotation tasks. The export module provides data in JSON format for easy use in AI training. M-MOVE-IT's design supports active learning and human-in-the-loop development, enhancing the efficiency and scalability of wearable technology research.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {911–915},
numpages = {5},
keywords = {active learning, activity recognition, multimodal ai, multimodal annotation, sensor fusion},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3678364,
author = {Li, Richard},
title = {Improving Medical Outcomes through At-Home, Longitudinal Health Monitoring},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678364},
doi = {10.1145/3675094.3678364},
abstract = {Ubiquitous Computing and Human-Computer Interaction health researchers motivate their work with the promise of improving health outcomes for patients. Although computing devices are producing more health data than ever, turning this data into actionable insights remains a challenge. In this Doctoral Colloquium submission, I argue that to improve health outcomes, medical systems must not only produce health data, but also provide interpretations of that health data personalized to the individual in order to deliver effective, actionable feedback. To illustrate this approach, I introduce two projects: Beacon and ExerciseRx-CP. Beacon is a system that screens for minimal hepatic encephalopathy through a novel critical flicker frequency (CFF) measure. I use Beacon's CFF measure as an illustration of defining ways to interpret new health data by incorporating personal baselines and longitudinal measurements. ExerciseRx-CP is a system that uses real-time feedback of exercise tracking to encourage physical activity among adolescents with cerebral palsy. I use ExerciseRx-CP as a way to show how motion data can be interpreted on multiple levels (e.g., raw sensor signals, subrepetition motion, and aggregated repetition metrics) to deliver different feedback mechanisms. Finally, I describe how an enabling activities framework can help Ubicomp researchers characterize activities necessary for translational research, promote more translational research to be conducted our community, and ultimately create greater impact as a field.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {262–266},
numpages = {5},
keywords = {health data, interactive systems, translational research},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3678373,
author = {Zhao, Yiran and Choudhury, Tanzeem},
title = {Evaluate Closed-Loop, Mindless Intervention in-the-Wild: A Micro-Randomized Trial on Offset Heart Rate Biofeedback},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678373},
doi = {10.1145/3675094.3678373},
abstract = {The vision of closed-loop intervention systems for behavioral health is growing with the flourishing of mobile sensors and multimodal data. There has been abundant work on identifying symptoms, diagnosis, and progression monitoring. However, there has been limited effort in intervention research, tailoring suitable interventions for closed-loop systems. About a decade ago, researchers began exploring mindless interventions -- subtle interventions to change behavior, cognition, or affect with minimal attention and effort. Despite their success in controlled laboratory settings, few mindless interventions have been deployed in the real world, and none have been integrated into closed-loop systems. Thus, it remains unclear how well these low-effort, low-attention interventions integrate with sensing systems, how their effectiveness varies over time and context, and their overall impact on behavioral health management. This study is the first to deploy mindless interventions in a closed-loop system in real-world settings. We developed a closed-loop intervention for individuals with moderate to severe anxiety, delivering offset heart rate biofeedback when stress symptoms are detected. This paper presents our work-in-progress, detailing the system and study design, and highlighting this research's methodological and empirical contributions.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {307–312},
numpages = {6},
keywords = {anxiety, behavioral health, closed-loop intervention, closed-loop system, general anxiety disorder, health and behavior change technology, micro-randomized trial, mindless computing, mindless intervention, offset heart rate biofeedback, stress, ubiquitous and mobile computing, wearable computing},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3678497,
author = {Mukhopadhyay, Shalini and Sharma, Varsha and Jaiswal, Dibyanshu and Dey, Swarnava and Ghose, Avik},
title = {TinyStressNAS: Automated Feature Selection and Model Generation for On-device Stress Detection},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678497},
doi = {10.1145/3675094.3678497},
abstract = {Over the past decades, Electrodermal Activity (EDA)-based stress detection systems have observed increasing interest due to the significant impact of stress on daily life and the need for continuous, unobtrusive monitoring. The wearable form-factor and the unobtrusive nature of the EDA sensor make it highly suitable for this purpose. Many researchers have focused on developing EDA-based stress detection techniques, but few consider the resource footprint of such systems. Maintaining a good trade-off by manual optimization is a challenging task. Automation techniques can be very helpful in such scenarios. In this work, we introduce a novel method for the selection of an optimal feature subset, utilizing a deep Reinforcement Learning (RL) network on top of a Neural Architecture Search (NAS) framework to design tiny, customized models for on-device stress detection. The feature subset selection uses a multi-metric reward objective function to balance feature accuracy contributions against computational complexity, identifying the optimal feature subset. Initial analysis indicates that this optimal subset enhances accuracy while keeping the model size under 100 kB and minimally increasing computation.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {430–436},
numpages = {7},
keywords = {edge computing, feature selection, nas, stress monitoring, wearable sensing},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3678440,
author = {Fujii, Atsuhiro and Murao, Kazuya},
title = {Water Level Recognition by Analyzing the Sound when Pouring Water},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678440},
doi = {10.1145/3675094.3678440},
abstract = {In our daily lives, we often pour liquids into containers, and it can be challenging to accurately gauge the liquid level in opaque, narrow-mouthed containers. Visual inspection is not always reliable and can lead to spillage. Using a liquid-level sensor is one option, but it requires attaching a device to each container, which is complicated. We propose a method that uses sound to estimate the water level inside a container. This method is versatile and does not require installing a device for each container. We conducted evaluation experiments, and the results showed that the estimation accuracy averaged 0.462 for bottle-dependent and 0.308 for bottle-independent estimation models when estimating water levels from 0\% to 100\% in 10\% increments. Additionally, the average estimation accuracy for determining whether the water level is above 90\% was 0.744, even when using a bottle-independent estimation model. These results suggest that while there is room for improvement in the estimation accuracy, the proposed method has potential applications in natural environments for overflow detection. In the future, we plan to develop a faucet-mounted device with a function to stop water pouring just before the container fills up.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {464–469},
numpages = {6},
keywords = {acoustic recognition, acoustic sensing, faucet, smart device, smart home, ubiquitous computing, water level},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3678428,
author = {Gruenerbl, Agnes and Bahle, Gernot and Lukowicz, Paul},
title = {Challenging the Standards of Mental Care: An Analysis of Self-Rating with respect to Sensor based State Detection},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678428},
doi = {10.1145/3675094.3678428},
abstract = {Despite all the developments in AI and Ubiquitous computing, man- agement of mental health and mental disorders mainly relies on human assessments, like daily self-reporting and standard psy- chological questionnaires. Self-reporting however, in addition to compliance issues, comes with the drawback of being subjective and thus often inaccurate. In a depressive episode, it is hard to recall the manic phase of the last weeks. Thus, mainly experienced and self-aware patients can use self-reports effectively. Even though, in the last 15 years, sensor-based objective algorithms to monitor mental disorders showed promising results [3 ],[1], such systems are not established in psychiatric care. We believe it would help psychi- atrists and patients greatly to consider using objective sensor-based support systems if it would be possible to visualize the drawbacks of self-reporting and the abilities of sensor-based analysis. Our work provides a direct comparison of the performance of sensor-based analysis, self-reporting, and psychological diagnosis. It is based on a real-life data set collected with psychiatric patients. It consists of smartphone sensor data, respective daily self-reporting ques- tionnaires, and a ground truth of standardized psychiatric scale tests. As a highlight of this work, in the evaluation, we can provide evidence that observations of deferred self-perception of patients concerning their mental states, as doctors reported to us, can be measured in the sensor data.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {761–765},
numpages = {5},
keywords = {deferred self-perception, management of mental health, sensor-based state recognition},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3680525,
author = {Zhao, Haibin and R\"{o}ddiger, Tobias and Feng, Yufei and Beigl, Michael},
title = {Fit2Ear: Generating Personalized Earplugs from Smartphone Depth Camera Images},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3680525},
doi = {10.1145/3675094.3680525},
abstract = {Earphones, due to their deep integration into daily life, have been developed for unobtrusive and ubiquitous health monitoring. However, these advanced algorithms greatly rely on the high quality sensing data. However, the data collected with universal earplugs could potentially generate undesirable noise, such as vibrations or even falling off. As a result, the algorithms may exhibit limited performance. In this regard, we build a dataset containing RGBD and IMU data captured by a smartphone. To provide a precise and solid ground truth, we employ additional control information from a robotic arm that holds the smartphone scanning ears along a predefined trajectory. With this dataset, we propose a tightly coupled information fusion algorithm for the ground truth ear modeling. Finally, we fabricate the earplugs and conduct an end-to-end evaluation of the wearability of the modeled earplugs in a user study.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {679–684},
numpages = {6},
keywords = {3d modeling, customization, earable, earplug, sensor fusion},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3678374,
author = {Seong, Minwoo and Kim, Gwangbin and Lee, Jaehee and DelPreto, Joseph and Matusik, Wojciech and Rus, Daniela and Kim, SeungJun},
title = {Intelligent Seat: Tactile Signal-Based 3D Sitting Pose Inference},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678374},
doi = {10.1145/3675094.3678374},
abstract = {Owing to people spending a large portion of their day sitting while working, commuting, or relaxing, monitoring their sitting posture is crucial for the development of adaptive interventions that respond to the user's pose, state, and behavior. This is because posture is closely linked to actions, health, attention, and engagement levels. The existing systems for posture estimation primarily use computer vision-based measurements or body-attached sensors; however, they are plagued by challenges such as privacy concerns, occlusion issues, and user discomfort. To address these drawbacks, this study proposed a posture-inference system that uses high-density piezoresistive sensors for joint reconstruction. Tactile pressure data were collected from six individuals, each performing seven different postures 20 times. The proposed system achieved an average L2 distance of 20.2 cm in the joint position reconstruction with a posture classification accuracy of 96.3\%. Future research will focus on the development of a system capable of providing real-time feedback to help users maintain the correct sitting posture.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {791–796},
numpages = {6},
keywords = {human pose reconstruction, sitting posture, tactile sensor},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3678391,
author = {Cui, MingYu and Yuan, Chao and Liu, Yujia and Zheng, Yingying},
title = {More Than Shapes: Exploring the Tactile Parameters of Art Appreciation for the Visually Impaired},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678391},
doi = {10.1145/3675094.3678391},
abstract = {Visual art appreciation for the visually impaired is not only a personal need, but also a need for equal social development. Our study investigates how tactile sensations can compensate for visual impairments in art appreciation. We assembled a focus group of experts to translate visual art elements into tactile equivalents by applying theories of art knowledge and appreciation. We deconstructed paintings into five primary elements-composition, content, color, light and shadow, brushstroke-and two secondary elements-style and emotion. These elements were then converted into tactile paintings for a series of Tactile Aesthetic Workshops aimed at visually impaired participants. The results showed that these tactile representations effectively allowed visually impaired individuals to appreciate art, with a success rate of 78.27\%. Moving forward, we plan to enhance this approach by integrating olfactory and auditory elements to create a more immersive augmented reality (AR) art appreciation environment for the visually impaired.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {786–790},
numpages = {5},
keywords = {art appreciation, multimodal, tactile interaction, visually impaired},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3678448,
author = {Arakawa, Riku and Goel, Mayank},
title = {Unified Framework for Procedural Task Assistants powered by Human Activity Recognition},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678448},
doi = {10.1145/3675094.3678448},
abstract = {Context awareness is key to developing intelligent voice assistants that offer situated support for users performing various daily tasks, like cooking and machine use. Human Activity Recognition (HAR) with various sensors can be a powerful approach to provide assistants with user context, i.e., what they are doing within the task procedure. This workshop paper introduces PrISM---Procedural Interaction from Sensing Module---, a framework tool to develop and evaluate assistants for procedural tasks that utilize the context gained through HAR. The framework consists of several modules: data collection, HAR, postprocessing HAR outputs with task knowledge represented as a graph structure, and situated interactions such as step reminder and question answering. The framework is developed to be generalizable to different procedural tasks and input sensor sets (e.g., smartwatch, wearable camera, ambient sensor, etc). Developers and designers can use the framework to register a new task, collect an initial dataset, gauge expected accuracy regarding step tracking and interactions, and deploy the pipeline. We demonstrate example use cases in the cooking task and discuss future work.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {513–518},
numpages = {6},
keywords = {context-aware, human-ai interaction, procedure tracking, task assistance},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3678468,
author = {Bian, Sizhen and Strahinja, Rakita and Schilk, Philipp and Marc-Andr\'{e}, Cl\'{e}nin and Cortesi, Silvano and Dheman, Kanika and Reinschmidt, Elio and Magno, Michele},
title = {Earable and Wrist-worn Setup for Accurate Step Counting Utilizing Body-Area Electrostatic Sensing},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678468},
doi = {10.1145/3675094.3678468},
abstract = {Step-counting has been widely implemented in wrist-worn devices and is accepted by end users as a quantitative indicator of everyday exercise. However, existing counting approach (mostly on wrist-worn setup) lacks robustness and thus introduces inaccuracy issues in certain scenarios like brief intermittent walking bouts and random arm motions or static arm status while walking (no clear correlation of motion pattern between arm and leg). This paper proposes a low-power step-counting solution utilizing the body area electric field acquired by a novel electrostatic sensing unit, consuming only 87.3 µW of power, hoping to strengthen the robustness of current dominant solution. We designed two wearable devices for on-the-wrist and in-the-ear deployment and collected body-area electric field-derived motion signals from ten volunteers. Four walking scenarios are considered: in the parking lot/shopping center with/without pushing the shopping trolley. The step-counting accuracy from the prototypes shows better accuracy than the commercial wrist-worn devices (e.g.,96\% of the wrist- and ear-worn prototype vs. 66\% of the Fitbit when walking in the shopping center while pushing a shopping trolley). We finally discussed the potential and limitations of sensing body-area electric fields for wrist-worn and ear-worn step-counting and beyond.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {904–910},
numpages = {7},
keywords = {body capacitance, earable, electric field, qvar, step counting, wearable},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3678386,
author = {Lu, Yu and Ding, Dian and Wang, Ran and Xue, Guangtao},
title = {HCMG: Human-Capacitance based Micro Gesture for VR/AR},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678386},
doi = {10.1145/3675094.3678386},
abstract = {Hand-tracking technology is a pivotal input method in augmented and virtual reality environments, providing enhanced interaction accuracy through micro-gesture recognition. This allows users to control devices with minimal knuckle movements, ensuring privacy and accessibility for individuals with mobility impairments. Building on the foundation of human capacitance, this paper introduces a novel approach termed human capacitance-based micro gesture (HCMG) recognition. This system employs capacitive sensors integrated within the inner lining of a wrist guard, capable of detecting subtle changes in skin-to-electrode contact caused by finger joint movements. Our approach leverages the inherent properties of human capacitance to facilitate accurate and efficient micro-gesture recognition. HCMG achieves recognition of five common micro gestures with an accuracy of 95.0\%, providing a promising solution to address the limitations of existing techniques.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {766–770},
numpages = {5},
keywords = {human capacitance, micro gesture},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3678457,
author = {Wang, Hao and Huang, Huazhen and Wang, Jinfeng and Sun, Fangmin, Professor},
title = {Sussex-Huawei Locomotion Recognition Using Machine Learning and Deep Learning with Multi-sensor data},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678457},
doi = {10.1145/3675094.3678457},
abstract = {Human activity recognition (HAR) has developed rapidly in recent years due to its widespread applications in motion analysis, mobile health monitoring, security, and rehabilitation. However, due to missing sensor data, complex application scenarios, poor model robustness, existing HAR algorithms still cannot meet application requirements. In this context, the Sussex-Huawei Locomotion (SHL) recognition challenge provides a dataset for improving HAR algorithms. In this study, our team (SIAT-BIT) proposes a three-branch convolutional neural network framework for SHL recognition challenge. Firstly, the data is preprocessed for feature extraction, and then three classifiers are trained in parallel using three cross-entropy loss functions. The experimental results show that the proposed model achieves the best performance with the least model parameters. In addition, we further improved the performance through post-smoothing. Finally, we get an average accuracy of 0.9274 on the validation dataset.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {563–568},
numpages = {6},
keywords = {deep learning, human activity recognition, machine learning, shl dataset, smartphone},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3678488,
author = {Liu, Xuanyu and Liu, Haoxian and Li, Jiao and Yang, Zongqi and Huang, Yi and Zhang, Jin},
title = {AcousAF: Acoustic Sensing-Based Atrial Fibrillation Detection System for Mobile Phones},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678488},
doi = {10.1145/3675094.3678488},
abstract = {Atrial fibrillation (AF) is characterized by irregular electrical impulses originating in the atria, which can lead to severe complications and even death. Due to the intermittent nature of the AF, early and timely monitoring of AF is critical for patients to prevent further exacerbation of the condition. Although ambulatory ECG Holter monitors provide accurate monitoring, the high cost of these devices hinders their wider adoption. Current mobile-based AF detection systems offer a portable solution. However, these systems have various applicability issues, such as being easily affected by environmental factors and requiring significant user effort. To overcome the above limitations, we present AcousAF, a novel AF detection system based on acoustic sensors of smartphones. Particularly, we explore the potential of pulse wave acquisition from the wrist using smartphone speakers and microphones. In addition, we propose a well-designed framework comprised of pulse wave probing, pulse wave extraction, and AF detection to ensure accurate and reliable AF detection. We collect data from 20 participants utilizing our custom data collection application on the smartphone. Extensive experimental results demonstrate the high performance of our system, with 92.8\% accuracy, 86.9\% precision, 87.4\% recall, and 87.1\% F1 Score.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {377–383},
numpages = {7},
keywords = {acoustic sensing, atrial fibrillation detection, mobile health},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3678441,
author = {Takahashi, Daiki and Murao, Kazuya},
title = {A System to Visualize Differences in Paddling Timing between Teammates in Rowing},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678441},
doi = {10.1145/3675094.3678441},
abstract = {The use of wearable devices and motion sensors has had a significant impact on sports. In this study, we propose a method to improve rowing performance by analyzing 3-axis acceleration data from smartwatches worn by rowers. The goal is to enhance synchronization and timing among multiple rowers on the same boat, which is crucial for competitive rowing. We conducted experiments using a Concept2 rowing ergometer and collected data from rowers wearing Apple Watches. The collected data was processed through filtering, peak detection, and distance calculations (DTW and Euclidean). Our evaluation demonstrated that the proposed method effectively identified discrepancies, providing a quantitative basis for performance improvement. The results emphasize the potential of wearable technology in providing precise, real-time feedback to rowers, ultimately assisting in better coordination and technique refinement.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {470–474},
numpages = {5},
keywords = {accelerometer, rowing, smart watch, wearable computing},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3675094.3678481,
author = {Sepanosian, Thomas and Durmaz Incel, Ozlem},
title = {Training Smarter with OpenEarable: A Boxing Gesture Recognition Dashboard Integration},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678481},
doi = {10.1145/3675094.3678481},
abstract = {Earables, wearable devices worn around the ear, offer new possibilities for sports applications requiring precise head movement analysis, such as boxing. However, boxing-specific gesture recognition using IMU sensors integrated into earables remains underexplored. This work addresses this gap by investigating the potential of the open-source OpenEarable platform for real-time recognition of defensive boxing manoeuvres, including slipping, rolling and pulling back. We propose an extension to OpenEarable, integrating a Python server that leverages machine learning and dynamic time warping for gesture recognition. Furthermore, the web dashboard is enhanced to enable server communication and implement a gesture mirroring feature, providing real-time visual feedback. Real-time testing achieved a high accuracy of 96\%, with feedback delivered within one second. All the system components are made available in a GitHub repository.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {921–924},
numpages = {4},
keywords = {earables, inertial measurement unit, real-time head gesture recognition},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3691016.3691041,
author = {Wang, Luhe and Wang, Yukai},
title = {Multi-UAV cooperative positioning technology based on visual sensors and UWB},
year = {2024},
isbn = {9798400710285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691016.3691041},
doi = {10.1145/3691016.3691041},
abstract = {Unmanned aerial vehicle (UAV) technology are widely utilized, but in a non-satellite navigation environment or a weak satellite navigation environment, the reliability of unmanned aerial vehicles drops severely, and this is especially the case for swarms. In order to achieve swarm navigation under the condition of no satellite navigation, this paper have developed a small but fully autonomous UAV based on visual sensors. Then, aiming at the problem that the positioning error continuously increases when only relying on visual positioning during the flight of multiple UAVs, this paper proposes an inter-aircraft positioning mutual correction algorithm based on Ultra Wideband (UWB) ranging. In a constrained environment, the stable maintenance of the swarm formation can also be achieved. A variety of real-world field experiments have proved the scalability of our system and the feasibility of the inter-aircraft correction technology.},
booktitle = {Proceedings of the 2024 International Conference on Image Processing, Intelligent Control and Computer Engineering},
pages = {146–151},
numpages = {6},
location = {Qingdao, China},
series = {IPICE '24}
}

@inproceedings{10.1145/3687311.3687410,
author = {Li, Jia and Yang, Chengfeng and Zhang, Fengyun},
title = {Application and Development of High-precision Positioning and Accurate Detection Based on UWB in the Field of Intelligent Sensing},
year = {2024},
isbn = {9798400709920},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3687311.3687410},
doi = {10.1145/3687311.3687410},
abstract = {Ultra Wideband (UWB) is a pivotal wireless communication technology that has become integral in applications requiring precise ranging, high-accuracy positioning, and non-invasive detection. Exhibiting clear advantages in wearable smart devices and intelligent sensing, UWB distinguishes itself from conventional wireless technologies such as Wi-Fi, Bluetooth, RFID, LoRa, and ZigBee by offering superior performance in terms of ranging accuracy, positioning precision, sensing sensitivity, and overall reliability. This paper provides a comprehensive analysis of the fundamental principles, methodologies, and applications of UWB technology. Initially, it elucidates the basic principles that underpin UWB technology. Subsequently, it explores UWB-based methods for ranging and positioning, highlighting their significance in enhancing location accuracy and reliability. Furthermore, this study delves into UWB-based detection techniques, showcasing their capability in non-invasive probing and intelligent sensing. Additionally, the paper discusses the advantages of UWB in high-performance wireless transmission and its potential in revolutionizing fields such as high-fidelity audio and video transmission, integrated precise positioning, and smart healthcare. The discussion extends to the technological characteristics, developmental trends, and current applications of UWB, proposing potential future research directions. The continued advancement and standardization of UWB technology suggest its burgeoning role across a spectrum of Internet of Things (IoT) domains, including smart homes, intelligent wearables, and connected vehicles, heralding a new era of smart, interconnected technology landscapes.},
booktitle = {Proceedings of the 2024 International Conference on Intelligent Education and Computer Technology},
pages = {553–559},
numpages = {7},
location = {Guilin, China},
series = {IECT '24}
}

@inproceedings{10.1145/3641512.3686388,
author = {Ni, Tao and Sun, Zehua and Han, Mingda and Xie, Yaxiong and Lan, Guohao and Li, Zhenjiang and Gu, Tao and Xu, Weitao},
title = {REHSense: Towards Battery-Free Wireless Sensing via Radio Frequency Energy Harvesting},
year = {2024},
isbn = {9798400705212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641512.3686388},
doi = {10.1145/3641512.3686388},
abstract = {Diverse Wi-Fi-based wireless applications have been proposed, ranging from daily activity recognition to vital sign monitoring. Despite their remarkable sensing accuracy, the high energy consumption and the requirement for customized hardware modification hinder the wide deployment of the existing sensing solutions. In this paper, we propose REHSense, an energy-efficient wireless sensing solution based on Radio-Frequency (RF) energy harvesting. Instead of relying on a power-hungry Wi-Fi receiver, REHSense leverages an RF energy harvester as the sensor and utilizes the voltage signals harvested from the ambient Wi-Fi signals to enable simultaneous context sensing and energy harvesting. We design and implement REHSense using a commercial-off-the-shelf (COTS) RF energy harvester. Extensive evaluation of three fine-grained wireless sensing tasks (i.e., respiration monitoring, human activity recognition, and hand gesture recognition) shows that REHSense can achieve comparable sensing accuracy with conventional Wi-Fi-based solutions while adapting to different sensing environments, reducing the power consumption of sensing by 98.7\% and harvesting up to 4.5 mW of power from RF energy.},
booktitle = {Proceedings of the Twenty-Fifth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {211–220},
numpages = {10},
keywords = {RF energy harvesting, battery-free, wireless sensing},
location = {Athens, Greece},
series = {MobiHoc '24}
}

@inproceedings{10.1145/3641512.3686361,
author = {Chen, Siyu and Jiang, Hongbo and Xiong, Jie and Jingyang, Hu and Wang, Penghao and Liu, Chao and Xiao, Zhu and Li, Bo},
title = {BeamCount: Indoor Crowd Counting Using Wi-Fi Beamforming Feedback Information},
year = {2024},
isbn = {9798400705212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641512.3686361},
doi = {10.1145/3641512.3686361},
abstract = {Real-time indoor crowd counting plays an important role in many applications such as crowd control, resource allocation and advertisement. Current research predominantly relies on camera-based methods. However, computer vision-based solutions raise severe privacy and ethical concerns. In this paper, we propose a privacy-preserving counting solution called BeamCount based on Wi-Fi sensing. Instead of using conventional Wi-Fi Channel State Information (CSI) readings, we utilize Wi-Fi Beamforming Feedback Information (BFI) for crowd counting estimation. Compared to CSI which can only be extracted from few commodity Wi-Fi cards (e.g., Intel 5300), BFI readings can be obtained from a large range of commodity Wi-Fi devices. We establish a mapping relationship between BFI and headcount and extract headcounts from BFI inputs through a carefully designed adversarial network. Owing to the adversarial network's cross-domain capability, the proposed counting system can achieve high accuracy across different environments, demonstrating its generalization capability. To mitigate the effect of BFI compression on sensing performance, we adopt a novel time series prediction model. Extensive real-world experiments validate the effectiveness of BeamCount in various environments, achieving an average counting accuracy of 93.6\%.},
booktitle = {Proceedings of the Twenty-Fifth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {1–10},
numpages = {10},
keywords = {wireless crowd counting, wi-fi sensing, beamforming feedback information},
location = {Athens, Greece},
series = {MobiHoc '24}
}

@inproceedings{10.1145/3641512.3690038,
author = {Nosrati, Farzam and Gelaw, Emebet and Corallo, Roberto and Schilleci, Silvia and Vicario, Alessio and Croce, Daniele},
title = {Cooperative Spectrum Sensing for Beyond-5G Networks in Fading Environments},
year = {2024},
isbn = {9798400705212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641512.3690038},
doi = {10.1145/3641512.3690038},
abstract = {The advent of pervasive wireless systems faces several challenges due to the massive data traffic growth resulting from the interconnection of billions of new devices. This makes it essential to provide smart decision-making in identifying available spectrum resources by sensing the radio frequency environment. In this study, we aim to improve the spectrum sensing process and enhance the detection efficiency of secondary users (sensing devices) in identifying primary users (transmitting devices). We consider a scenario in which secondary users are affected by noise and fading, and employ distributed detection and data fusion to combine data from geographically distributed sensors. The results show that collaborative spectrum sensing, where multiple SUs share their sensing data, significantly enhances detection performance. By applying optimization techniques to assign optimal weight vectors to the sensors, we further increase the detection performance of the primary user, where each one is affected by different noise factors. The study reveals that detection performance improves as more users collaborate, and this improvement is validated through scenarios with varying SNR values.},
booktitle = {Proceedings of the Twenty-Fifth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {446–451},
numpages = {6},
keywords = {beyond-5G, 5G networks, spectrum sensing, collaborative sensing},
location = {Athens, Greece},
series = {MobiHoc '24}
}

@inproceedings{10.1145/3691573.3691588,
author = {Pereira, Milena Batalha and Lancelotte, Felipe da Silva and de Classe, Tadeu Moreira and Garcia, Ana Cristina Bicharra},
title = {Simulation Sickness in Virtual Reality Games, How to Relieve it - Systematic Literature Study},
year = {2024},
isbn = {9798400709791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691573.3691588},
doi = {10.1145/3691573.3691588},
abstract = {Immersive technologies have introduced innovative possibilities across various fields, including entertainment, sports, education, healthcare, training, and rehabilitation. Despite the increasing popularity and feasibility of these technologies, owing to their ability to create a heightened sense of presence, a significant challenge remains in the widespread adoption of virtual reality: simulation sickness, also known as cybersickness. This phenomenon manifests as symptoms such as nausea, dizziness, and fatigue, arising from factors like sensory discrepancies between real and virtual movements, latency, graphic quality, and individual susceptibility. This article aims to provide a comprehensive review of the current literature on cybersickness, exploring its causes and identifying methods to mitigate its adverse effects through advancements in hardware, software, and usability settings, ultimately striving for a more comfortable and positive virtual experience.},
booktitle = {Proceedings of the 26th Symposium on Virtual and Augmented Reality},
pages = {168–176},
numpages = {9},
keywords = {Causes, Cybersickness, Simulation Sickness, Symptoms., Virtual Reality},
location = {Manaus, Brazil},
series = {SVR '24}
}

@inproceedings{10.1145/3691573.3691574,
author = {Correia, Pedro Henrique Barcha},
title = {Adaptive Virtual Reality Solutions: A Literature Review},
year = {2024},
isbn = {9798400709791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691573.3691574},
doi = {10.1145/3691573.3691574},
abstract = {Virtual Reality technology emerges as a versatile tool with applications ranging from gaming to therapy. Particularly, Adaptive Virtual Reality (AVR) stands out as a promising field, employing adaptive mechanisms to automatically customize the user experience according to individual preferences and needs. The use cases for this technology are broad, and so is the technical landscape. AVR technology enables, for instance, educational platforms that adapt to the user’s learning style, upper limb rehabilitation with auto-adjustable difficulty, and horror games that detect and incorporate the user’s most feared elements. Implementation-wise, some studies utilize Neural Networks and Support Vector Machines to adapt the system’s logic, while others use Fuzzy Logic Systems, for example. Additionally, some works integrate physiological sensors to estimate user stress, cognitive workload, or cybersickness. This technological and practical diversity poses a challenge for researchers and developers seeking to build AVR-based applications. To assist these professionals, this work reviews 30 studies that propose, develop, and assess AVR-based solutions. It provides a technological overview of these systems and classifies them into four domain groups: healthcare, learning/training, entertainment, and cybersickness mitigation. This work also presents key considerations and guidelines for designing and assessing AVR-based systems.},
booktitle = {Proceedings of the 26th Symposium on Virtual and Augmented Reality},
pages = {1–10},
numpages = {10},
keywords = {Adaptive Virtual Reality, Cybersickness Mitigation, Entertainment, Healthcare, Learning, Review, Training},
location = {Manaus, Brazil},
series = {SVR '24}
}

@article{10.1145/3659060,
author = {Cabrera, Miguel Altamirano and Rakhmatulin, Viktor and Fedoseev, Aleksey and Sautenkov, Oleg and Alyounes, Oussama and Puchkov, Andrei and Tsetserukou, Dzmitry},
title = {OmniCharger: CNN-Based Hand Gesture Interface to Operate an Electric Car Charging Robot through Teleconference},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
url = {https://doi.org/10.1145/3659060},
doi = {10.1145/3659060},
abstract = {The automation of the car charging process is motivated by the rapid development of technologies for self-driving cars and the increasing importance of ecological transportation units. Automation of this process requires the implementation of Computer Vision (CV) techniques. However, it remains challenging to precisely position the charger plug autonomously due to the sensitivity of CV algorithms to lighting and weather conditions. We introduce a novel robotic operation system based on hand gesture recognition through teleconferencing software. The users, connected by teleconference, use their hand gestures to teleoperate the electric plug located on the collaborative robot end-effector. We conducted a user study to evaluate the system performance and suitability using OmniCharger and two baseline interfaces (a UR10 Teach Pendant and a Logitech F710 Wireless Gamepad). Except for two trials, all the users were able to locate the plug inside of a 5 cm target using the interfaces. The distance to the target and the orientation error did not present statistically significant differences ( (p=0.1099 gt 0.05) and  (p=0.0903 gt 0.05) , respectively) in the use of the three interfaces. The NASA-TLX questionnaire results showed low values in all the sub-classes, the SUS results rated the usability of the proposed interface above average (68\%), and the UEQ showed excellent performance of the OmniCharger interface in the attractiveness, stimulation, and novelty attributes.},
journal = {J. Hum.-Robot Interact.},
month = sep,
articleno = {33},
numpages = {18},
keywords = {Human–robot interaction, teleoperation, gesture recognition, robotics}
}

@inproceedings{10.1145/3665318.3677166,
author = {Marques, Gabriel and N\'{o}brega, Rui and Neves Madeira, Rui},
title = {Exploring Virtual Reality in Exposure Therapy for Sensory Food Aversion},
year = {2024},
isbn = {9798400706899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665318.3677166},
doi = {10.1145/3665318.3677166},
abstract = {This paper presents research on how to use Virtual Reality with gamified exercises in a therapeutic context with children, focusing on the particular case of warning sensations triggered by sensory properties of food (sensory food aversion). To achieve this goal, we developed a tool featuring several food exposure challenges for patients to use. In the gamified system, the child explores a virtual environment while facing the food they have a problem with. These environments present tasks that resemble typical interactions performed in the real world to develop accommodation. The therapist also has an external system to control the system from outside. In addition, the system sends data collected during the session for the therapist to analyze. We researched how to keep a child engaged in therapeutic tasks and how a child perceives virtual interaction interfaces. The results suggest our system kept the users engaged. Moreover, data show a tendency for the users’ results (ease of use, presence, and performance) to remain the same when using controllers or hand tracking. The preliminary results are encouraging and allow us to apply the current system to a wider audience.},
booktitle = {Proceedings of the 29th International ACM Conference on 3D Web Technology},
articleno = {14},
numpages = {10},
keywords = {3D Environments, Children, Digital Health, Exposure Therapy, Gamification, Sensory Food Aversion, Virtual Reality},
location = {Guimar\~{a}es, Portugal},
series = {Web3D '24}
}

@article{10.1145/3676507,
author = {Constantinides, Marios and Bogucka, Edyta Paulina and Scepanovic, Sanja and Quercia, Daniele},
title = {Good Intentions, Risky Inventions: A Method for Assessing the Risks and Benefits of AI in Mobile and Wearable Uses},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {MHCI},
url = {https://doi.org/10.1145/3676507},
doi = {10.1145/3676507},
abstract = {Integrating Artificial Intelligence (AI) into mobile and wearables offers numerous benefits at individual, societal, and environmental levels. Yet, it also spotlights concerns over emerging risks. Traditional assessments of risks and benefits have been sporadic, and often require costly expert analysis. We developed a semi-automatic method that leverages Large Language Models (LLMs) to identify AI uses in mobile and wearables, classify their risks based on the EU AI Act, and determine their benefits that align with globally recognized long-term sustainable development goals; a manual validation of our method by two experts in mobile and wearable technologies, a legal and compliance expert, and a cohort of nine individuals with legal backgrounds who were recruited from Prolific, confirmed its accuracy to be over 85\%. We uncovered that specific applications of mobile computing hold significant potential in improving well-being, safety, and social equality. However, these promising uses are linked to risks involving sensitive data, vulnerable groups, and automated decision-making. To avoid rejecting these risky yet impactful mobile and wearable uses, we propose a risk assessment checklist for the Mobile HCI community.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = sep,
articleno = {262},
numpages = {28},
keywords = {LLM, mobile, prompt engineering, risk assessment, sustainable development goals, wearables}
}

@article{10.1145/3676505,
author = {Islam, Rahul and Bae, Sang Won},
title = {FacePsy: An Open-Source Affective Mobile Sensing System - Analyzing Facial Behavior and Head Gesture for Depression Detection in Naturalistic Settings},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {MHCI},
url = {https://doi.org/10.1145/3676505},
doi = {10.1145/3676505},
abstract = {Depression, a prevalent and complex mental health issue affecting millions worldwide, presents significant challenges for detection and monitoring. While facial expressions have shown promise in laboratory settings for identifying depression, their potential in real-world applications remains largely unexplored due to the difficulties in developing efficient mobile systems. In this study, we aim to introduce FacePsy, an open-source mobile sensing system designed to capture affective inferences by analyzing sophisticated features and generating real-time data on facial behavior landmarks, eye movements, and head gestures - all within the naturalistic context of smartphone usage with 25 participants. Through rigorous development, testing, and optimization, we identified eye-open states, head gestures, smile expressions, and specific Action Units (2, 6, 7, 12, 15, and 17) as significant indicators of depressive episodes (AUROC=81\%). Our regression model predicting PHQ-9 scores achieved moderate accuracy, with a Mean Absolute Error of 3.08. Our findings offer valuable insights and implications for enhancing deployable and usable mobile affective sensing systems, ultimately improving mental health monitoring, prediction, and just-in-time adaptive interventions for researchers and developers in healthcare.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = sep,
articleno = {260},
numpages = {32},
keywords = {affective computing, application instrumentation, depression, empirical study that tells us about people, field study, machine learning, mobile computing, system}
}

@article{10.1145/3676527,
author = {You, Chuang-Wen and Chen, Hsin-Ai and Chen, Pin-Chieh and Lai, Wen-Ni and Yuan, Chien Wen (Tina) and Bi, Nanyi},
title = {Toward Understanding the Impact of Visualized Focus Levels in Virtual Reality on User Presence and Experience},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {MHCI},
url = {https://doi.org/10.1145/3676527},
doi = {10.1145/3676527},
abstract = {Neurofeedback refers to the process of feeding a sensory representation of brain activity back to users in real time to improve a particular brain function, e.g., their focus and/or attention on a particular task. This study addressed the notable lack of research on methods used to visualize EEG data and their effects on the immersive quality of VR. We developed an algorithm to quantify focus, yielding a focus score. A pre-study with twenty participants confirmed its effectiveness in distinguishing between focused and relaxed mental states. Subsequently, we used this focus score to prototype a VR experience system visualizing the focus score in preconfigured manners, which was utilized in an exploratory study to assess the impact of different neurofeedback visualization methods on user engagement and focus in VR. Among all the visualization methods evaluated, the environmental scheme stood out due to its superior usability during task execution, its ability to evoke positive emotions through the visualization of objects or scenes, and its minimal deviation from user expectations. Additionally, we explored design guidelines based on collected results for future research to further refine the visualization scheme, ensuring effective integration of the focus score within the VR environment. These enhancements are crucial for designing neurofeedback visualization schemes that aim to boost participant focus in VR settings, offering significant insights into the optimization of such technologies.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = sep,
articleno = {280},
numpages = {30},
keywords = {focus score, neurofeedback, virtual reality, visualization}
}

