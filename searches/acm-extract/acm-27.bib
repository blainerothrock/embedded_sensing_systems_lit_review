@inproceedings{10.1145/3610419.3610489,
author = {Yadav, Rajeshwar and Halder, Raju and Thakur, Atul and Banda, Gourinath},
title = {A Lightweight Deep Learning-based Weapon Detection Model for Mobile Robots},
year = {2023},
isbn = {9781450399807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610419.3610489},
doi = {10.1145/3610419.3610489},
abstract = {As mobile robotics continues to advance, the need for adequate surveillance in robotic environments is becoming increasingly important. Detecting suspicious objects in sensitive areas using mobile robots is challenging due to the limited computational resources available on these devices. This paper describes a new system for automatically detecting weapons in real-time video footage designed for low-computing devices in mobile robots. We present a novel weapon detection model that aims to balance the trade-off between inference time and detection accuracy, making it a lightweight model compared to existing models. The proposed model is trained and tested on existing benchmark datasets. The model is compared to existing lightweight weapon detection models to determine its suitability for low-computing devices. We obtain the mAP of 90.3\%, 85.13\% and 92.38\% for the IITP_W, Handgun and Sohas datasets, respectively. The results outperforming the well-known PicoDet model. We envisage that the proposed model could be a useful tool for surveillance using mobile robots during events such as riots and anti-terrorist operations.},
booktitle = {Proceedings of the 2023 6th International Conference on Advances in Robotics},
articleno = {70},
numpages = {6},
keywords = {Object detection, neural networks, robotics, weapon detection},
location = {Ropar, India},
series = {AIR '23}
}

@inproceedings{10.1145/3610419.3610472,
author = {Peter, Jerin and Thomas, Mervin Joe and Mohan, Santhakumar},
title = {Development of an Autonomous Ground Robot Using a Real-Time Appearance Based (RTAB) Algorithm for Enhanced Spatial Mapping: Development of a Ground Robot with Autonomous Navigation Capability},
year = {2023},
isbn = {9781450399807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610419.3610472},
doi = {10.1145/3610419.3610472},
abstract = {This paper describes the preliminary results of developing an autonomous ground robot that can navigate dynamic indoor/outdoor and GPS-restricted environments. This study outlines the demerit of applying conventional LIDAR sensors alone for mapping, as they cannot detect obstacles beyond their range and compromise reliability in real-time applications. This paper showcases the application of a Real-Time Appearance Based (RTAB) algorithm by combining LIDAR sensors with 3D cameras for three-dimensional mapping. This algorithm generates a realistic view of the spatial environment enabling superior clarity of the robot's surroundings. The simulations have been conducted on the open-source Robotic Operating System (ROS) NOETIC platform. Additionally, the real-time mapping of outdoor environments using an in-house miniature-sized prototype designed for proof of concept is also described in this paper. The developed robot can find suitable applications as delivery bots, surveillance robots, rescue operations, and teleoperation.},
booktitle = {Proceedings of the 2023 6th International Conference on Advances in Robotics},
articleno = {53},
numpages = {5},
location = {Ropar, India},
series = {AIR '23}
}

@inproceedings{10.1145/3610419.3610441,
author = {Jain, Ravi Kant},
title = {Development of IoT-enabled Mobile Robotic System for Monitoring},
year = {2023},
isbn = {9781450399807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610419.3610441},
doi = {10.1145/3610419.3610441},
abstract = {Robotics has always been demanding rapid and faster development in different applications like automobiles, material handling, power plant, coal mines, inspection, agriculture, defense, biomedical, and so on where controlling the robots is a major challenge which can be provided the remote operation through wirelessly. Internet-of-Things (IoT) technology has been introduced for robotic applications which can facilitate the transferring of data almost without the intervention of human-to-machine-interface (HMI) or human-to-computer interaction via a wireless communication approach. In this paper, an IoT-enabled mobile robot is designed for proving the robot's functionality which provides the user with all the necessary controls in mobile android phones using IoT. A robotic system is developed and the data visualization of gasses content and air quality testing within the working environment have been carried out where the use of sensors is essential for finding and detecting certain factors within the working environment. A mobile app is developed using a web application by which the different gasses content and air quality can be monitored in various industrial applications. This kind of application demands for use of an IoT cloud for storing data in futuristic robotic applications.},
booktitle = {Proceedings of the 2023 6th International Conference on Advances in Robotics},
articleno = {22},
numpages = {8},
keywords = {IoT, Mobile Robotic System, Monitoring, Web/Android Applications, Wireless control etc},
location = {Ropar, India},
series = {AIR '23}
}

@inproceedings{10.1145/3610419.3610430,
author = {Kumar, Utkarsh and Kala, Rahul and Nandi, G C},
title = {Low-Cost Domain Adaptive Experience Based Localization for Autonomous Robots},
year = {2023},
isbn = {9781450399807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610419.3610430},
doi = {10.1145/3610419.3610430},
abstract = {Localization is one of the key aspect of any automated navigation system, it means finding own position in a given environment, without it an agent might get lost. In a dynamic environment localization is a very difficult task because various factors such as weather and illumination may change over time. We developed a low-cost solution to the localization problem, relying only on a camera sensor, without using expensive sensors like lidars. The solution allows to handle dynamic nature of the environment and uses previous knowledge for localization, hence it is named as Experience Based Localization. STREET-CNN is a core part of the method which is trained on STREET dataset. This dataset contains several images of streets in IIIT-A along with their GPS coordinates as labels. The trained network calculates the similarity score between the two cross domain images. Presented method is equipped with Kalman Filter to include speed variations of agent while predicting its location. Results produced with domain adaptation technique is compared with, without the domain adaptation technique.},
booktitle = {Proceedings of the 2023 6th International Conference on Advances in Robotics},
articleno = {11},
numpages = {6},
keywords = {Autonomous navigation system, Datasets, Deep neural network, Domain adaptation, Robot localization},
location = {Ropar, India},
series = {AIR '23}
}

@article{10.1145/3626471,
author = {Zhang, Yidan and Ens, Barrett and Satriadi, Kadek Ananta and Yang, Ying and Goodwin, Sarah},
title = {Embodied Provenance for Immersive Sensemaking},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {ISS},
url = {https://doi.org/10.1145/3626471},
doi = {10.1145/3626471},
abstract = {Immersive analytics research has explored how embodied data representations and interactions can be used to engage users in sensemaking. Prior research has broadly overlooked the potential of immersive space for supporting analytic provenance, the understanding of sensemaking processes through users’ interaction histories. We propose the concept of embodied provenance, the use of three-dimensional space and embodied interactions in supporting recalling, reproducing, annotating and sharing analysis history in immersive environments. We design a conceptual framework for embodied provenance by highlighting a set of design criteria for analytic provenance drawn from prior work and identifying essential properties for embodied provenance. We develop a prototype system in virtual reality to demonstrate the concept and support the conceptual framework by providing multiple data views and embodied interaction metaphors in a large virtual space. We present a use case scenario of energy consumption analysis and evaluated the system through a qualitative evaluation with 17 participants, which show the system’s potential for assisting analytic provenance using embodiment. Our exploration of embodied provenance through this prototype provides lessons learnt to guide the design of immersive analytic tools for embodied provenance.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {435},
numpages = {19},
keywords = {spatial memory, sensemaking, immersive analytics, embodied provenance}
}

@inproceedings{10.1145/3620678.3624659,
author = {Wu, Xiaolong and Tian, Dave Jing and Kim, Chung Hwan},
title = {Building GPU TEEs using CPU Secure Enclaves with GEVisor},
year = {2023},
isbn = {9798400703874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620678.3624659},
doi = {10.1145/3620678.3624659},
abstract = {Trusted execution environments (TEEs) have been proposed to protect GPU computation for machine learning applications operating on sensitive data. However, existing GPU TEE solutions either require CPU and/or GPU hardware modification to realize TEEs for GPUs, which prevents current systems from adopting them, or rely on untrusted system software such as GPU device drivers. In this paper, we propose using CPU secure enclaves, e.g., Intel SGX, to build GPU TEEs without modifications to existing hardware. To tackle the fundamental limitations of these enclaves, such as no support for I/O operations, we design and develop GEVisor, a formally verified security reference monitor software to enable a trusted I/O path between enclaves and GPU without trusting the GPU device driver. GEVisor operates in the Virtual Machine Extension (VMX) root mode, monitors the host system software to prevent unauthorized access to the GPU code and data outside the enclave, and isolates the enclave GPU context from other contexts during GPU computation. We implement and evaluate GEVisor on a commodity machine with an Intel SGX CPU and an NVIDIA Pascal GPU. Our experimental results show that our approach maintains an average overhead of 13.1\% for deep learning and 18\% for GPU benchmarks compared to native GPU computation while providing GPU TEEs for existing CPU and GPU hardware.},
booktitle = {Proceedings of the 2023 ACM Symposium on Cloud Computing},
pages = {249–264},
numpages = {16},
keywords = {Confidential Computing, GPU, TEE},
location = {Santa Cruz, CA, USA},
series = {SoCC '23}
}

@inproceedings{10.1145/3603421.3603431,
author = {Alhazzaa, Kifah and Yan, Wei},
title = {Integrating Parametric Modeling, BIM, and Building Performance Analysis into Augmented Reality for Architectural Design and Education},
year = {2023},
isbn = {9781450397469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603421.3603431},
doi = {10.1145/3603421.3603431},
abstract = {In 2020, about 38\% of annual global carbon dioxide emissions come from buildings. Energy-efficient buildings that help mitigate the effects of climate change need a novel strategy to bridge the knowledge gap between construction practitioners and the advancement&nbsp;in building design methodology and environmental analysis. Parameters and forms of the building envelope have a major impact on energy efficiency, and Passive design methods are the most practical and cost-effective means of thermal and energy management in buildings if designed correctly during the preliminary stages. The purpose of this research is to determine whether the current augmented reality (AR) platform can successfully show the potential and significance of parametric design, as well as present a dynamic environmental analysis from the earliest stage of architectural design. In order to enhance the form finding phase of the design process and getting architecture students and professionals aware of the potential of parametric design for design alterations and optimization. The project aims to deliver the subject matter in a clear and engaging manner by the use of AR. In the developed AR app (ParametricAR) users will be able to sense scale, volume, and context, by placing virtual objects in the physical environment. Users will be able to study and evaluate the geometric and spatial design by walking around their building virtually in 1:1 scale. The innovative combination of parametric modeling, BIM, and real-time shadow analysis that was accomplished by ParametricAR served to enhance the user experience and resulted in the production of a package that expanded the user's capacity to develop and assess building forms. ParametricAR was successful in combining parametric modeling, BIM, and shadow analysis into a seamless AR experience that is compatible with the mobile AR hardware that is widely available now.},
booktitle = {Proceedings of the 2023 7th International Conference on Virtual and Augmented Reality Simulations},
pages = {68–76},
numpages = {9},
keywords = {Real Time Design, Parametric Design, Parametric BIM Design, Grid System, BIM, Augmented Reality},
location = {Sydney, Australia},
series = {ICVARS '23}
}

@inproceedings{10.1145/3617694.3623244,
author = {Wang, Yanchen and Singh, Lisa},
title = {Mitigating demographic bias of machine learning models on social media},
year = {2023},
isbn = {9798400703812},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617694.3623244},
doi = {10.1145/3617694.3623244},
abstract = {Social media posts have been used to predict different user behaviors and attitudes, including mental health condition, political affiliation, and vaccine hesitancy. Unfortunately, while social media platforms make APIs available for collecting user data, they also make it challenging to collect well structured demographic features about individuals who post on their platforms. This makes it difficult for researchers to assess the fairness of models they develop using these data. Researchers have begun considering approaches for determining fairness of machine learning models built using social media data. In this paper, we consider both the case when the sensitive demographic feature is available to the researcher and when it is not. After framing our specific problem and discussing the challenges, we focus on the scenario when the training data does not explicitly contain a sensitive demographic feature, but instead contains a hidden sensitive feature that can be approximated using a sensitive feature proxy. In this case, we propose an approach for determining whether a sensitive feature proxy exists in the training data and apply a fixing method to reduce the correlation between the sensitive feature proxy and the sensitive feature. To demonstrate our approach, we present two case studies using micro-linked Twitter/X data and show biases resulting from sensitive feature proxies that are present in the training data and are highly correlated to hidden sensitive features. We then show that a standard fixing approach can effectively reduce bias even if the sensitive attribute needs to be inferred by the researcher using existing reliable inference models. This is an important step toward understanding approaches for improving fairness on social media.},
booktitle = {Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {24},
numpages = {12},
keywords = {algorithmic fairness, demographic inference, social media},
location = {Boston, MA, USA},
series = {EAAMO '23}
}

@inproceedings{10.1145/3616391.3622764,
author = {Fischer, Marten and T\"{o}njes, Ralf},
title = {Resource-aware Security Configuration for Constrained IoT Devices},
year = {2023},
isbn = {9798400703683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616391.3622764},
doi = {10.1145/3616391.3622764},
abstract = {The Internet of Things (IoT) is the enabler for new innovations in several domains. It allows the connection of digital services with entities in the physical world. These entities are devices of different sizes ranging from large machinery to tiny sensors. In the latter case, devices are typically characterized by limited resources in terms of computational power, available memory and sometimes limited power supply. As a consequence, the use of security algorithms requires of them to work within the limited resources. This means to find a suitable implementation and configuration for a security algorithm, that performs properly on the device, which may become a challenging task. On the other side, there is the desire to protect valuable assets as strong as possible. Usually, security goals are recorded in security policies, but they do not consider resource availability on the involved device and its power consumption while executing security algorithms. This paper presents an IoT security configuration tool that helps the designer of an IoT environment to experiment with the trade-offs between maximizing security and extending the lifetime of a resource constrained IoT device. The tool is controlled with high-level description of security goals in the form of policies. It allows the designer to validate various (security) configurations for a single IoT device up to a large sensor network},
booktitle = {Proceedings of the 19th ACM International Symposium on QoS and Security for Wireless and Mobile Networks},
pages = {7–14},
numpages = {8},
keywords = {iot, resource-constraints, security configuration},
location = {Montreal, Quebec, Canada},
series = {Q2SWinet '23}
}

@inproceedings{10.1145/3603421.3603429,
author = {Wu, Yen-Liang and Chou, Wen-Huei and Li, Francis},
title = {The Design Methods of Augmented Reality in a Historical Site},
year = {2023},
isbn = {9781450397469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603421.3603429},
doi = {10.1145/3603421.3603429},
abstract = {Augmented reality (AR) technology has the potential to enhance the visitor experience at historical and cultural sites by providing rich, interactive information about the site's history and culture. In our project to develop an AR guide app for a historical and cultural park, we identified three key design methods for creating engaging and informative AR content. The first method is the use of small interactive 3D models, such as 3D maps of a historical site at different points in time, which can help visitors understand the evolution of the site over the years. The second approach is the overlay method, which involves superimposing images of historical objects or buildings onto the present-day environment to give visitors a sense of what the site looked like in the past. The third method is the replacement of current scenes with historical ones, allowing visitors to experience the appearance and location of historical buildings as they once were. These design methods can serve as a useful reference for creating AR guides at other historical and cultural sites around the world. As AR technology continues to advance, it has the potential to revolutionize the way we learn about and engage with history and culture.},
booktitle = {Proceedings of the 2023 7th International Conference on Virtual and Augmented Reality Simulations},
pages = {52–58},
numpages = {7},
keywords = {Historical site, Content design, Augmented Reality, AR tour guide},
location = {Sydney, Australia},
series = {ICVARS '23}
}

@inproceedings{10.1145/3623652.3623672,
author = {Banerjee, Sarbartha and Wei, Shijia and Ramrakhyani, Prakash and Tiwari, Mohit},
title = {Triton: Software-Defined Threat Model for Secure Multi-Tenant ML Inference Accelerators},
year = {2023},
isbn = {9798400716232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623652.3623672},
doi = {10.1145/3623652.3623672},
abstract = {Secure machine-learning inference is essential with the advent of multi-tenancy in machine learning-as-a-service (MLaaS) accelerators. Model owners demand the confidentiality of both model weights and architecture, while end users want to protect their personal data. Moreover, ML models used in mission-critical applications like autonomous vehicles or disease classification need integrity protection. While hardware trusted execution environments (TEE)&nbsp;[4, 41] provide data confidentiality and integrity, they face two challenges in the adoption for ML inference. First, TEEs are susceptible to numerous side channels, arising from resource sharing in multi-tenant systems. Second, the performance overhead of these TEEs is often proportional to the secret data size, making them unattractive for data-intensive real-time inference. The diverse deployment threats further complicate these challenges. For instance, compared to time-sharing execution, multi-tenant accelerators must assume a larger attack surface with adversaries monitoring or tampering with on-accelerator resources. Some inference process sensitive inputs while others compute on public inputs. As a result, existing TEE designs often adopt a single, perhaps the most restrictive threat model, which overburdens many secure ML inference deployments. To address the challenges in adopting TEEs for secure ML inference, we introduce the Triton TEE framework. Triton tailors threat models to each deployment with low overhead while mitigating side-channel leakages. Triton achieves this by offering an interface to define fine-grained secrets in an ML model or input, along with the attacker observation capabilities. Triton framework generates code for a custom threat model for each application based on its security requirements. The security policy of each secret is embedded in the instruction to convey the security guarantee to the hardware. The expressive threat model and secret declaration can reduce the secure ML inference overhead from to across different multi-tenant deployments.},
booktitle = {Proceedings of the 12th International Workshop on Hardware and Architectural Support for Security and Privacy},
pages = {19–28},
numpages = {10},
keywords = {ML accelerator, Secure hardware, TEE, Threat model},
location = {Toronto, Canada},
series = {HASP '23}
}

@inproceedings{10.1145/3586183.3606728,
author = {Sun, Zhe and Liang, Qixuan and Wang, Meng and Zhang, Zhenliang},
title = {Neighbor-Environment Observer: An Intelligent Agent for Immersive Working Companionship},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606728},
doi = {10.1145/3586183.3606728},
abstract = {Human-computer symbiosis is a crucial direction for the development of artificial intelligence. As intelligent systems become increasingly prevalent in our work and personal lives, it is important to develop strategies to support users across physical and virtual environments. While technological advances in personal digital devices, such as personal computers and virtual reality devices, can provide immersive experiences, they can also disrupt users’ awareness of their surroundings and enhance the frustration caused by disturbances. In this paper, we propose a joint observation strategy for artificial agents to support users across virtual and physical environments. We introduce a prototype system, neighbor-environment observer (NEO), that utilizes non-invasive sensors to assist users in dealing with disruptions to their immersive experience. System experiments evaluate NEO from different perspectives and demonstrate the effectiveness of the joint observation strategy. A user study is conducted to evaluate its usability. The results show that NEO could lessen users’ workload with the learned user preference. We suggest that the proposed strategy can be applied to various smart home scenarios.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {112},
numpages = {14},
keywords = {Virtual reality, collaborative agent, interactive system},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3586183.3606797,
author = {Kim, Bogoan and Jeong, Dayoung and Kim, Jennifer G and Hong, Hwajung and Han, Kyungsik},
title = {V-DAT (Virtual Reality Data Analysis Tool): Supporting Self-Awareness for Autistic People from Multimodal VR Sensor Data},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606797},
doi = {10.1145/3586183.3606797},
abstract = {Virtual reality (VR) has become a valuable tool for social and educational purposes for autistic people, as it provides flexible environmental support to create a variety of experiences. A growing body of recent research has examined the behaviors of autistic people using sensor-based data to better understand autistic people and investigate the effectiveness of VR. Comprehensive analysis of the various signals that can be easily collected in the VR environment can promote understanding of autistic people. While this quantitative evidence has the potential to help both autistic people and others (e.g., autism experts) to understand behaviors of autistic people, existing studies have focused on single signal analysis and have not determined the acceptability of signal analysis results from the autistic person’s point of view. To facilitate the use of multiple sensor signals in VR for autistic people and experts, we introduce V-DAT (Virtual Reality Data Analysis Tool), designed to support a VR sensor data handling pipeline. V-DAT takes into account four sensor modalities—head position and rotation, eye movement, audio, and physiological signals—that are actively used in current VR research for autistic people. We explain the characteristics and processing methods of the data for each modality as well as the analysis with comprehensive visualizations of V-DAT. We also conduct a case study to investigate the feasibility of V-DAT as a way of broadening understanding of autistic people from the perspectives of both autistic people and autism experts. Finally, we discuss issues with the process of V-DAT development and complementary measures for the applicability and scalability of a sensor data management system for autistic people.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {40},
numpages = {13},
keywords = {accessibility, autism, data handling pipeline, virtual reality},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3586183.3606784,
author = {Lee, Wan-Chen and Hung, Ching-Wen and Ting, Chao-Hsien and Chi, Peggy and Chen, Bing-Yu},
title = {TacNote: Tactile and Audio Note-Taking for Non-Visual Access},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606784},
doi = {10.1145/3586183.3606784},
abstract = {Blind and visually impaired (BVI) people primarily rely on non-visual senses to interact with a physical environment. &nbsp;Doing so requires a high cognitive load to perceive and memorize the presence of a large set of objects, such as at home or in a learning setting. In this work, we explored opportunities to enable object-centric note-taking &nbsp;by using a 3D printing pen for interactive, personalized tactile annotations. We first identified the benefits and challenges of self-created tactile graphics in a formative diary study. Then, we developed TacNote, a system that enables BVI users to annotate, explore, and memorize critical information associated with everyday objects. Using TacNote, the users create tactile graphics with a 3D printing pen and attach them to the target objects. They capture and organize the physical labels &nbsp;by using TacNote’s camera-based mobile app. In addition, they can specify locations, ordering, and hierarchy via finger-pointing interaction and receive audio feedback. Our user study with ten BVI participants showed that TacNote effectively alleviated the memory burden, offering a promising solution for enhancing users’ access to information.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {37},
numpages = {14},
keywords = {3D printing pen, Accessibility, Assistive technology, Fabrication, Mobile Application, Tactile graphics},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3586183.3606769,
author = {Zhu, Jingwen and El Nesr, Nadine and Simon, Christina and Rettenmaier, Nola and Beiler, Kaitlyn and Kao, Cindy Hsin-Liu},
title = {BioWeave: Weaving Thread-Based Sweat-Sensing On-Skin Interfaces},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606769},
doi = {10.1145/3586183.3606769},
abstract = {There has been a growing interest in developing and fabricating wearable sweat sensors in recent years, as sweat contains various analytes that can provide non-invasive indications of various conditions in the body. Although recent HCI research has been looking into wearable sensors for understanding health conditions, textile-based wearable sweat sensors remain underexplored. We present BioWeave, a woven thread-based sweat-sensing on-skin interface. Through weaving single-layer and multi-layer structures, we combine sweat-sensing threads with versatile fiber materials. We identified a design space consisting of colorimetric and electrochemical sensing approaches, targeting biomarkers including pH, glucose, and electrolytes. We explored 2D and 3D weaving structures for underexplored body locations to seamlessly integrate sweat-sensing thread into soft wearable interfaces. We developed five example applications to demonstrate the design capability offered. The BioWeave sensing interface can provide seamless integration into everyday textile-based wearables and offers the unobtrusive analysis of health conditions.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {35},
numpages = {11},
keywords = {biosensing, on-skin interfaces, sweat sensing, textiles, wearables, weaving},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3586183.3606794,
author = {Chen, Wenqiang and Wang, Ziqi and Quan, Pengrui and Peng, Zhencan and Lin, Shupei and Srivastava, Mani and Matusik, Wojciech and Stankovic, John},
title = {Robust Finger Interactions with COTS Smartwatches via Unsupervised Siamese Adaptation},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606794},
doi = {10.1145/3586183.3606794},
abstract = {Wearable devices like smartwatches and smart wristbands have gained substantial popularity in recent years. However, their small interfaces create inconvenience and limit computing functionality. To fill this gap, we propose ViWatch, which enables robust finger interactions under deployment variations, and relies on a single IMU sensor that is ubiquitous in COTS smartwatches. To this end, we design an unsupervised Siamese adversarial learning method. We built a real-time system on commodity smartwatches and tested it with over one hundred volunteers. Results show that the system accuracy is about 97\% over a week. In addition, it is resistant to deployment variations such as different hand shapes, finger activity strengths, and smartwatch positions on the wrist. We also developed a number of mobile applications using our interactive system and conducted a user study where all participants preferred our unsupervised approach to supervised calibration. The demonstration of ViWatch is shown at https://youtu.be/N5-ggvy2qfI.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {25},
numpages = {14},
keywords = {Finger Interaction, Gesture Recognition, Unsupervised Adversarial Training, Vibration Sensing},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3586183.3606774,
author = {Nicolae, Madalina and Roussel, Vivien and Koelle, Marion and Huron, Samuel and Steimle, J\"{u}rgen and Teyssier, Marc},
title = {Biohybrid Devices: Prototyping Interactive Devices with Growable Materials},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606774},
doi = {10.1145/3586183.3606774},
abstract = {Living bio-materials are increasingly used in HCI for fabricating objects by growing. However, how to integrate electronics to make these objects interactive still needs to be clarified. This paper presents an exploration of the fabrication design space of Biohybrid Interactive Devices, a class of interactive devices fabricated by merging electronic components and living organisms. From the exploration of this space using bacterial cellulose, we outline a fabrication framework centered on the biomaterials‘ life cycle phases. We introduce a set of novel fabrication techniques for embedding conductive elements, sensors, and output components through biological (e.g. bio-fabrication and bio-assembling) and digital processes. We demonstrate the combinatory aspect of the framework by realizing three tangible, wearable, and shape-changing interfaces. Finally, we discuss the sustainability of our approach, its limitations, and the implications for bio-hybrid systems in HCI.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {31},
numpages = {15},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3586183.3606766,
author = {Han, Violet Yinuo and Boadi-Agyemang, Abena and Lin, Yuyu and Lindlbauer, David and Ion, Alexandra},
title = {Parametric Haptics: Versatile Geometry-based Tactile Feedback Devices},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606766},
doi = {10.1145/3586183.3606766},
abstract = {Haptic feedback is important for immersive, assistive, or multimodal interfaces, but engineering devices that generalize across applications is notoriously difficult. To address the issue of versatility, we propose Parametric Haptics, geometry-based tactile feedback devices that are customizable to render a variety of tactile sensations. To achieve this, we integrate the actuation mechanism with the tactor geometry into passive 3D printable patches, which are then connected to a generic wearable actuation interface consisting of micro gear motors. The key benefit of our approach is that the 3D-printed patches are modular, can consist of varying numbers and shapes of tactors, and that the tactors can be grouped and moved by our actuation geometry over large areas of the skin. The patches are soft, thin, conformable, and easy to customize to different use cases, thus potentially enabling a large design space of diverse tactile sensations. In our user study, we investigate the mapping between geometry parameters of our haptic patches and users’ tactile perceptions. Results indicate a good agreement between our parameters and the reported sensations, showing initial evidence that our haptic patches can produce a wide range of sensations for diverse use scenarios. We demonstrate the utility of our approach with wearable prototypes in immersive Virtual Reality (VR) scenarios, embedded into wearable objects such as glasses, and as wearable navigation and notification interfaces. We support designing such patches with a design tool in Rhino.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {65},
numpages = {13},
keywords = {Fabrication, Metamaterials, Programmable Matter},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3607828.3617799,
author = {Tai, Chi-en Amy and Keller, Matthew and Nair, Saeejith and Chen, Yuhao and Wu, Yifan and Markham, Olivia and Parmar, Krish and Xi, Pengcheng and Keller, Heather and Kirkpatrick, Sharon and Wong, Alexander},
title = {NutritionVerse: Empirical Study of Various Dietary Intake Estimation Approaches},
year = {2023},
isbn = {9798400702846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607828.3617799},
doi = {10.1145/3607828.3617799},
abstract = {Accurate dietary intake estimation is critical for informing policies and programs to support healthy eating, as malnutrition has been directly linked to decreased quality of life. However self-reporting methods such as food diaries suffer from substantial bias. Other conventional dietary assessment techniques and emerging alternative approaches such as mobile applications incur high time costs and may necessitate trained personnel. Recent work has focused on using computer vision and machine learning to automatically estimate dietary intake from food images, but the lack of comprehensive datasets with diverse viewpoints, modalities and food annotations hinders the accuracy and realism of such methods. To address this limitation, we introduce NutritionVerse-Synth, the first large-scale dataset of 84,984 photorealistic synthetic 2D food images with associated dietary information and multimodal annotations (including depth images, instance masks, and semantic masks). Additionally, we collect a real image dataset, NutritionVerse-Real, containing 889 images of 251 dishes to evaluate realism. Leveraging these novel datasets, we develop and benchmark NutritionVerse, an empirical study of various dietary intake estimation approaches, including indirect segmentation-based and direct prediction networks. We further fine-tune models pretrained on synthetic data with real images to provide insights into the fusion of synthetic and real data. Finally, we release both datasets (NutritionVerse-Synth, NutritionVerse-Real) and the collection of models (NutritionVerse) on https://bit.ly/genai4good as part of an open initiative to accelerate machine learning for dietary sensing.},
booktitle = {Proceedings of the 8th International Workshop on Multimedia Assisted Dietary Management},
pages = {11–19},
numpages = {9},
keywords = {datasets, dietary, prediction, segmentation},
location = {Ottawa ON, Canada},
series = {MADiMa '23}
}

@inproceedings{10.1145/3586183.3606765,
author = {Luo, Yiyue and Zhu, Junyi and Wu, Kui and Honnet, Cedric and Mueller, Stefanie and Matusik, Wojciech},
title = {MagKnitic: Machine-knitted Passive and Interactive Haptic Textiles with Integrated Binary Sensing},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606765},
doi = {10.1145/3586183.3606765},
abstract = {In this paper, we introduce MagKnitic, a novel approach to integrate passive force feedback and binary sensing into fabrics via digital machine knitting. Our approach utilizes digital fabrication technology to enable haptic interfaces that are soft, flexible, lightweight, and conform to the user’s body shape. Despite these characteristics, our interfaces provide diverse, interactive, and responsive force feedback, expanding the design space for haptic experiences.MagKnitic provides scalable and customizable passive haptic sensations by utilizing the attractive force between ferromagnetic yarns and permanent magnets, both of which are seamlessly integrated into knitted fabrics. Moreover, we present a binary sensing capability based on the resistance drop resulting from the activated electrical path between the integrated magnets and ferromagnetic yarn upon direct contact. We offer parametric design templates for users to customize MagKnitic layouts and patterns. With various design layouts and combinations, MagKnitic supports passive haptics interactions of linear, polar, angular, planar, radial, and user-defined motions. We perform a technical evaluation of the passive force feedback and the binary sensing capabilities with different machine knitting layouts and patterns, embedded magnet sizes, and interaction distances. In addition, we conduct two user studies to validate the effectiveness of MagKnitic. Finally, we demonstrate various application scenarios, including wearable input interfaces, game controllers, passive VR/AR wearables, and interactive furniture coverings.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {66},
numpages = {13},
keywords = {Machine knitting, Passive Haptics, Personal Fabrication, Rapid Function Prototyping, eTextiles},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3581783.3612147,
author = {Wang, Yang and Dong, Bo and Zhang, Yuji and Zhou, Yunduo and Mei, Haiyang and Wei, Ziqi and Yang, Xin},
title = {Event-Enhanced Multi-Modal Spiking Neural Network for Dynamic Obstacle Avoidance},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612147},
doi = {10.1145/3581783.3612147},
abstract = {Autonomous obstacle avoidance is of vital importance for an intelligent agent such as a mobile robot to navigate in its environment. Existing state-of-the-art methods train a spiking neural network (SNN) with deep reinforcement learning (DRL) to achieve energy-efficient and fast inference speed in complex/unknown scenes. These methods typically assume that the environment is static while the obstacles in real-world scenes are often dynamic. The movement of obstacles increases the complexity of the environment and poses a great challenge to the existing methods. In this work, we approach robust dynamic obstacle avoidance twofold. First, we introduce the neuromorphic vision sensor (i.e., event camera) to provide motion cues complementary to the traditional Laser depth data for handling dynamic obstacles. Second, we develop an DRL-based event-enhanced multimodal spiking actor network (EEM-SAN) that extracts information from motion events data via unsupervised representation learning and fuses Laser and event camera data with learnable thresholding. Experiments demonstrate that our EEM-SAN outperforms state-of-the-art obstacle avoidance methods by a significant margin, especially for dynamic obstacle avoidance.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {3138–3148},
numpages = {11},
keywords = {deep reinforcement learning (drl), dynamic obstacle avoidance, dynamic vision sensor (dvs), spiking neural network (snn)},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3581783.3612329,
author = {Lin, Feng and fu, Kaiqiang and Luo, Hao and Zhan, Ziyue and Wang, Zhibo and Liu, Zhenguang and Cavallaro, Lorenzo and Ren, Kui},
title = {Cross-Modal and Multi-Attribute Face Recognition: A Benchmark},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612329},
doi = {10.1145/3581783.3612329},
abstract = {Face recognition has made significant advances with the development of deep learning and has begun to be deployed in some unrestricted scenarios. Many smartphones, for example, have infrared sensors that allow them to capture clear images even in low-light conditions. Face authentication under complex environmental conditions can thus be accomplished by matching NIR-VIS face images across modalities. However, existing NIR-VIS datasets lack enough variation in face attributes and are insufficient for real-world scenarios. To address the aforementioned issues, we first propose a 300-person NIR-VIS cross-modality face dataset with a variety of attributes. Based on modal information removal, we proposed a NIR-VIS cross-modal face recognition model. We can effectively extract modal information by constraining the similarity distribution of modalities and then using the orthogonal loss to remove modal information from identity features. The method achieves excellent results on our dataset and CASIA NIR-VIS 2.0 dataset.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {271–279},
numpages = {9},
keywords = {cross-modal dataset, face recognition, feature extraction},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3604078.3604158,
author = {Zhang, Manyu},
title = {Research on the application of AR technology in reconstructing the digital presentation of urban identity},
year = {2023},
isbn = {9798400708237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604078.3604158},
doi = {10.1145/3604078.3604158},
abstract = {Augmented reality (AR) technology is playing an important role in the ever-changing conservation of cultural heritage. As economic growth gathers pace, urban living spaces are increasingly being reorganised, and this raises the question of how people can find a sense of identity and community in the new urban settings. To enable people to fully understand their cultural heritage and help them discover a sense of identity in the changing urban social landscape. This will enable people to fully understand their cultural heritage and help them to discover a sense of identity in the changing urban social landscape. This study expanded the effect of popular science education, designed by using AR coding, and set out to evaluate its impact on people reconstructing their identity and sense of belonging to a community in a reconfigured Chinese city. Before developing the design program, using AR coding, the first step was to analyse examples of AR application and coding in existing popular science education. On completing the analysis, we tested our implemented system by carrying out a series of experiments, focussing on a survey of 918 vocational school students in China. The survey gathered data on satisfaction levels, and a statistical analysis was undertaken to determine the correlation between the competence of the students and the degree of satisfaction they reported. The results demonstrated that AR popular science education has the most significant effect on students’ overall educational satisfaction - followed by common competence, and finally information competence. The new program evidenced that it has the potential to improve student competence, and that AR coding can make a major contribution to design education.},
booktitle = {Proceedings of the 15th International Conference on Digital Image Processing},
articleno = {80},
numpages = {7},
location = {Nanjing, China},
series = {ICDIP '23}
}

@inproceedings{10.1145/3604078.3604110,
author = {Qin, Wei},
title = {Cyclostationary signal sensing algorithm based on principal component analysis and AdaBoost},
year = {2023},
isbn = {9798400708237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604078.3604110},
doi = {10.1145/3604078.3604110},
abstract = {At present, with the development of 5G technology, the bandwidth requirement of data communication is increasing, which makes the limited spectrum resources become more and more tight. Based on the current situation of the utilization and allocation of radio spectrum resources, the unused frequency bands have become less and less, and the shortage of radio spectrum resources is very serious. In this paper, a spectrum sensing algorithm based on principal component analysis (PCA) and AdaBoost is proposed to solve the problem of low detection rate of main user signal in wireless channel environment. Firstly, we extract the feature parameters of the signal by using the cyclostationary PCA algorithm, obtain the principal components of the signal, generate the samples, and construct the sample set, then the AdaBoost algorithm is used to classify and detect the signals in the presence and absence of the main user. The simulation results show that the proposed algorithm has better classification and detection performance compared with the artificial neural network and the max-min eigenvalue algorithm under low signal-to-noise ratio, the sensing of primary user signal is realized effectively.},
booktitle = {Proceedings of the 15th International Conference on Digital Image Processing},
articleno = {32},
numpages = {5},
keywords = {spectrum sensing, principal component analysis, cognitive network, AdaBoost},
location = {Nanjing, China},
series = {ICDIP '23}
}

@inproceedings{10.1145/3597638.3608430,
author = {Yamagami, Momona and Portnova-Fahreeva, Alexandra A and Kong, Junhan and Wobbrock, Jacob O. and Mankoff, Jennifer},
title = {How Do People with Limited Movement Personalize Upper-Body Gestures? Considerations for the Design of Personalized and Accessible Gesture Interfaces},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608430},
doi = {10.1145/3597638.3608430},
abstract = {Always-on, upper-body input from sensors like accelerometers, infrared cameras, and electromyography hold promise to enable accessible gesture input for people with upper-body motor impairments. When these sensors are distributed across the person’s body, they can enable the use of varied body parts and gestures for device interaction. Personalized upper-body gestures that enable input from diverse body parts including the head, neck, shoulders, arms, hands and fingers and match the abilities of each user, could be useful for ensuring that gesture systems are accessible. In this work, we characterize the personalized gesture sets designed by 25 participants with upper-body motor impairments and develop design recommendations for upper-body personalized gesture interfaces. We found that the personalized gesture sets that participants designed were highly ability-specific. Even within a specific type of disability, there were significant differences in what muscles participants used to perform upper-body gestures, with some predominantly using shoulder and upper-arm muscles, and others solely using their finger muscles. Eight percent of gestures that participants designed were with their head, neck, and shoulders, rather than their hands and fingers, demonstrating the importance of tracking the whole upper-body. To combat fatigue, participants performed 51\% of gestures with their hands resting on or barely coming off of their armrest, highlighting the importance of using sensing mechanisms that are agnostic to the location and orientation of the body. Lastly, participants activated their muscles but did not visibly move during 10\% of the gestures, demonstrating the need for using sensors that can sense muscle activations without movement. Both inertial measurement unit (IMU) and electromyography (EMG) wearable sensors proved to be promising sensors to differentiate between personalized gestures. Personalized upper-body gesture interfaces that take advantage of each person’s abilities are critical for enabling accessible upper-body gestures for people with upper-body motor impairments.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {1},
numpages = {15},
keywords = {accessibility, gestures, input, motor impairments},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{10.1145/3583780.3615464,
author = {Choudhuri, Akash and Jang, Hankyu and Segre, Alberto M. and Polgreen, Philip M. and Jha, Kishlay and Adhikari, Bijaya},
title = {Continually-Adaptive Representation Learning Framework for Time-Sensitive Healthcare Applications},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615464},
doi = {10.1145/3583780.3615464},
abstract = {Continual learning has emerged as a powerful approach to address the challenges of non-stationary environments, allowing machine learning models to adapt to new data while retaining the previously acquired knowledge. In time-sensitive healthcare applications, where entities such as physicians, hospital rooms, and medications exhibit continuous changes over time, continual learning holds great promise, yet its application remains relatively unexplored. This paper aims to bridge this gap by proposing a novel framework, i.e., Continually-Adaptive Representation Learning, designed to adapt representations in response to changing data distributions in evolving healthcare applications. Specifically, the proposed approach develops a continual learning strategy wherein the context information (e.g., interactions) of healthcare entities is exploited to continually identify and retrain the representations of those entities whose context evolved over time. Moreover, different from existing approaches, the proposed approach leverages the valuable patient information present in clinical notes to generate accurate and robust healthcare embeddings. Notably, the proposed continually-adaptive representations have practical benefits in low-resource clinical settings where it is difficult to training machine learning models from scratch to accommodate the newly available data streams. Experimental evaluations on real-world healthcare datasets demonstrate the effectiveness of our approach in time-sensitive healthcare applications such as Clostridioides difficile (C.diff) Infection (CDI) incidence prediction task and medical intensive care unit transfer prediction task.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {4538–4544},
numpages = {7},
keywords = {electronic healthcare records, dynamic embeddings, continual learning, clinical notes},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3583780.3614962,
author = {Cai, Zekun and Jiang, Renhe and Yang, Xinyu and Wang, Zhaonan and Guo, Diansheng and Kobayashi, Hill Hiroki and Song, Xuan and Shibasaki, Ryosuke},
title = {MemDA: Forecasting Urban Time Series with Memory-based Drift Adaptation},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614962},
doi = {10.1145/3583780.3614962},
abstract = {Urban time series data forecasting featuring significant contributions to sustainable development is widely studied as an essential task of the smart city. However, with the dramatic and rapid changes in the world environment, the assumption that data obey Independent Identically Distribution is undermined by the subsequent changes in data distribution, known as concept drift, leading to weak replicability and transferability of the model over unseen data. To address the issue, previous approaches typically retrain the model, forcing it to fit the most recent observed data. However, retraining is problematic in that it leads to model lag, consumption of resources, and model re-invalidation, causing the drift problem to be not well solved in realistic scenarios. In this study, we propose a new urban time series prediction model for the concept drift problem, which encodes the drift by considering the periodicity in the data and makes on-the-fly adjustments to the model based on the drift using a meta-dynamic network. Experiments on real-world datasets show that our design significantly outperforms state-of-the-art methods and can be well generalized to existing prediction backbones by reducing their sensitivity to distribution changes.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {193–202},
numpages = {10},
keywords = {urban computing, time series, domain adaptation, concept drift},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@article{10.1145/3605780,
author = {Wei, Bo and Xu, Weitao and Gao, Mingcen and Lan, Guohao and Li, Kai and Luo, Chengwen and Zhang, Jin},
title = {SolarKey: Battery-free Key Generation Using Solar Cells},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1550-4859},
url = {https://doi.org/10.1145/3605780},
doi = {10.1145/3605780},
abstract = {Solar cells have been widely used for offering energy for Internet of Things (IoT) devices. Recently, solar cells have also been used as sensors for context awareness sensing due to their sensitivity to varying lighting conditions. In this article, we are the first to use solar cells for symmetric key generation. To generate symmetric keys, we take advantage of photovoltage measurements generated from solar cells equipped with a pair of IoT devices. Symmetric keys are essential for pairing IoT devices and further securing wireless communication. Despite the sensitivity to varying lighting conditions, challenges still remain for the use of solar cells for key generation, such as time unsynchronisation and noisy measurements. To solve these challenges, we design a novel key generation framework, SolarKey, which includes the starting point detection and a compressed sensing-based two-tier key reconciliation method. Extensive experiments have been conducted to evaluate the performance of our proposed key generation method in various environments, which shows the proposed method can improve the key matching rate by up to 25\%. We also conduct security analysis and the randomness test, which shows that SolarKey is resilient to common attacks such as the eavesdropping attack and the imitating attack and sufficiently random.},
journal = {ACM Trans. Sen. Netw.},
month = oct,
articleno = {7},
numpages = {24},
keywords = {compressed sensing, solar cells, battery free, Key generation}
}

@inproceedings{10.1145/3608298.3608301,
author = {Li, Ai-Hsien Adams and Nguyen, Duc-Khanh and Lai, Yen-Jun and Chien, Ting-Ying and Chiu, Yen-Ling and Chan, Chien-Lung and Yang, Pan-Chyr},
title = {A deep learning approach to Lung Nodule Growth Prediction using CT image combined with Demographic and image features},
year = {2023},
isbn = {9798400700712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3608298.3608301},
doi = {10.1145/3608298.3608301},
abstract = {Lung cancer remains a significant global public health concern, being the leading cause of cancer-related deaths worldwide. Despite recent medical advancements, the disease still has a high mortality rate, making early detection and treatment critical for improving patient outcomes. Accurate prediction of nodule growth is key to early lung cancer treatment, but current assessments offer unclear indications of future growth, leading physicians to recommend costly and anxiety-provoking follow-up appointments every 3 to 12 months. To address this need, this study develops an ensemble deep learning approach that combines demographic data with existing CT images to predict whether lung nodules will grow, potentially reducing unnecessary examinations and easing the burden on patients. Our study, conducted on 862 patients with 1004 nodules, produced promising preliminary results with Accuracy, Sensitivity, Precision, F1 Score, and AUC of 0.66, 0.66, 0.67, 0.66, and 0.71, respectively. The proposed method provides a promising support system to empower patients to make informed decisions about seeking medical attention and helps physicians facilitate early treatment regimens, leading to improved patient outcomes and potentially saving lives.},
booktitle = {Proceedings of the 2023 7th International Conference on Medical and Health Informatics},
pages = {11–18},
numpages = {8},
keywords = {ensemble deep learning, lung nodule, nodule growth prediction},
location = {Kyoto, Japan},
series = {ICMHI '23}
}

@inproceedings{10.1145/3608298.3608351,
author = {Lee, Chin-Feng and Hsu, Chung-Wei and Lin, Iuon-Chang},
title = {A Novel Reversible Data Hiding Method with Contrast Enhancement for Color Medical Images},
year = {2023},
isbn = {9798400700712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3608298.3608351},
doi = {10.1145/3608298.3608351},
abstract = {Medical images contain sensitive information about patients' health conditions and medical history. Therefore, it is important to protect these images to ensure patient privacy and prevent unauthorized access, tampering, or theft of these sensitive medical records. Additionally, protecting medical images can help prevent medical identity theft, which is a growing concern in the healthcare industry. To address these concerns, data hiding with image contrast enhancement can be used to ensure that the hidden data is not easily detectable by unauthorized parties while still providing a visually appealing and interpretable medical image. The proposed method achieves higher performance in terms of embedding capacity as well as overall image quality compared to some previous methods.},
booktitle = {Proceedings of the 2023 7th International Conference on Medical and Health Informatics},
pages = {287–292},
numpages = {6},
location = {Kyoto, Japan},
series = {ICMHI '23}
}

@inproceedings{10.1145/3608298.3608323,
author = {Lee, Tian-Fu and Chu, Yen-Wei and Huang, Wei-Jie and Hsu, Chun-Wei and Chen, Zhi-Yang},
title = {Security Enhancement of Authentication Scheme for Smart Healthcare Services},
year = {2023},
isbn = {9798400700712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3608298.3608323},
doi = {10.1145/3608298.3608323},
abstract = {When a medical personnel attempts to access medical sensors on the patient end, an authentication scheme for smart healthcare services provides the medical personnel and the medical sensor to authenticate each other and establish a secure communication channel for healthcare data transmission through the help of medical server. Recently, Amintoosi et al. developed a lightweight authentication scheme for smart healthcare services to address previous security issues. The authors have proved the security of their scheme formally and showed that their scheme can resist possible attacks. However, this study demonstrates the security weaknesses of Amintoosi et al.'s scheme, which leads to their scheme may be subject to some potential attacks including sensor node capture attacks and impersonation attacks and session key security, and develops an enhanced authentication scheme based on Amintoosi et al.'s scheme. The enhanced authentication scheme not only overcomes the weaknesses of Amintoosi et al.'s scheme, but also retains a lower computational cost.},
booktitle = {Proceedings of the 2023 7th International Conference on Medical and Health Informatics},
pages = {129–135},
numpages = {7},
keywords = {Healthcare, authentication, key agreement, network security},
location = {Kyoto, Japan},
series = {ICMHI '23}
}

@inproceedings{10.1145/3624032.3624042,
author = {Coutinho, Jarbele C. S. and Andrade, Wilkerson L. and Machado, Patricia},
title = {Implementing Exploratory Testing in an Agile Context: A Study Based on Design Science Research},
year = {2023},
isbn = {9798400716294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624032.3624042},
doi = {10.1145/3624032.3624042},
abstract = {Exploratory Testing (ET) is a testing technique that has been spreading in the agile environment, as it allows professionals to learn quickly, adjust their tests and, in the process, find software problems that often need to be foreseen in test plans. However, the ET approaches evidenced by the literature are not applicable in the practical context of agile teams. In this sense, this paper proposes a method that facilitates the implementation of ET, considering the agile development scenario, the frequent change in requirements, and the generation of simple and robust artifacts. We use a problem-oriented research method called Design Science Research (DSR). Our main findings are that a method with well-defined activities facilitates the understanding and use of ET in the agile environment.},
booktitle = {Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {67–76},
numpages = {10},
keywords = {Software Testing, Exploratory Testing, Design Science Research, Agile Development},
location = {Campo Grande, MS, Brazil},
series = {SAST '23}
}

@inproceedings{10.1145/3624486.3624488,
author = {Arroyo Galende, Borja and Mata Naranjo, Juan and Uribe Mayoral, Silvia},
title = {Scalable and Portable Federated Learning Simulation Engine},
year = {2023},
isbn = {9798400708350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624486.3624488},
doi = {10.1145/3624486.3624488},
abstract = {Federated learning (FL) is one of the most promising approaches to ensure privacy in the application of data-driven techniques to sensitive information. However, the implementation of such approaches in a production environment is still an important challenge. In this paper, we present a scalable, portable, hardware-independent, model-agnostic FL Simulation Engine (FLSE) with the aim of easing the job of researchers who want to train FL models to be deployed in production environments. The FLSE offers a tool that can be used both standalone or embedded within a larger architecture, it can be deployed seamlessly and allows concurrent, scalable, and highly available V&amp;V assessment support for FL models. The tool allows researchers to understand the behaviour, in terms of metric performance, of their proposed models in production scenarios, allowing a boost in trustworthiness towards ethical AI.},
booktitle = {Proceedings of the 3rd Eclipse Security, AI, Architecture and Modelling Conference on Cloud to Edge Continuum},
pages = {10–14},
numpages = {5},
keywords = {Cloud Computing, Data Privacy, Federated Learning, Machine Learning, Portability, Scalability, Simulation},
location = {Ludwigsburg, Germany},
series = {eSAAM '23}
}

@inproceedings{10.1145/3615366.3615419,
author = {Pontes, Davi and Silva, Fernando and Falc\~{a}o, Eduardo and Brito, Andrey},
title = {Attesting AMD SEV-SNP Virtual Machines with SPIRE},
year = {2023},
isbn = {9798400708442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615366.3615419},
doi = {10.1145/3615366.3615419},
abstract = {SPIRE is an open-source project that enables the provisioning of verifiable identities to software components based on an attestation of the software properties, avoiding the leakage risks of pre-provisioned secrets. This paper presents an implementation of a SPIRE plugin that enables the attestation of AMD SEV-SNP confidential virtual machines. Our approach leverages the pluggable architecture from SPIRE and depends only on minor changes to QEMU, changes taken from its open-source community, and that should soon be merged. As a result, application providers can now use SPIRE to restrict sensitive credentials to be available only to services in environments protected from malicious hosts and cloud operators using AMD SEV-SNP technology. Our experiments show that the steps needed to create and attest the confidential VM do not prohibitively increase boot times (from 10.8 to 20.9 seconds) and that confidential VMs with encrypted disks only slightly degrade the CPU and RAM performance (about ) of unmodified applications.},
booktitle = {Proceedings of the 12th Latin-American Symposium on Dependable and Secure Computing},
pages = {1–10},
numpages = {10},
keywords = {confidential computing, SPIRE, AMD SEV-SNP},
location = {La Paz, Bolivia},
series = {LADC '23}
}

@inproceedings{10.1145/3614008.3614047,
author = {Xu, Ruixuan and Liu, Zhaoyu and Zhang, Shiya and Ke, Ting and Guo, Tingting},
title = {The measurement of liquid concentration based on Arduino UNO platform using ultrasonic wave sensor},
year = {2023},
isbn = {9798400707575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3614008.3614047},
doi = {10.1145/3614008.3614047},
abstract = {In this study, we combined the Arduino UNO development board with ULM_Plus ultrasonic sensor and used the integrated development environment Arduino IDE for serial data parsing and construct a new liquid concentration measuring instrument. We also used this instrument to measure NaCl solutions of different concentrations and verified the reliability and stability of the experimental setup.},
booktitle = {Proceedings of the 2023 6th International Conference on Signal Processing and Machine Learning},
pages = {253–259},
numpages = {7},
location = {Tianjin, China},
series = {SPML '23}
}

@inproceedings{10.1145/3614008.3614018,
author = {Shang, Xiurui and Zhang, Xueting and Zhang, Xinyue and Yuan, Bohao and Hao, Jianjun},
title = {An Efficient Signal enhancement scheme for Metamaterial-enhanced Magnetic Induction-based underground Wireless Sensor networks},
year = {2023},
isbn = {9798400707575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3614008.3614018},
doi = {10.1145/3614008.3614018},
abstract = {In underground coal mines, sensors are usually deployed in the roof rock layer or coal seam to monitor its states, and wireless sensors show its prominent advantages in massive sensor data acquisition. But these media cause large attenuation for transmitting signals. In view of the short distance of underground MI communication, the metamaterial enhanced magnetic induction communication (M2I) is proven to improve the coupling efficiency between the receiver and transmitter effectively, and further enhance the signal power. In this article, the M2I in soil environment is studied. Under the ideal condition, COMSOL Multiphysics simulation software is used to establish the two-dimensional axisymmetric magnetic communication transceiver model. The influence of different installation schemes of single-layer metamaterial plate (MP), three-layered composite MP, and bending MP on the induction voltage level at the receiver within 1m distance from the transmitter is studied, and the best installation method is determined. The results show that the MP placed on the front side of the transmitter coil with a gap of 10mm, three-layered composite MP placed on the front side with a gap of 3mm, and a single layer 160° bending plate on the front side of the coil with a gap of 16mm, are the optimal antenna schemes. When comparing the simulation results to other existing systems, it is found that the transmission characteristics are greatly improved. Compared with the traditional MI communication structure, the voltage value at the receiving end of these schemes is increased by 56.6dB, 65.6dB and 86.4dB respectively.},
booktitle = {Proceedings of the 2023 6th International Conference on Signal Processing and Machine Learning},
pages = {62–69},
numpages = {8},
keywords = {metamaterial, Wireless sensor, Underground communication, Negative permeability, Magnetic induction communication},
location = {Tianjin, China},
series = {SPML '23}
}

@article{10.1145/3622854,
author = {Iraci, Grant and Chuang, Cheng-En and Hu, Raymond and Ziarek, Lukasz},
title = {Validating IoT Devices with Rate-Based Session Types},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622854},
doi = {10.1145/3622854},
abstract = {We develop a session types based framework for implementing and validating rate-based message passing systems in Internet of Things (IoT) domains. To model the indefinite repetition present in many embedded and IoT systems, we introduce a timed process calculus with a periodic recursion primitive. This allows us to model rate-based computations and communications inherent to these application domains. We introduce a definition of rate based session types in a binary session types setting and a new compatibility relationship, which we call rate compatibility. Programs which type check enjoy the standard session types guarantees as well as rate error freedom --- meaning processes which exchanges messages do so at the same rate. Rate compatibility is defined through a new notion of type expansion, a relation that allows communication between processes of differing periods by synthesizing and checking a common superperiod type. We prove type preservation and rate error freedom for our system, and show a decidable method for type checking based on computing superperiods for a collection of processes. We implement a prototype of our type system including rate compatibility via an embedding into the native type system of Rust. We apply this framework to a range of examples from our target domain such as Android software sensors, wearable devices, and sound processing.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {278},
numpages = {29},
keywords = {type systems, session types, rate-based systems}
}

@inproceedings{10.1145/3565287.3616527,
author = {Samikwa, Eric and Sch\"{a}rer, Jakob and Braun, Torsten and Di Maio, Antonio},
title = {Machine Learning-based Energy Optimisation in Smart City Internet of Things},
year = {2023},
isbn = {9781450399265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565287.3616527},
doi = {10.1145/3565287.3616527},
abstract = {The deployment of Internet of Things (IoT) temperature sensors in urban areas is essential for the monitoring and understanding of the thermal environment. However, accurate temperature measurements can be compromised by factors such as direct sunlight, leading to overheating and inaccurate readings. We propose a Machine Learning-based approach that addresses this challenge by dynamically ventilating the sensor environment using small fans, enabling accurate and energy-efficient temperature measurements. This paper focuses on two interconnected problems: predicting steady-state temperature using a limited window of initial temperature measurements and investigating the impact of ventilation time. We employ various DNNs suitable for low-power IoT sensor devices to predict temperature using multivariate time series from different sensors and compare their accuracy. Furthermore, we highlight the tradeoff between prediction accuracy, which is correlated to the length of the observed input sequence, and energy consumption dependent on ventilation time. By adopting advanced prediction techniques, we can develop efficient IoT systems for accurate and energy-efficient environment monitoring in smart cities.},
booktitle = {Proceedings of the Twenty-Fourth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {364–369},
numpages = {6},
keywords = {temperature monitoring, smart sensors, energy optimisation, internet of things, machine learning},
location = {Washington, DC, USA},
series = {MobiHoc '23}
}

@inproceedings{10.1145/3565287.3610270,
author = {Li, Xin and Yang, Yilin and Ye, Zhengkun and Wang, Yan and Chen, Yingying},
title = {EarCase: Sound Source Localization Leveraging Mini Acoustic Structure Equipped Phone Cases for Hearing-challenged People},
year = {2023},
isbn = {9781450399265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565287.3610270},
doi = {10.1145/3565287.3610270},
abstract = {Sound source localization is vital for daily tasks such as communication or navigating environments. However, millions of adults struggle with hearing impairment, which limits their ability to identify the direction and distance of sound sources. Traditional methods for sound spatial sensing, such as microphone arrays, are not suitable for resource-constrained IoT devices like smartphones due to power consumption or hardware complexity. To overcome these limitations, this paper proposes EarCase, an alternative scheme that utilizes commercial smartphones with only two microphones to recognize 3D acoustic spatial information. EarCase draws inspiration from the human auditory system, where two ears amplify minute differences in acoustic signals to help pinpoint sound sources. This ability can be regarded as a response function trained through a large amount of sound source information, which can be used to extract spectral cues from a sound source position to the ears drums. We imitate this effect by designing a smartphone case with perforated mini-structures covering the microphones to help the smartphone infer the location of the sound source. Sound waves that pass through the mini-structure will undergo unique changes in diffraction at the hole, amplifying directional information similar to ears. Our scheme uses the top and bottom microphones to eliminate noises and multi-path effects, making the design robust to different sound sources in varying environments. By using only built-in microphones and low-cost phone cases, EarCase provides an accessible tool to enhance the quality of life for hearing impaired individuals. Extensive experimental results show that EarCase achieves high accuracy in localizing sounds, with a mean error of 3.7° at a distance of 200cm and 96\% accuracy for real-world sounds (e.g., car horns).},
booktitle = {Proceedings of the Twenty-Fourth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {240–249},
numpages = {10},
keywords = {smartphone cases, acoustic sensing, sound source localization},
location = {Washington, DC, USA},
series = {MobiHoc '23}
}

@inproceedings{10.1145/3565287.3623388,
author = {Chatterjee, Pushpita and Das, Debashis and Banerjee, Sourav and Ghosh, Uttam and Mpembele, Armando B. and Rogers, Tamara},
title = {An Approach Towards the Security Management for Sensitive Medical Data in the IoMT Ecosystem},
year = {2023},
isbn = {9781450399265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565287.3623388},
doi = {10.1145/3565287.3623388},
abstract = {The Internet of Medical Things (IoMT) is a network of interconnected medical devices, wearables, and sensors integrated into healthcare systems. It enables real-time data collection and transmission using smart medical devices with trackers and sensors. IoMT offers various benefits to healthcare, including remote patient monitoring, improved precision, and personalized medicine, enhanced healthcare efficiency, cost savings, and advancements in telemedicine. However, with the increasing adoption of IoMT, securing sensitive medical data becomes crucial due to potential risks such as data privacy breaches, compromised health information integrity, and cybersecurity threats to patient information. It is necessary to consider existing security mechanisms and protocols and identify vulnerabilities. The main objectives of this paper aim to identify specific threats, analyze the effectiveness of security measures, and provide a solution to protect sensitive medical data. In this paper, we propose an innovative approach to enhance security management for sensitive medical data using blockchain technology and smart contracts within the IoMT ecosystem. The proposed system aims to provide a decentralized and tamper-resistant platform that ensures data integrity, confidentiality, and controlled access. By integrating blockchain into the IoMT infrastructure, healthcare organizations can significantly enhance the security and privacy of sensitive medical data.},
booktitle = {Proceedings of the Twenty-Fourth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {400–405},
numpages = {6},
keywords = {big data analytics, healthcare, authentication, privacy, data security, internet of medical things},
location = {Washington, DC, USA},
series = {MobiHoc '23}
}

@article{10.1145/3615867,
author = {Kim, Dohyun and Cho, Mangi and Shin, Hocheol and Kim, Jaehoon and Noh, Juhwan and Kim, Yongdae},
title = {Lightbox: Sensor Attack Detection for Photoelectric Sensors via Spectrum Fingerprinting},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {2471-2566},
url = {https://doi.org/10.1145/3615867},
doi = {10.1145/3615867},
abstract = {Photoelectric sensors are utilized in a range of safety-critical applications, such as medical devices and autonomous vehicles. However, the public exposure of the input channel of a photoelectric sensor makes it vulnerable to malicious inputs. Several studies have suggested possible attacks on photoelectric sensors by injecting malicious signals. While a few defense techniques have been proposed against such attacks, they could be either bypassed or used for limited purposes.In this study, we propose Lightbox, a novel defense system to detect sensor attacks on photoelectric sensors based on signal fingerprinting. Lightbox uses the spectrum of the received light as a feature to distinguish the attacker’s malicious signals from the authentic signal, which is a signal from the sensor’s light source. We evaluated Lightbox against (1) a saturation attacker, (2) a simple spoofing attacker, and (3) a sophisticated attacker who is aware of Lightbox and can combine multiple light sources to mimic the authentic light source. Lightbox achieved the overall accuracy over 99\% for the saturation attacker and simple spoofing attacker, and robustness against a sophisticated attacker. We also evaluated Lightbox considering various environments such as transmission medium, background noise, and input waveform. Finally, we demonstrate the practicality of Lightbox with experiments using a single-board computer after further reducing the training time.},
journal = {ACM Trans. Priv. Secur.},
month = oct,
articleno = {46},
numpages = {30},
keywords = {signal fingerprinting, neural networks, Photoelectric sensors}
}

@inproceedings{10.1145/3607822.3614538,
author = {Waghmare, Anandghan and Boldu, Roger and Whitmire, Eric and Kienzle, Wolf},
title = {OptiRing: Low-Resolution Optical Sensing for Subtle Thumb-to-Index Micro-Interactions},
year = {2023},
isbn = {9798400702815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607822.3614538},
doi = {10.1145/3607822.3614538},
abstract = {We present OptiRing, a ring-based wearable device that enables subtle single-handed micro-interactions using low-resolution camera-based sensing. We demonstrate that our approach can work with ultra-low image resolutions (e.g., 5 \texttimes{} 5 pixels), which is instrumental in addressing privacy concerns and reducing computational needs. Using a miniature camera, OptiRing supports thumb-to-index finger gestures, such as stateful pinch and left/right swipes, as well as continuous 1-DOF input. We present a modeling approach that uses heuristic-based methods to identify interactions and machine learning for input gating, generalizing recognition across users and sessions. We assess this technique’s capabilities, accuracy, and limitations through a user study with 15 participants. OptiRing achieved 93.1\% accuracy in gesture recognition, 99.8\% accuracy for stateful pinch gestures, and a minimal number of false positives. Further, we validate OptiRing’s ability to handle continuous 1D input in a Fitts’ law study. We discuss these findings, the tradeoff between resolution and interaction accuracy, and the potential of this technology, which can help address challenges associated with optical sensing for input.},
booktitle = {Proceedings of the 2023 ACM Symposium on Spatial User Interaction},
articleno = {8},
numpages = {13},
keywords = {Camera, Human-Computer Interaction, Input, Privacy, Wearable Computing},
location = {Sydney, NSW, Australia},
series = {SUI '23}
}

@inproceedings{10.1145/3616195.3616205,
author = {Christensen, Justin and Kauenhofen, Shawn and Loehr, Janeen D and Lang, Jennifer and Peacock, Shelley and Nicol, Jennifer J},
title = {MMM Duet System: New accessible musical technology for people living with dementia},
year = {2023},
isbn = {9798400708183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616195.3616205},
doi = {10.1145/3616195.3616205},
abstract = {Music offers a meaningful way for people living with dementia to interact with others and can provide health and wellbeing benefits. Enjoying shared activities helps couples affected by dementia retain a sense of couplehood and can support a spousal caregiver’s mental health. This paper describes the development of the Music Memory Makers (MMM) Duet System, a prototype that has been developed as part of a qualitative, multi-phase, iterative research study to test its feasibility for use with people living with dementia and their spousal caregivers. Through the iterative process, the diverse individual needs of the participants directly led to the adding, adjusting, or removal of features and components to better fit their needs and to make the system require as little technical experience from the users as possible for quick and easy engagement. In line with our work of developing system hardware and software to meet users’ needs, including 3D printed cases, coordination facilitation processes, a visual interface, and source separation tools to create familiar duets, participants found the duet system offered them an opportunity to enjoyably interact with one another by playing meaningful songs together.},
booktitle = {Proceedings of the 18th International Audio Mostly Conference},
pages = {38–44},
numpages = {7},
keywords = {music technology, joint agency, design, dementia},
location = {Edinburgh, United Kingdom},
series = {AM '23}
}

@inproceedings{10.1145/3615834.3615841,
author = {Schumann, Robert and Li, Frederic and Grzegorzek, Marcin},
title = {WiFi Sensing with Single-Antenna Devices for Ambient Assisted Living},
year = {2023},
isbn = {9798400708169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615834.3615841},
doi = {10.1145/3615834.3615841},
abstract = {The absolute coverage WiFi networks is higher than it has ever been and WiFi sensing offers a device-free, contactless alternative to intrusive wearable devices for a variety of applications, in particular for ambient assisted living (AAL). However, a majority of sensing systems proposed in literature use multiple antennas for sensing, resulting in high cost that hinders development of such solutions in real life. This work surveys existing single-antenna systems as a low-cost solution in an AAL scenario. The capabilities of these systems are examined regarding their practical applicability based on testing in AAL environments, leveraging multiple links (i.e. transmitter-receiver pairs) and considering mobile and repositioned receivers. It is found that while the AAL use-cases of respiration monitoring, fall detection and activity recognition are realised by existing systems, further testing in realistic AAL environments with the inclusion of activities of daily living is needed. Additionally the full use of multiple links and consideration of a mobile receiver is still rare, but shows promising improvements. A multi-task system enabling all three applications is discussed using the Model for Ethical Evaluation of Socio-Technical Arrangements (MEESTAR). We suggest an ethically sensitive use, but identify a need for mitigation strategies to address privacy-related concerns of potentially unwilling users.},
booktitle = {Proceedings of the 8th International Workshop on Sensor-Based Activity Recognition and Artificial Intelligence},
articleno = {3},
numpages = {8},
keywords = {WiFi sensing, ambient assisted living, social aspects of security and privacy, ubiquitous computing},
location = {L\"{u}beck, Germany},
series = {iWOAR '23}
}

@inproceedings{10.1145/3615834.3615845,
author = {Jablonski, Lennart and Jensen, Torge and Ahlemann, Greta M. and Huang, Xinyu and Tetzlaff-Lelleck, Vivian V. and Piet, Artur and Schmelter, Franziska and Dinkler, Valerie S. and Sina, Christian and Grzegorzek, Marcin},
title = {Sensor-Based Detection of Food Hypersensitivity Using Machine Learning},
year = {2023},
isbn = {9798400708169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615834.3615845},
doi = {10.1145/3615834.3615845},
abstract = {The recognition of physiological reactions with the help of machine learning methods already plays a major role in many research areas, but is still little represented in the field of food hypersensitivity recognition. The present work addresses the question of how food hypersensitivity can be detected by analysing sensor data with explainable machine learning algorithms. In a first step, the Empatica E4 wristband, a wearable device that can be easily integrated into everyday life, collects raw data on various physiological patterns, and algorithms are implemented to extract a variety of features from the raw data. Subsequently, machine learning methods are used to target this classification problem and examine how food hypersensitivity can be detected using these objectively measurable features. In a subject-independent setup, an accuracy of 91\% could be achieved, which provides a promising basis for a new non-invasive and objectively measurable method to detect food hypersensitivity.},
booktitle = {Proceedings of the 8th International Workshop on Sensor-Based Activity Recognition and Artificial Intelligence},
articleno = {23},
numpages = {8},
keywords = {adverse reaction to food, carbohydrate malassimilation, classification, explainable AI, feature engineering, food hypersensitivity, machine learning, precision nutrition, random forest, sensor-based, time series analysis},
location = {L\"{u}beck, Germany},
series = {iWOAR '23}
}

@inproceedings{10.1145/3577190.3614120,
author = {Gemicioglu, Tan and Winters, R. Michael and Wang, Yu-Te and Gable, Thomas M. and Tashev, Ivan J.},
title = {TongueTap: Multimodal Tongue Gesture Recognition with Head-Worn Devices},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577190.3614120},
doi = {10.1145/3577190.3614120},
abstract = {Mouth-based interfaces are a promising new approach enabling silent, hands-free and eyes-free interaction with wearable devices. However, interfaces sensing mouth movements are traditionally custom-designed and placed near or within the mouth. TongueTap synchronizes multimodal EEG, PPG, IMU, eye tracking and head tracking data from two commercial headsets to facilitate tongue gesture recognition using only off-the-shelf devices on the upper face. We classified eight closed-mouth tongue gestures with 94\% accuracy, offering an invisible and inaudible method for discreet control of head-worn devices. Moreover, we found that the IMU alone differentiates eight gestures with 80\% accuracy and a subset of four gestures with 92\% accuracy. We built a dataset of 48,000 gesture trials across 16 participants, allowing TongueTap to perform user-independent classification. Our findings suggest tongue gestures can be a viable interaction technique for VR/AR headsets and earables without requiring novel hardware.},
booktitle = {Proceedings of the 25th International Conference on Multimodal Interaction},
pages = {564–573},
numpages = {10},
keywords = {BCI, hands-free, non-intrusive, tongue gestures, tongue interface},
location = {Paris, France},
series = {ICMI '23}
}

@inproceedings{10.1145/3577190.3614109,
author = {Xiao, Zixuan and Muszynski, Michal and Marcinkevi\v{c}s, Ri\v{c}ards and Zimmerli, Lukas and Ivankay, Adam Daniel and Kohlbrenner, Dario and Kuhn, Manuel and Nordmann, Yves and Muehlner, Ulrich and Clarenbach, Christian and Vogt, Julia E. and Brunschwiler, Thomas},
title = {Breathing New Life into COPD Assessment: Multisensory Home-monitoring for Predicting Severity},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577190.3614109},
doi = {10.1145/3577190.3614109},
abstract = {Chronic obstructive pulmonary disease (COPD) is a significant public health issue, affecting more than 100 million people worldwide. Remote patient monitoring has shown great promise in the efficient management of patients with chronic diseases. This work presents the analysis of the data from a monitoring system developed to track COPD symptoms alongside patients’ self-reports. In particular, we investigate the assessment of COPD severity using multisensory home-monitoring device data acquired from 30 patients over a period of three months. We describe a comprehensive data pre-processing and feature engineering pipeline for multimodal data from the remote home-monitoring of COPD patients. We develop and validate predictive models forecasting i) the absolute and ii) differenced COPD Assessment Test (CAT) scores based on the multisensory data. The best obtained models achieve Pearson’s correlation coefficient of 0.93 and 0.37 for absolute and differenced CAT scores. In addition, we investigate the importance of individual sensor modalities for predicting CAT scores using group sparse regularization techniques. Our results suggest that feature groups indicative of the patient’s general condition, such as static medical and physiological information, date, spirometer, and air quality, are crucial for predicting the absolute CAT score. For predicting changes in CAT scores, sleep and physical activity features are most important, alongside the previous CAT score value. Our analysis demonstrates the potential of remote patient monitoring for COPD management and investigates which sensor modalities are most indicative of COPD severity as assessed by the CAT score. Our findings contribute to the development of effective and data-driven COPD management strategies.},
booktitle = {Proceedings of the 25th International Conference on Multimodal Interaction},
pages = {84–93},
numpages = {10},
keywords = {chronic obstructive pulmonary disease, digital health, grouped feature importance, healthcare, predictive analysis, remote patient monitoring, time series},
location = {Paris, France},
series = {ICMI '23}
}

@inproceedings{10.1145/3577190.3614129,
author = {Kammoun, Nathan and Meegahapola, Lakmal and Gatica-Perez, Daniel},
title = {Understanding the Social Context of Eating with Multimodal Smartphone Sensing: The Role of Country Diversity},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577190.3614129},
doi = {10.1145/3577190.3614129},
abstract = {Understanding the social context of eating is crucial for promoting healthy eating behaviors. Multimodal smartphone sensor data could provide valuable insights into eating behavior, particularly in mobile food diaries and mobile health apps. However, research on the social context of eating with smartphone sensor data is limited, despite extensive studies in nutrition and behavioral science. Moreover, the impact of country differences on the social context of eating, as measured by multimodal phone sensor data and self-reports, remains under-explored. To address this research gap, our study focuses on a dataset of approximately 24K self-reports on eating events provided by 678 college students in eight countries to investigate the country diversity that emerges from smartphone sensors during eating events for different social contexts (alone or with others). Our analysis revealed that while some smartphone usage features during eating events were similar across countries, others exhibited unique trends in each country. We further studied how user and country-specific factors impact social context inference by developing machine learning models with population-level (non-personalized) and hybrid (partially personalized) experimental setups. We showed that models based on the hybrid approach achieve AUC scores up to 0.75 with XGBoost models. These findings emphasize the importance of considering country differences in building and deploying machine learning models to minimize biases and improve generalization across different populations.},
booktitle = {Proceedings of the 25th International Conference on Multimodal Interaction},
pages = {604–612},
numpages = {9},
keywords = {eating behavior, mobile food diary, mobile health, multimodal sensor data, passive sensing, smartphone sensing, social context},
location = {Paris, France},
series = {ICMI '23}
}

@inproceedings{10.1145/3611659.3615689,
author = {Morimoto, Kosuke and Hashiura, Kenta and Watanabe, Keita},
title = {Effect of Virtual Hand's Fingertip Deformation on the Stiffness Perceived Using Pseudo-Haptics},
year = {2023},
isbn = {9798400703287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611659.3615689},
doi = {10.1145/3611659.3615689},
abstract = {In this study, using a novel method for haptic presentation based on pseudo-haptics, the perceived stiffness was visually altered by changing the fingertips shape of a virtual hand, as users engaged with objects in a VR environment. While past approaches have primarily focused on instigating such sensations through object deformation, we focused on how an individual’s fingertips deform upon making contact with an object. In this study, we investigated pseudo-haptics based on the deformation of the fingertips of a virtual hand. In Experiment 1, we determined how the shape deformation of a virtual hand’s fingertip affected the sense of body ownership. The experiment determined that the maximum change in the fingertip width should be 2.25 times. In Experiment 2, subjects touched a virtual object in the VR space and evaluated the perception of the stiffness of the virtual object. The results confirmed that when the deformation of the fingertip shape of the virtual hand was small, the object was perceived as hard, whereas when it was large, the object was perceived as soft. These results indicated that a haptic presentation is possible without using a haptic device that restricts user movement, which will users could broaden the range of natural interactions in VR spaces.},
booktitle = {Proceedings of the 29th ACM Symposium on Virtual Reality Software and Technology},
articleno = {32},
numpages = {10},
keywords = {haptics illusions, pseudo-haptics, virtual reality},
location = {Christchurch, New Zealand},
series = {VRST '23}
}

@inproceedings{10.1145/3577190.3614173,
author = {Matheus, Kayla and Mamantov, Ellie and V\'{a}zquez, Marynel and Scassellati, Brian},
title = {Deep Breathing Phase Classification with a Social Robot for Mental Health},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577190.3614173},
doi = {10.1145/3577190.3614173},
abstract = {Social robots are in a unique position to aid mental health by supporting engagement with behavioral interventions. One such behavioral intervention is the practice of deep breathing, which has been shown to physiologically reduce symptoms of anxiety. Multiple robots have been recently developed that support deep breathing, but none yet implement a method to detect how accurately an individual is performing the practice. Detecting breathing phases (i.e., inhaling, breath holding, or exhaling) is a challenge with these robots since often the robot is being manipulated or moved by the user, or the robot itself is moving to generate haptic feedback. Accordingly, we first present OMMDB: a novel, multimodal, public dataset made up of individuals performing deep breathing with an Ommie robot in multiple conditions of robot ego-motion. The dataset includes RGB video, inertial sensor data, and motor encoder data, as well as ground truth breathing data from a respiration belt. Our second contribution features experimental results with a convolutional long-short term memory neural network trained using OMMDB. These results show the system’s ability to be applied to the domain of deep breathing and generalize between individual users. We additionally show that our model is able to generalize across multiple types of robot ego-motion, reducing the need to train individual models for varying human-robot interaction conditions.},
booktitle = {Proceedings of the 25th International Conference on Multimodal Interaction},
pages = {153–162},
numpages = {10},
keywords = {anxiety, datasets, deep breathing, human-robot interaction, mental health, multimodal datasets, social robotics, vital signs},
location = {Paris, France},
series = {ICMI '23}
}

