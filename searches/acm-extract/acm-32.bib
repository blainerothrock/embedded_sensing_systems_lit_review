@inproceedings{10.1145/3543873.3587682,
author = {Rickens, Bryan and Tonekaboni, Navid Hashemi},
title = {Towards High Resolution Urban Heat Analysis: Incorporating Thermal Drones to Enhance Satellite Based Urban Heatmaps},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587682},
doi = {10.1145/3543873.3587682},
abstract = {As remote-sensing becomes more actively utilized in the environmental sciences, our research continues the efforts in adapting smart cities by using civilian UAVs and drones for land surface temperature (LST) analysis. Given the increased spatial resolution that this technology provides as compared to standard satellite measurements, we sought to further study the urban heat island (UHI) effect – specifically when it comes to heterogeneous and dynamic landscapes such as the Charleston peninsula. Furthermore, we sought to develop a method to enhance the spatial resolution of publicly available LST temperature data (such as those measured from the Landsat satellites) by building a machine learning model utilizing remote-sensed data from drones. While we found a high correlation and an accurate degree of prediction for areas of open water and vegetation (respectively), our model struggled when it came to areas containing highly impervious surfaces. We believe, however, that these findings further illustrate the discrepancy between high and medium spatial resolutions, and demonstrate how urban environments specifically are prone to inaccurate LST measurements and are uniquely in need of an industry pursuit of higher spatial resolution for hyperlocal environmental sciences and urban analysis.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {707–714},
numpages = {8},
keywords = {Landsat, Remote sensing, UAV, drone, high spatial resolution., land surface temperature, smart cities, urban heat island},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3544793.3560399,
author = {Rothrock, Blaine and Curtiss, Alexander and Bai, Juyang and Hester, Josiah},
title = {Towards a Toolkit for Free Living Wearable Development},
year = {2023},
isbn = {9781450394239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544793.3560399},
doi = {10.1145/3544793.3560399},
abstract = {Real-world Data collection and analysis is a significant pain point in wearable device development, requiring multidisciplinary skills in: embedded systems, application development, data science, and domain expert knowledge. In this work, we first build motivation based on previous experiences in wearable development, then introduce a toolkit for data collection and iterative development to reduce engineering efforts for free living experimentation of wearable devices. This toolkit utilizes Bluetooth Low Energy and an adaptive mobile application to help researchers quickly test new hardware, collect meaningful data, and assist in developing embedded algorithms with minimal intermediary code changes. We demonstrate the utility of our toolkit by collecting data in-the-wild from multiple sensors using a prototype wearable and a ground truth heart rate sensor. In addition, we demonstrate the toolkit’s capabilities with a baseline throughput test. Finally, we show how this tool has helped in early development of a new custom device. Our work is released as open source and welcomes contributions in an effort to broaden the tookit’s utility for the wearable research community.},
booktitle = {Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers},
pages = {379–385},
numpages = {7},
keywords = {Health, Machine Learning, Wearables},
location = {Cambridge, United Kingdom},
series = {UbiComp/ISWC '22 Adjunct}
}

@inproceedings{10.1145/3577923.3583648,
author = {Islam, Md Shihabul and Zamani, Mahmoud and Kim, Chung Hwan and Khan, Latifur and Hamlen, Kevin W.},
title = {Confidential Execution of Deep Learning Inference at the Untrusted Edge with ARM TrustZone},
year = {2023},
isbn = {9798400700675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577923.3583648},
doi = {10.1145/3577923.3583648},
abstract = {This paper proposes a new confidential deep learning (DL) inference system with ARM TrustZone to provide confidentiality and integrity of DL models and data in an untrusted edge device with limited memory. Although ARM TrustZone supplies a strong, hardware-supported trusted execution environment for protecting sensitive code and data in an edge device against adversaries, resource limitations in typical edge devices have raised significant challenges for protecting on-device DL requiring large memory consumption without sacrificing the security and accuracy of the model. The proposed solution addresses this challenge without modifying the protected DL model, thereby preserving the original prediction accuracy. Comprehensive experiments using different DL architectures and datasets demonstrate that inference services for large and complex DL models can be deployed in edge devices with TrustZone with limited trusted memory, ensuring data confidentiality and preserving the original model's prediction exactness.},
booktitle = {Proceedings of the Thirteenth ACM Conference on Data and Application Security and Privacy},
pages = {153–164},
numpages = {12},
keywords = {deep learning, embedded device, trusted execution environment},
location = {Charlotte, NC, USA},
series = {CODASPY '23}
}

@inproceedings{10.1145/3544793.3560409,
author = {Wang, Danyang and Weng, Jianhao and Zou, Yongpan and Wu, Kaishun},
title = {EmoTracer: A Wearable Physiological and Psychological Monitoring System With Multi-modal Sensors},
year = {2023},
isbn = {9781450394239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544793.3560409},
doi = {10.1145/3544793.3560409},
abstract = {The monitoring of physical and mental health has been the focus of attention. At the same time, with the advancement of wearable sensors and information acquisition technology, there is no longer any satisfaction with the centralized, fixed-point traditional physiological indicator collection methods. Physiological indicator monitoring systems based on portable devices and mobile collection are gradually becoming a priority for research. Based on these needs, we present a wearable human physiological indicator monitoring system called EmoTracer. Through the self-developed and well-designed wearable collection device, body temperature, blood oxygen, heart rate and electrical skin signals are transmitted to the mobile terminal via Bluetooth communication; the mobile terminal then stores and calculates the data and displays the changes in physiological indicators in a visualized form in real-time. At the same time, we conducted an emotion recognition experiment (N = 10) based on our device, which can reach an average accuracy of 91.23\%, making a preliminary exploration in the direction of emotion perception.},
booktitle = {Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers},
pages = {444–449},
numpages = {6},
keywords = {Body sensor, Multimodal data, Ubiquitous computing, Wearable computing},
location = {Cambridge, United Kingdom},
series = {UbiComp/ISWC '22 Adjunct}
}

@inproceedings{10.1145/3544793.3560391,
author = {Liu, Mengxi and Suh, Sungho and Zhou, Bo and Gruenerbl, Agnes and Lukowicz, Paul},
title = {Smart-Badge: A wearable badge with multi-modal sensors for kitchen activity recognition},
year = {2023},
isbn = {9781450394239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544793.3560391},
doi = {10.1145/3544793.3560391},
abstract = {Human health is closely associated with their daily behavior and environment. However, keeping a healthy lifestyle is still challenging for most people as it is difficult to recognize their living behaviors and identify their surrounding situations to take appropriate action. Human activity recognition is a promising approach to building a behavior model of users, by which users can get feedback about their habits and be encouraged to develop a healthier lifestyle. In this paper, we present a smart light wearable badge with six kinds of sensors, including an infrared array sensor MLX90640 offering privacy-preserving, low-cost, and non-invasive features, to recognize daily activities in a realistic unmodified kitchen environment. A multi-channel convolutional neural network (MC-CNN) based on data and feature fusion methods is applied to classify 14 human activities associated with potentially unhealthy habits. Meanwhile, we evaluate the impact of the infrared array sensor on the recognition accuracy of these activities. We demonstrate the performance of the proposed work to detect the 14 activities performed by ten volunteers with an average accuracy of 92.44 \% and an F1 score of 88.27 \%.},
booktitle = {Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers},
pages = {356–363},
numpages = {8},
keywords = {Kitchen Activity Recognition, Multi-sensor Wearable Device, Sensor Fusion},
location = {Cambridge, United Kingdom},
series = {UbiComp/ISWC '22 Adjunct}
}

@inproceedings{10.1145/3544793.3563425,
author = {Barish, Greg and Ijadi-Maghsoodi, Roya and Lavelle Trinh, Carla and Alvarez, Jinger and Hobson, Brianna J. and Ollen, Elizabeth and Karim, Brittany and Kataoka, Sheryl and Lester, Patricia},
title = {Sensing school community needs: a co-designed, personalized mental health app for high school students, parents, and staff},
year = {2023},
isbn = {9781450394239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544793.3563425},
doi = {10.1145/3544793.3563425},
abstract = {The need to scale proven approaches towards the mental health care of students in under-resourced, urban public school districts grows increasingly urgent. To help address this challenge, we are developing Connectd for Schools, a mobile app aimed at providing personalized digital and local resources towards the mental health needs of high school students, parents, and teachers in large, urban school systems. The app builds on an established community partnered participatory approach to the design, development, and deployment of resilience-focused prevention interventions for school populations, co-designed by school and academic partners. This novel approach is designed to deploy a personalized and localized privacy-preserving, community sensing technology within a large, urban public school setting to three distinct audiences, with the potential for improving linkage to established behavioral health care pipelines. Along with the opportunities of this app come some challenges: scaling participatory design, the potential for mislabeled data, and community trust. The release of Connectd for Schools will occur in Fall of 2022; we share how we got to this point, and the next steps ahead.},
booktitle = {Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers},
pages = {478–482},
numpages = {5},
keywords = {Behavioral health, High schools, Mobile applications, Recommender systems, Wellbeing},
location = {Cambridge, United Kingdom},
series = {UbiComp/ISWC '22 Adjunct}
}

@inproceedings{10.1145/3544793.3561318,
author = {Nijholt, Anton},
title = {Perceptual Modifications in Augmented Reality: A Short Survey},
year = {2023},
isbn = {9781450394239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544793.3561318},
doi = {10.1145/3544793.3561318},
abstract = {Views on Augmented Reality (AR) and its technology have changed. Rather than being a research area on its own, with a focus on computer vision and computer-generated imagery, we now have to deal with the integration of AR technology with ubiquitous computing where we have sensors, actuators, and processing capabilities beyond what is possible with a single AR device. The plethora of AR applications that follow from this point of view has not yet fully been investigated. In this paper, we review how in reported AR research this issue is dealt with. We survey the attempts from the AR community to integrate perception technology with AR. These attempts should be added to and integrated with augmented human and multisensorial research that nowadays is being done in the context of ubiquitous computing (Internet of Things, Ambient Intelligence, smart environments).},
booktitle = {Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers},
pages = {520–523},
numpages = {4},
keywords = {Augmented reality, augmented humans, diminished reality, ever-present augmented reality, eyewear, perceptual modifications, smart environments, societal concerns, ubiquitous computing},
location = {Cambridge, United Kingdom},
series = {UbiComp/ISWC '22 Adjunct}
}

@inproceedings{10.1145/3577923.3583641,
author = {Abu Jabal, Amani and Bertino, Elisa and Lobo, Jorge and Verma, Dinesh and Calo, Seraphin and Russo, Alessandra},
title = {FLAP - A Federated Learning Framework for Attribute-based Access Control Policies},
year = {2023},
isbn = {9798400700675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577923.3583641},
doi = {10.1145/3577923.3583641},
abstract = {Technology advances in areas such as sensors, IoT, and robotics, enable new collaborative applications (e.g., autonomous devices). A primary requirement for such collaborations is to have a secure system that enables information sharing and information flow protection. A policy-based management system is a key mechanism for secure selective sharing of protected resources. However, policies in each party of a collaborative environment cannot be static as they have to adapt to different contexts and situations. One advantage of collaborative applications is that each party in the collaboration can take advantage of the knowledge of the other parties for learning or enhancing its own policies. We refer to this learning mechanism as policy transfer. The design of a policy transfer framework has challenges, including policy conflicts and privacy issues. Policy conflicts typically arise because of differences in the obligations of the parties, whereas privacy issues result because of data sharing constraints for sensitive data. Hence, the policy transfer framework should be able to tackle such challenges by considering minimal sharing of data and supporting policy adaptation to address conflict. In the paper, we propose a framework that aims at addressing such challenges. We introduce a formal definition of the policy transfer problem for attribute-based access control policies. We then introduce the transfer methodology which consists of three sequential steps. Finally, we report experimental results.},
booktitle = {Proceedings of the Thirteenth ACM Conference on Data and Application Security and Privacy},
pages = {263–272},
numpages = {10},
keywords = {access control, adaptation, coalitions, machine learning},
location = {Charlotte, NC, USA},
series = {CODASPY '23}
}

@inproceedings{10.1145/3544793.3563422,
author = {Stuchbury-Wass, Jake and Ferlini, Andrea and Mascolo, Cecilia},
title = {Multimodal Attention Networks for Human Activity Recognition From Earable Devices},
year = {2023},
isbn = {9781450394239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544793.3563422},
doi = {10.1145/3544793.3563422},
abstract = {Earables (a.k.a ear-worn wearable devices) are gaining traction in the wearables ecosystem for monitoring user health. Human activity recognition (HAR) is a promising use case of earables due to their placement on the head and the combination of sensors. In this paper, we explore using multimodal attention-based neural networks for HAR from the ear. Attention networks have had a large impact on other disciplines’ machine learning tasks and we believe they present opportunities in HAR from earable data. Different methods of utilising attention mechanisms in the literature are discussed as well as the benefits and challenges of using such networks in the context of HAR on real systems.},
booktitle = {Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers},
pages = {258–260},
numpages = {3},
location = {Cambridge, United Kingdom},
series = {UbiComp/ISWC '22 Adjunct}
}

@inproceedings{10.1145/3544793.3563413,
author = {R\"{o}ddiger, Tobias and Clarke, Christopher and Breitling, Paula and Schneegans, Tim and Zhao, Haibin and Gellersen, Hans and Beigl, Michael},
title = {Excerpt from “Sensing with Earables: A Systematic Literature Review and Taxonomy of Phenomena”},
year = {2023},
isbn = {9781450394239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544793.3563413},
doi = {10.1145/3544793.3563413},
abstract = {By adding sensing capabilities to ear-worn devices, earables have emerged as a new platform. The ears are located closely to a number of important anatomical structures (e.g., brain, blood vessels). Also, the ear canal deforms upon facial movements and the ears can be comfortably touched by the hands. In a recent paper, we conducted a systematic literature review of 271 earable papers. We synthesized an open-ended taxonomy of 47 phenomena that can be sensed in, on, or around the ear. We identified 13 fundamental phenomena from which all other phenomena can be derived, and discuss sensing principles to detect them. The phenomena were reviewed in-depth in four main areas: (i) physiological monitoring and health, (ii) movement and activity, (iii) interaction, and (iv) authentication and identification. This breadth highlights the potential earables have to offer as a ubiquitous, general-purpose platform.},
booktitle = {Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers},
pages = {244–245},
numpages = {2},
keywords = {ear wearable, ear-attached, ear-based, ear-mounted, ear-worn, earables, earbuds, earphones, earpiece, headphones, hearables},
location = {Cambridge, United Kingdom},
series = {UbiComp/ISWC '22 Adjunct}
}

@inproceedings{10.1145/3544793.3563427,
author = {Alchieri, Leonardo and Abdalazim, Nouran and Alecci, Lidia and Gashi, Shkurta and Di Lascio, Elena and Santini, Silvia},
title = {On the Impact of Lateralization in Physiological Signals from Wearable Sensors},
year = {2023},
isbn = {9781450394239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544793.3563427},
doi = {10.1145/3544793.3563427},
abstract = {Wearable devices enable the continuous and unobtrusive monitoring of physiological data, e.g., electrodermal activity (EDA), and they allow to build machine learning models to recognize human emotions, stress, and more. However, the quality of the collected data can significantly impact the performance of such models. When wrist-worn sensors are used, this may happen due to differences in the signal collected on the left and right wrist. In this work, we quantify the impact of physiological signal lateralization in a laughter recognition task. Building upon an existing dataset from 34 users, we devise a laughter recognition classifier and compare the performance of models trained and tested with data from different wrists. Our results show that, when using EDA, classification performance might depend on the side used for training and testing. Our quantification of lateralization on model performance provides insights for the design of EDA-based models as well as of data collection studies.},
booktitle = {Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers},
pages = {472–477},
numpages = {6},
keywords = {Correlation Analysis, Effect Size, Lateralization Analysis, Laughter Recognition},
location = {Cambridge, United Kingdom},
series = {UbiComp/ISWC '22 Adjunct}
}

@inproceedings{10.1145/3544793.3563412,
author = {Alecci, Lidia and Abdalazim, Nouran and Alchieri, Leonardo and Gashi, Shkurta and Santini, Silvia},
title = {On the mismatch between measured and perceived sleep quality},
year = {2023},
isbn = {9781450394239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544793.3563412},
doi = {10.1145/3544793.3563412},
abstract = {Sleep quality has a significant impact on human’s physical and mental well-being. While there exist several definitions of what makes a good quantity and quality of sleep, such measures are not always in agreement with each other. Further, they often do not match with the human perception of a restorative sleep. In this paper, we investigate the correlation between objective sleep quality measurements – which can be sensed using wearable devices – and the sleeper’s subjective – i.e., self-reported – sleep quality. We analyze the M2Sleep dataset, which contains sleep data of 16 participants over one month, and show that only few sensor-based measures (sleep duration, skin temperature, tonic component of electrodermal activity, acceleration skewness and kurtosis) and personality-based features (extroversion, agreeableness, PSQI score) correlate with self-reported sleep quality. These results may inform the design of future studies on sleep quality assessment.},
booktitle = {Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers},
pages = {148–152},
numpages = {5},
keywords = {Correlation Analysis, Objective, Sleep Quality, Subjective, Wearable Sensors},
location = {Cambridge, United Kingdom},
series = {UbiComp/ISWC '22 Adjunct}
}

@inproceedings{10.1145/3544793.3560396,
author = {Hotta, Junya and Murao, Kazuya},
title = {A Method for Estimating Temperature of Grasped Object using PPG Sensor},
year = {2023},
isbn = {9781450394239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544793.3560396},
doi = {10.1145/3544793.3560396},
abstract = {Wearable devices can obtain a variety of information, such as pulse waves, by measuring biometric data. They are applied to many services, including support for the elderly and medical assistance. In the measurement of pulse waves, capillaries dilate and constrict due to changes in skin temperature, which may result in changes in blood flow. Since changes in blood flow are the same as changes in pulse wave, it can be inferred that there is a correlation between skin temperature and pulse sensor readings, which changes depending on the temperature of the object being grasped. In this study, we propose a method for estimating the temperature of a grasped object using a photoplethysmography (PPG) sensor installed in a smartwatch or activity meter. By estimating the grasped object temperature, cold burns and temperature-related sensory disturbances such as unconsciously holding a hot object for a long time in people who have no pain sense due to congenital lack of pain perception or in the elderly who have no pain sense due to a functional decline in the cerebrum caused by dementia can be prevented. In this study, we propose a method to estimate the temperature of an object being touched using a PPG sensor. The proposed method measures the pulse wave from the PPG sensor and estimates the temperature from the rate of change of the difference between the peak of the pulse wave after touching the object and the pulse wave under normal conditions. The results of the temperature estimation experiment for liquids on four subjects showed that the proposed method could estimate the temperature with a 78.1\% correct rate, and for solids on two subjects the rate was 75\%.},
booktitle = {Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers},
pages = {335–339},
numpages = {5},
keywords = {Object temperature estimation, PPG sensor, pulse wave},
location = {Cambridge, United Kingdom},
series = {UbiComp/ISWC '22 Adjunct}
}

@inproceedings{10.1145/3544548.3581289,
author = {Daepp, Madeleine I. G. and Cabral, Alex and Werner, Tiffany M and Mansour, Raed and Catlett, Charlie and Roseway, Asta and Needham, Chuck and Udeagbala, Nneka and Counts, Scott},
title = {The “Three-Legged Stool": Designing for Equitable City, Community, and Research Partnerships in Urban Environmental Sensing},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581289},
doi = {10.1145/3544548.3581289},
abstract = {Urban environmental monitoring campaigns depend on expertise from city agencies, residents, and researchers. Deployment efforts rarely include all three stakeholders, typically leading to initiatives that struggle to produce credible, actionable data. We describe the implementation of a large-scale, long-term air quality sensing network in Chicago&nbsp; Illinois; detail stakeholder interviews and meetings; and present three interfaces—–a website accessible via in-situ QR codes, APIs, and a mobile, mixed-media experience. We show how a collaborative approach created a more equitable sensor distribution compared to crowdsourced or regulatory designs. We highlight shared goals of education, engagement, and empowerment despite the diversity of tool and analytics needs across stakeholder groups. Reflecting on our work, we develop a “three-legged stool” framework representing the criticality of balanced participation from three key stakeholder groups—city, community, and research—in deploying novel urban technologies. This approach can help HCI researchers facilitate more democratic technology deployments in urban spaces.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {526},
numpages = {19},
keywords = {Collaborative Design, Design Frameworks, Environmental Monitoring, Smart Cities, User Interfaces},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581256,
author = {Margariti, Eleni and Vlachokyriakos, Vasilis and Kirk, David},
title = {Understanding occupants’ experiences in quantified buildings: results from a series of exploratory studies.},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581256},
doi = {10.1145/3544548.3581256},
abstract = {Quantified smart buildings increasingly utilise data-rich technologies (such as embedded sensors and personal wearables). Research and development however, rarely addresses occupants’ experiences and expectations in such environments, which is critical for designing ethical and occupant-centred workspaces. To support the design of human-centred smart buildings, a series of 4 workshops was conducted with a total of 27 participants, over 2 months, with occupants of a smart office building. Workshops used discursive (focus group) and projective (design fiction) techniques to qualitatively explore occupants’ perceptions of and concerns around the collection, processing and use of data within the building. Workshop data was thematically analysed, resulting in design implications for improving occupant experience in current smart workplaces, while also contributing implications for increasing the perceivability, accessibility and usability of data in such buildings. Contributing to discourses around Human-Building Interaction the paper concludes with discussion of future research challenges for occupant-centred development of quantified buildings.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {640},
numpages = {15},
keywords = {Human-Building Interaction, Qualitative Methods, Workplaces},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581118,
author = {Bartle, Vince and Albright, Liam and Dell, Nicola},
title = {"This machine is for the aides": Tailoring Voice Assistant Design to Home Health Care Work},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581118},
doi = {10.1145/3544548.3581118},
abstract = {This paper explores how the design of interactive voice assistants (IVAs) might be tailored to support home health aides’ important work in complex home care contexts. We designed two custom IVAs: one that looks like an aide’s medical kit and one that blends into the home environment. We also designed a voice-based application that provides aides with guidance for day-to-day tasks and for performing a medical assessment. Via a lab-based study with 25 aides and seven patients, we explore how tailoring the IVAs’ design to home health care might impact its acceptability as a work device, enabling cooperative work among aides and clients, while potentially causing conflict that will require IVA designers to decide whose values to prioritize. We also highlight limits in aides’ power to control IVAs in clients’ homes. Finally, we discuss implications for designing privacy-preserving IVAs, including leveraging IVAs’ physical design to enact privacy mechanisms and opportunities to build ‘always on’ IVAs for privacy-sensitive contexts like home health care.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {227},
numpages = {19},
keywords = {Digital Health, Home Care Context, Interactive Voice Assistant, Internet of Things, Technology Probes},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580908,
author = {Ryu, Hyeyoung and Berry, Andrew B.L. and Lim, Catherine Y and Hartzler, Andrea and Hirsch, Tad and Trejo, Juanita I and Bermet, Zo\"{e} Abigail and Crawford-Gallagher, Brandi and Tran, Vi and Ferguson, Dawn and Cronkite, David J and Tiffany, Brooks and Weeks, John and Ralston, James},
title = {“You Can See the Connections”: Facilitating Visualization of Care Priorities in People Living with Multiple Chronic Health Conditions},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580908},
doi = {10.1145/3544548.3580908},
abstract = {Individuals with multiple chronic health conditions (MCC) often face an overwhelming set of self-management work, resulting in a need to set care priorities. Yet, much self-management work is invisible to healthcare providers. This study aimed to understand how to support the development and sharing of connections between personal values and self-management tasks through the facilitated use of an interactive visualization system: Conversation Canvas. We conducted a field study with 13 participants with MCC, 3 caregivers, and 7 primary care providers in Washington State. Analysis of interviews with MCC participants showed that developing visualizations of connections between personal values, self-management tasks, and health conditions helped individuals make sense of connections relevant to their health and wellbeing, recognize a road map of central issues and their impacts, feel respected and understood, share priorities with providers, and support value-aligned changes. These findings demonstrated potential for the guided process and visualization to support priorities-aligned care.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {473},
numpages = {17},
keywords = {multiple chronic health conditions, patient priorities care, patient-clinician communication, reflection, sensemaking, values, visualization},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580747,
author = {Yeo, Hui-Shyong and Wu, Erwin and Kim, Daehwa and Lee, Juyoung and Kim, Hyung-il and Oh, Seo Young and Takagi, Luna and Woo, Woontack and Koike, Hideki and Quigley, Aaron John},
title = {OmniSense: Exploring Novel Input Sensing and Interaction Techniques on Mobile Device with an Omni-Directional Camera},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580747},
doi = {10.1145/3544548.3580747},
abstract = {An omni-directional (360°) camera captures the entire viewing sphere surrounding its optical center. Such cameras are growing in use to create highly immersive content and viewing experiences. When such a camera is held by a user, the view includes the user’s hand grip, finger, body pose, face, and the surrounding environment, providing a complete understanding of the visual world and context around it. This capability opens up numerous possibilities for rich mobile input sensing. In OmniSense, we explore the broad input design space for mobile devices with a built-in omni-directional camera and broadly categorize them into three sensing pillars: i) near device ii) around device and iii) surrounding device. In addition we explore potential use cases and applications that leverage these sensing capabilities to solve user needs. Following this, we develop a working system to put these concepts into action, by leveraging these sensing capabilities to enable potential use cases and applications. We studied the system in a technical evaluation and a preliminary user study to gain initial feedback and insights. Collectively these techniques illustrate how a single, omni-purpose sensor on a mobile device affords many compelling ways to enable expressive input, while also affording a broad range of novel applications that improve user experience during mobile interaction.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {530},
numpages = {18},
keywords = {360° camera, Omni-directional, input sensing, interaction technique.},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581295,
author = {Guo, Zengrong and Liang, Rong-Hao},
title = {TexonMask: Facial Expression Recognition Using Textile Electrodes on Commodity Facemasks},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581295},
doi = {10.1145/3544548.3581295},
abstract = {This paper presents TexonMask, a facial expression recognition system using lightweight electrode-augmented commodity facemasks. With a matrix of textile electrodes carefully deployed on a commodity mask, our edge computing system recognizes the wearer’s facial expressions with machine learning based on the capacitive sensor readings, provides a wearable affective display and communicates with external devices using low bandwidth. Results from user studies show that the system is effective and efficient at recognizing five or ten facial expressions with an accuracy of around , using a personalized classifier trained with only six data points per expression. The system’s performance is stable across the use sessions and further improves when more data points are collected. We further developed two LiveEmoji applications for facilitating online and face-to-face communication of facemask wearers, demonstrated them in user interviews, and obtained positive participant feedback. Based on the results and findings of the study, we discuss implications and future research directions for facilitating emotional communication between facemask wearers and others.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {627},
numpages = {15},
keywords = {capacitive sensing, edge computing, embedded interaction, facemasks, facial expression recognition, textile electrodes, wearable},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3584376.3584495,
author = {Liu, Haodong},
title = {Design of Pet temperature and humidity control system based on microcontroller},
year = {2023},
isbn = {9781450398343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584376.3584495},
doi = {10.1145/3584376.3584495},
abstract = {The companionship of pets can help the owner's mental health to a large extent, reduce the stress of the owner, and give the owner a sense of security and need. However, the tension of the pace of life sometimes makes the owner ignore the improvement of the pet's living environment, which is not conducive to the health of the pet. Therefore, the monitoring and control of the temperature and humidity of the pet living environment and the light environment is very important. In this design, the high reliability of the more comprehensive research is adopted, and the microcontroller processor STC89C52 is the core, which improves the temperature and humidity of the pet environment Implement functions for sub-function design. The main functions of the design include temperature and humidity acquisition, display and automatic control adjustment. For the selection of equipment, with the goal of operational stability and cost performance, the corresponding equipment is selected, and finally the temperature and humidity adjustment is realized. At the same time, for the corresponding software process, the molecular programming and the main program synthesis are adopted to finally ensure that the pet temperature and humidity and control system functions can be realized and the system can operate perfectly. The system devices designed in this paper are cost-effective, have clear circuit structures and have strong operational stability. The design of this article can be widely used in family life to benefit the living environment of pets.},
booktitle = {Proceedings of the 2022 4th International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {674–679},
numpages = {6},
location = {Dongguan, China},
series = {RICAI '22}
}

@inproceedings{10.1145/3544548.3581088,
author = {Meegahapola, Lakmal and Constantinides, Marios and Radivojevic, Zoran and Li, Hongwei and Quercia, Daniele and Eggleston, Michael S},
title = {Quantified Canine: Inferring Dog Personality From Wearables},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581088},
doi = {10.1145/3544548.3581088},
abstract = {Being able to assess dog personality can be used to, for example, match shelter dogs with future owners, and personalize dog activities. Such an assessment typically relies on experts or psychological scales administered to dog owners, both of which are costly. To tackle that challenge, we built a device called “Patchkeeper” that can be strapped on the pet’s chest and measures activity through an accelerometer and a gyroscope. In an in-the-wild deployment involving 12 healthy dogs, we collected 1300 hours of sensor activity data and dog personality test results from two validated questionnaires. By matching these two datasets, we trained ten machine learning classifiers that predicted dog personality from activity data, achieving AUCs in [0.63-0.90], suggesting the value of tracking psychological signals of pets using wearable technologies.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {855},
numpages = {19},
keywords = {activity level, behavior modeling, dog activity recognition, dog personality, passive sensing, wearables},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3584376.3584386,
author = {Wu, Changzhong and Wang, Shengqiang and Fan, Wenchao and Tai, Wentao},
title = {Design of VISUAL SLAM indoor disinfection robot based on STM32 control},
year = {2023},
isbn = {9781450398343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584376.3584386},
doi = {10.1145/3584376.3584386},
abstract = {In recent years, the novel corona virus pandemic is raging around the world, and the safety of home environment and public environment has become the focus of people's attention [2]. Therefore, the research on disinfection robot has become one of the important directions in the field of machinery and artificial intelligence. This paper proposes a robot with the STM32 MCU as the core of disinfection, and is equipped with a variety of sensors and a camera vision, has the original cloud service management platform, the remote deployment of navigation, based on visual SLAM to realize high precision navigation and positioning, can realize to indoor environment autonomously route planning, automatic obstacle avoidance checking, disinfection, epidemic prevention function, at the same time can pass Bit computer software realizes remote control of robot, which has great development potential.},
booktitle = {Proceedings of the 2022 4th International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {50–53},
numpages = {4},
location = {Dongguan, China},
series = {RICAI '22}
}

@inproceedings{10.1145/3544548.3580692,
author = {Yu, Tianhong Catherine and Arakawa, Riku and McCann, James and Goel, Mayank},
title = {uKnit: A Position-Aware Reconfigurable Machine-Knitted Wearable for Gestural Interaction and Passive Sensing using Electrical Impedance Tomography},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580692},
doi = {10.1145/3544548.3580692},
abstract = {A scarf is inherently reconfigurable: wearers often use it as a neck wrap, a shawl, a headband, a wristband, and more. We developed uKnit, a scarf-like soft sensor with scarf-like reconfigurability, built with machine knitting and electrical impedance tomography sensing. Soft wearable devices are comfortable and thus attractive for many human-computer interaction scenarios. While prior work has demonstrated various soft wearable capabilities, each capability is device- and location-specific, being incapable of meeting users’ various needs with a single device. In contrast, uKnit explores the possibility of one-soft-wearable-for-all. We describe the fabrication and sensing principles behind uKnit, demonstrate several example applications, and evaluate it with 10-participant user studies and a washability test. uKnit achieves 88.0\%/78.2\% accuracy for 5-class worn-location detection and 80.4\%/75.4\% accuracy for 7-class gesture recognition with a per-user/universal model. Moreover, it identifies respiratory rate with an error rate of 1.25 bpm and detects binary sitting postures with an average accuracy of 86.2\%.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {628},
numpages = {17},
keywords = {Electrical Impedance Tomography, Gestural Interaction, Machine Knitting, Reconfigurable Wearable, Smart Textile},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580738,
author = {Sehrt, Jessica and Wi\ss{}mann, Tim and Breitenbach, Jan and Schwind, Valentin},
title = {The Effects of Body Location and Biosignal Feedback Modality on Performance and Workload Using Electromyography in Virtual&nbsp;Reality},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580738},
doi = {10.1145/3544548.3580738},
abstract = {Using biosignals through electromyography (EMG) and rendering them as feedback for hands-free interaction finally migrates to engaging virtual reality (VR) experiences for health and fitness-related applications. Previous work proposes various body locations as input sources and different output modalities for creating effective biofeedback loops. However, it is currently unknown which muscles and sensory modalities can provide optimal real-time interaction regarding the performance and perceived workload of the users. In two VR studies (N=18 and N=40) based on a Fitts’ law target selection task, we explored sensor placement at different body locations and investigate auditory, tactile, and visual feedback modalities. Objective and subjective results indicate that input performance can be improved by presenting muscle tension as simultaneous tactile and visual feedback. We contribute with recommendations for registration of isometric muscle contraction at different body locations and conclude that reproducing physiological feedback through multimodal channels can assist users interacting with EMG devices.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {84},
numpages = {16},
keywords = {Accessibility, Biofeedback, Electromyography, Physiological Sensing, Virtual Reality},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580755,
author = {Kleinberger, Rebecca and Van Troyer, Akito Oshiro and Wang, Qian Janice},
title = {Auditory Seasoning Filters: Altering Food Perception via Augmented Sonic Feedback of Chewing Sounds},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580755},
doi = {10.1145/3544548.3580755},
abstract = {The experience of what we eat depends not only on the taste of the food, but also on other modalities of sensory feedback. Perceptual research has shown the potential of altering visual, olfactory, and textural food cues to affect flavor, texture, and satiety. Recently, the HCI community has leveraged such research to encourage healthy eating, but the resulting tools often require specialised and/or invasive devices. Ubiquitous and unobtrusive, audio feedback-based tools could alleviate those drawbacks, but research in this area has been limited to food texture. We expand on prior psychology research by exploring a wide range of auditory feedback styles to modify not only flavor attributes but also appetite-related measures. We present Auditory Seasoning, a mobile app that offers various curated audio modes to alter chewing sounds. In a Pringles-tasting experiment (N=37), this tool significantly influenced food perception and eating behavior beyond texture alone. Based on these results, we discuss design implications to create custom real-world flavor/satiety-enhancing tools.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {318},
numpages = {15},
keywords = {auditory feedback, closed-loop system, crossmodal correspondences, food},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581292,
author = {Uhl, Jakob Carl and Schrom-Feiertag, Helmut and Regal, Georg and Gallhuber, Katja and Tscheligi, Manfred},
title = {Tangible Immersive Trauma Simulation: Is Mixed Reality the next level of medical skills training?},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581292},
doi = {10.1145/3544548.3581292},
abstract = {In medical simulation training two approaches are currently rather disjunct: realistic manikins are used to teach physical skills and procedures and VR systems are used to train situation assessment and decision making. We propose a mixed reality approach, which allows trainees to use real tools and their hands when interacting with a physical manikin overlaid with a responsive virtual avatar. In close exchange with first responder organizations, we developed and evaluated an MR training scenario. In the scenario, users can talk to injured people in a car accident, assess the threat of the environment, and utilize real medical equipment. Participants experienced high levels of physical- and self-presence, increased stress levels, and reported a high technology acceptance. The proposed main requirements of first responders regarding haptic multi-sensory skill training in MR and the lessons learned from the workshop aim to guide the design of training solutions for medical training in MR.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {513},
numpages = {17},
keywords = {first responder, haptic feedback, mixed reality, presence, training},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3581264,
author = {Xu, Chenhan and Zhou, Bing and Krishnan, Gurunandan and Nayar, Shree},
title = {AO-Finger: Hands-free Fine-grained Finger Gesture Recognition via Acoustic-Optic Sensor Fusing},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581264},
doi = {10.1145/3544548.3581264},
abstract = {Finger gesture recognition is gaining great research interest for wearable device interactions such as smartwatches and AR/VR headsets. In this paper, we propose a hands-free fine-grained finger gesture recognition system AO-Finger based on acoustic-optic sensor fusing. Specifically, we design a wristband with a modified stethoscope microphone and two high-speed optic motion sensors to capture signals generated from finger movements. We propose a set of natural, inconspicuous and effortless micro finger gestures that can be reliably detected from the complementary signals from both sensors. We design a multi-modal CNN-Transformer model for fast gesture recognition (flick/pinch/tap), and a finger swipe contact detection model to enable fine-grained swipe gesture tracking. We built a prototype which achieves an overall accuracy of 94.83\% in detecting fast gestures and enables fine-grained continuous swipe gestures tracking. AO-Finger is practical for use as a wearable device and ready to be integrated into existing wrist-worn devices such as smartwatches.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {306},
numpages = {14},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3544548.3580811,
author = {Cheng, Tingyu and Tabb, Taylor and Park, Jung Wook and Gallo, Eric M and Maheshwari, Aditi and Abowd, Gregory D. and Oh, Hyunjoo and Danielescu, Andreea},
title = {Functional Destruction: Utilizing Sustainable Materials’ Physical Transiency for Electronics Applications},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580811},
doi = {10.1145/3544548.3580811},
abstract = {Today’s electronics are manufactured to provide stable functionality and fixed physical forms optimized for reliable operation over long periods and repeated use. However, even when applications don’t call for such robustness, the permanency of these electronics comes with environmental consequences. In this paper, we describe an alternative approach that utilizes sustainable transient electronics whose method of destruction is also key to their functionality. We create these electronics through three different methods: 1) by inkjet printing conductive silver traces on poly(vinyl alcohol) (PVA) substrates to create water-soluble sensors; 2) by mixing a conductive beeswax material configured as a meltable sensor; and 3) by fabricating edible electronics with 3D printed chocolate and culinary gold leaf. To enable practical applications of these devices, we implement a fully transient and sustainable chipless RF detection system.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {366},
numpages = {16},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3584376.3584541,
author = {Guo, Lina and Sun, Liwen and Zhu, Jinfeng},
title = {Design of Intelligent Control System and Positioning Algorithm for Deployable Wheeled Mobile Robot},
year = {2023},
isbn = {9781450398343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584376.3584541},
doi = {10.1145/3584376.3584541},
abstract = {With the increasing difficulty of tasks to be performed by robots and the more complex working environment, wheeled mobile robots must improve their adaptability to achieve real autonomy, which puts forward higher requirements for control theory and positioning technology. In order to solve the shortcomings of the existing intelligent control system design and positioning algorithm research of the deployable wheeled mobile robot, this paper discusses the controller hardware selection and the overall performance index of the deployable wheeled mobile robot based on the discussion of the intelligent control system mode of the deployable wheeled mobile robot and FastSLAM2.0 and Kalman filtering positioning algorithm, And the design of intelligent control system and positioning algorithm are discussed. Finally, through the comparison experiment between the multisensor information fusion technology and Kalman filtering algorithm designed in this paper and FastSLAM algorithm. The experimental results show that the positioning accuracy of multi-sensor information fusion technology and Hermann filtering algorithm is above 98\% on average. Therefore, the design of intelligent control system and positioning algorithm of deployable wheeled mobile robot designed in this paper has high practical value.},
booktitle = {Proceedings of the 2022 4th International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {924–927},
numpages = {4},
location = {Dongguan, China},
series = {RICAI '22}
}

@inproceedings{10.1145/3584376.3584595,
author = {Hao, Hongyi},
title = {Researches Advanced in Visual Simultaneously Localization and Mapping based on Deep Learning},
year = {2023},
isbn = {9781450398343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584376.3584595},
doi = {10.1145/3584376.3584595},
abstract = {Simultaneous localization and mapping (SLAM) has always been a research hotspot in the field of mobile robots. It uses the camera as the main sensor to model the environment during the movement, aiming to help the agent perceive the surrounding environment. Traditional SLAM research mainly focuses on static or low-speed moving objects. For the dynamic scene, because there is no prior information about the environment when estimating its motion, the accuracy and robustness of the system cannot meet the requirements of practical applications. In recent years, due to the development of deep learning technology, visual SLAM has made an unprecedented breakthrough. This paper will discuss recent developments in the use of deep learning for visual SLAM and provide an overview of the key findings from studies that replaced one or more standard SLAM and semantic SLAM modules with deep learning. Finally, the future development of deep learning-based visual SLAM is considered.},
booktitle = {Proceedings of the 2022 4th International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {1241–1246},
numpages = {6},
location = {Dongguan, China},
series = {RICAI '22}
}

@inproceedings{10.1145/3584376.3584531,
author = {Pei, Yunjie and Chen, Jinlong and Yang, Minghao and Zhang, Jiaqing},
title = {Loop Closure in 2D LIDAR and RGB-D SLAM},
year = {2023},
isbn = {9781450398343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584376.3584531},
doi = {10.1145/3584376.3584531},
abstract = {According to the advantages of different sensors, it can improve the speed and accuracy of Simultaneous Localization and Mapping (SLAM) based on multi-sensor information. However, with the accumulation of time, the error will gradually increase due to the factors of environment and hardware in Mapping. Therefore, the paper proposes a loop closure algorithm based on the combination of 2D LIDAR and RGB-D camera. The algorithm will create a grid map by the use of 2D LIDAR, and use a coarse-to-fine matching method on the candidate region. Firstly, it will get the RGB-D image of the current location and compare the global descriptors of the RGB image using the similarity evaluation based on distance. In the initial screening stage, part of the RGB image in the map storage with a high similarity to the RGB image at the current location will be selected. Secondly, use local descriptors to compare and extract the most similar RGB image, thus establish the corresponding relationship between the RGB images. Finally, the depth image corresponding to the RGB image is segmented through the local descriptors, and adjust the relative pose according to the point cloud matching technology to obtain a robust loop closure algorithm.},
booktitle = {Proceedings of the 2022 4th International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {867–872},
numpages = {6},
keywords = {multi-sensor information, loop closure, Simultaneous Localization and Mapping},
location = {Dongguan, China},
series = {RICAI '22}
}

@inproceedings{10.1145/3584376.3584412,
author = {Yang, Wenxuan},
title = {Researches Advanced in Autonomous Underwater Robots based on SLAM},
year = {2023},
isbn = {9781450398343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584376.3584412},
doi = {10.1145/3584376.3584412},
abstract = {Simultaneous Localization and Mapping (SLAM) has always been a research hotspot in the field of computer vision and robotics, which aims to predict the position of a robot and use various sensors to perceive its surrounding environment information to build a map and complete navigation. The technology of SLAM has been widely applied in different fileds, especially the autonomous underwater vehicles (AUVs). AUVs can replace humans in various dangerous operations and exploration work underwater to assist humans in underwater development and research. In the past 20 years, benifited from the rapid development of deep learning and SLAM, AUV developed rapidly and has become a mainstream oceanographic exploration tool. At present, due to the complexity of underwater environment, the research on the precision navigation of AUV based on SLAM is still an open issue. Taking China and the United States as examples, this paper first introduced the representative AUV-SLAM algorithm and its development. Secondly, this paper also quantitatively compares the performance of different underwater SLAM algorithms and analyzes their application difficulties. Finally, the future development trend of AUV-SLAM is discussed.},
booktitle = {Proceedings of the 2022 4th International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {200–203},
numpages = {4},
location = {Dongguan, China},
series = {RICAI '22}
}

@inproceedings{10.1145/3544548.3581265,
author = {Ahmed, Tousif and Rahman, Md Mahbubur and Nemati, Ebrahim and Ahmed, Mohsin Yusuf and Kuang, Jilong and Gao, Alex Jun},
title = {Remote Breathing Rate Tracking in Stationary Position Using the Motion and Acoustic Sensors of Earables},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581265},
doi = {10.1145/3544548.3581265},
abstract = {Breathing rate is critical for the user’s respiratory health and is hard to track outside the clinical context, requiring specialized devices. Earables could provide a convenient solution to track the breathing rate anywhere by leveraging the user’s breathing-related motion and sound captured through the earables’ motion sensors and microphones. However, small non-breathing head movements or background noises during the assessment affect the estimation accuracy. While noise filtering improves accuracy, it can discard valid measurements. This paper presents a multimodal approach to tracking the user’s breathing rate using a signal-processing-based algorithm on motion sensors and a lightweight machine-learning algorithm on acoustic sensors from the earables that balances the accuracy and data retention. A user study with 30 participants shows that the system can accurately calculate breathing rate (Mean Absolute Error &lt; 2 breaths per minute) while retaining most breathing sessions (75\%) performed in real-world settings. This work provides an essential direction for remote breathing rate monitoring.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {325},
numpages = {22},
keywords = {Breathing, Breathing Rate, Hearable, Remote Monitoring},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3584376.3584489,
author = {Chen, Bowen and Zhang, Hui and Sun, Xuan and Duan, Derong},
title = {Intelligent fitness system design based on ESP32 and human posture recognition},
year = {2023},
isbn = {9781450398343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584376.3584489},
doi = {10.1145/3584376.3584489},
abstract = {Aiming at the fixed and single function of fitness device, this system designs an intelligent fitness system that monitors the human body status in real time by the lower computer and judges whether the fitness action is standard by the upper computer. The system uses ESP32 as the main control chip and obtains environmental information and user's body data in real time through temperature and humidity sensor, pH strength sensor and ECG detection sensor. The lower computer uploads the data to the upper computer through UDP communication protocol and visualizes the data. The upper computer connects to the monocular camera and identifies and determines whether the user's fitness movements are standard or not by using the human posture estimation algorithm in deep learning and gives tips. The user performs the exercise according to the prompts. The system ensures the efficiency of fitness and increases the fun of human-computer interaction.},
booktitle = {Proceedings of the 2022 4th International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {642–646},
numpages = {5},
location = {Dongguan, China},
series = {RICAI '22}
}

@inproceedings{10.1145/3544548.3581422,
author = {Waghmare, Anandghan and Ben Taleb, Youssef and Chatterjee, Ishan and Narendra, Arjun and Patel, Shwetak},
title = {Z-Ring: Single-Point Bio-Impedance Sensing for Gesture, Touch, Object and User Recognition},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581422},
doi = {10.1145/3544548.3581422},
abstract = {We present Z-Ring, a wearable ring that enables gesture input, object detection, user identification, and interaction with passive user interface (UI) elements using a single sensing modality and a single point of instrumentation on the finger. Z-Ring uses active electrical field sensing to detect changes in the hand’s electrical impedance caused by finger motions or contact with external surfaces. We develop a diverse set of interactions and evaluate them with 21 users. We demonstrate: (1) Single- and two-handed gesture recognition with up to 93\% accuracy (2) Tangible input with a set of passive touch UI elements, including buttons, a continuous 1D slider, and a continuous 2D trackpad with 91.8\% accuracy, &lt;4.4 cm MAE, and &lt;4.1cm MAE, respectively (3) Object recognition across six household objects with 94.5\% accuracy (4) User identification among 14 users with 99\% accuracy. Z-Ring’s sensing methodology uses only a single co-located electrode pair for both receiving and sensing, lending itself well to future miniaturization for use in on-the-go scenarios.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {150},
numpages = {18},
keywords = {bio-impedance, gesture recognition, interaction, object identification, passive user interface, rings, sensing, tangible user interface, user identification},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1145/3582487,
author = {Gr\"{o}hn, Tommi and Liikkanen, Sammeli and Huttunen, Teppo and M\"{a}kinen, Mika and Liljeberg, Pasi and Marttinen, Pekka},
title = {Quantifying Movement Behavior of Chronic Low Back Pain Patients in Virtual Reality},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
url = {https://doi.org/10.1145/3582487},
doi = {10.1145/3582487},
abstract = {Chronic low back pain (CLBP) is a globally common musculoskeletal problem. Measuring the sensation of pain and the effect of a treatment has always been a challenge for healthcare. Here, we study how the movement data, collected while using a virtual reality (VR) program, could be used as an objective measurement in patients with CLBP. A specific data collection method based on VR was developed and used with CLBP patients and healthy volunteers. We demonstrate that the movement data in VR can be used to classify individuals in these two groups with a high accuracy by using logistic regression. The most discriminative features are the duration of the movements and the total variation of movement velocity. Furthermore, we show that hidden Markov models can divide movement data into meaningful segments, which creates possibilities for defining even more detailed features, with potential to improve accuracy, when larger datasets become available in the future.},
journal = {ACM Trans. Comput. Healthcare},
month = apr,
articleno = {11},
numpages = {24},
keywords = {virtual reality, time series segmentation, movement data, hidden Markov models, gesture recognition, digital therapeutics, digital biomarker, Chronic low back pain}
}

@article{10.1145/3583593,
author = {Yang, Zhenyu and Li, Yantao and Zhou, Gang},
title = {TS-GAN: Time-series GAN for Sensor-based Health Data Augmentation},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
url = {https://doi.org/10.1145/3583593},
doi = {10.1145/3583593},
abstract = {Deep learning has achieved significant success on intelligent medical treatments, such as automatic diagnosis and analysis of medical data. To train an automatic diagnosis system with high accuracy and strong robustness in healthcare, sufficient training data are required when using deep learning-based methods. However, given that the data collected by sensors that are embedded in medical or mobile devices are inadequate, it is challenging to train an effective and efficient classification model with state-of-the-art performance. Inspired by generative adversarial networks (GANs), we propose TS-GAN, a Time-series GAN architecture based on long short-term memory (LSTM) networks for sensor-based health data augmentation, thereby improving the performance of deep learning-based classification models. TS-GAN aims to learn a generative model that creates time-series data with the same space and time dependence as the real data. Specifically, we design an LSTM-based generator for creating realistic data and an LSTM-based discriminator for determining how similar the generated data are to real data. In particular, we design a sequential-squeeze-and-excitation module in the LSTM-based discriminator to better understand space dependence of real data, and apply the gradient penalty originated from Wasserstein GANs in the training process to stabilize the optimization. We conduct comparative experiments to evaluate the performance of TS-GAN with TimeGAN, C-RNN-GAN and Conditional Wasserstein GANs through discriminator loss, maximum mean discrepancy, visualization methods and classification accuracy on health datasets of ECG_200, NonInvasiveFatalECG_Thorax1, and mHealth, respectively. The experimental results show that TS-GAN exceeds other state-of-the-art time-series GANs in almost all the evaluation metrics, and the classifier trained on synthetic datasets generated by TS-GAN achieves the highest classification accuracy of 97.50\% on ECG_200, 94.12\% on NonInvasiveFatalECG_Thorax1, and 98.12\% on mHealth, respectively.},
journal = {ACM Trans. Comput. Healthcare},
month = apr,
articleno = {12},
numpages = {21},
keywords = {space dependence, time dependence, SSE, data augmentation, LSTM, GANs}
}

@article{10.1145/3579482,
author = {Eardley, Rachel and Tonkin, Emma L. and Soubutts, Ewan and Ayobi, Amid and Tourte, Gregory J. L. and Gooberman-Hill, Rachael and Craddock, Ian and O'Kane, Aisling Ann},
title = {Explanation before Adoption: Supporting Informed Consent for Complex Machine Learning and IoT Health Platforms},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579482},
doi = {10.1145/3579482},
abstract = {Explaining health technology platforms to non-technical members of the public is an important part of the process of informed consent. Complex technology platforms that deal with safety-critical areas are particularly challenging, often operating within private domains (e.g. health services within the home) and used by individuals with various understandings of hardware, software, and algorithmic design. Through two studies, the first an interview and the second an observational study, we questioned how experts (e.g. those who designed, built, and installed a technology platform) supported provision of informed consent by participants. We identify a wide range of tools, techniques, and adaptations used by experts to explain the complex SPHERE sensor-based home health platform, provide implications for the design of tools to aid explanations, suggest opportunities for interactive explanations, present the range of information needed, and indicate future research possibilities in communicating technology platforms.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {49},
numpages = {25},
keywords = {consent, explainable, health, privacy, qualitative, smart homes, user study}
}

@inproceedings{10.1145/3578245.3585036,
author = {Tonini, Simone and Barsacchi, Fernando and Chiaromonte, Francesca and Licari, Daniele and Vandin, Andrea},
title = {Towards Novel Statistical Methods for Anomaly Detection in Industrial Processes},
year = {2023},
isbn = {9798400700729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578245.3585036},
doi = {10.1145/3578245.3585036},
abstract = {This paper presents a novel methodology based on first principles of statistics and statistical learning for anomaly detection in industrial processes and IoT environments. We present a 5-level analytical pipeline that cleans, smooths, and eliminates redundancies from the data, and identifies outliers as well as the features that contribute most to these anomalies. We show how smoothing can make our methodology less sensitive to short-lived anomalies that might be, e.g., due to sensor noise. We validate the methodology on a dataset freely available in the literature. Our results show that we can identify all anomalies in the considered dataset, with the ability of controlling the amount of false positives. This work is the result of a research project co-funded by the Tuscany Region and a company leader in the paper and nonwovens sector. Although the methodology was developed for this domain, we consider here a dataset from a different industrial sector. This shows that our methodology can be generalized to other contexts with similar constraints on limited resources, interpretability, time, and budget.},
booktitle = {Companion of the 2023 ACM/SPEC International Conference on Performance Engineering},
pages = {147–153},
numpages = {7},
keywords = {anomaly detection, industrial processes, mahalanobis distance},
location = {Coimbra, Portugal},
series = {ICPE '23 Companion}
}

@inproceedings{10.1145/3582084.3582089,
author = {Liouane, Oumaima and Femmam, Smain and Bakir, Toufik and Abdelali, Abdessalem Ben},
title = {Node Localization in Range-Free 3D-WSNs Using New DV-Hop Algorithm Based Machine Learning Techniques},
year = {2023},
isbn = {9781450397940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582084.3582089},
doi = {10.1145/3582084.3582089},
abstract = {In many Wireless Sensor Network (WSN) applications, location is critical. Another intriguing aspect of the acquired data is the ability to obtain exact information about sensors' locations. In order to localize multi-hop WSNs, based connectivity algorithms use their benefits, such as simplicity and acceptable accuracy, to do so. However, the localization accuracy may be limited due to the two- or three-dimensional environment (2D or 3D). Range-Free 3D-WSNs can benefit from an analytic model for hop-size quantization and an Extreme Learning Machine (ELM) method for localization to reduce localization errors. The additional third dimension greatly affects the accuracy of localization. Since many applications require 3D localization, it is important to develop efficient self-localization algorithms for 3D WSNs. In this paper, for a uniform distribution of sensor nodes, a new probabilistic quantization of hop size in 3D WSNs is proposed. Moreover, the extreme learning machine (ELM) which represents a new approach to WSN localization is exploited combining a conventional method (probabilistic approach) with a non-conventional method (Machine Learning). For a variety of conditions, our algorithms have been tested through simulation in isotropic settings. The performance of the localization model was assessed using the average localization error (LE). When compared to previous 3D-DV-Hop heuristics, the suggested localization algorithm's performance in terms of accuracy is clearly demonstrated by simulation data. With the help of the predicted hop quantization for hop-size estimation and the ELM was used for position estimation, our localization method for 3D-WSNs lowers the average localization error of nodes and has a greater location accuracy compared to its rivals.},
booktitle = {Proceedings of the 2022 4th International Conference on Software Engineering and Development},
pages = {21–25},
numpages = {5},
keywords = {hop-quantization, Machine Learning, Localization, 3D Wireless Sensors Network},
location = {Xiamen, China},
series = {ICSED '22}
}

@article{10.1145/3579832,
author = {Bannis, Adeola and Pan, Shijia and Ruiz, Carlos and Shen, John and Noh, Hae Young and Zhang, Pei},
title = {IDIoT: Multimodal Framework for Ubiquitous Identification and Assignment of Human-carried Wearable Devices},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
url = {https://doi.org/10.1145/3579832},
doi = {10.1145/3579832},
abstract = {IoT (Internet of Things) devices, such as network-enabled wearables, are carried by increasingly more people throughout daily life. Information from multiple devices can be aggregated to gain insights into a person’s behavior or status. For example, an elderly care facility could monitor patients for falls by combining fitness bracelet data with video of the entire class. For this aggregated data to be useful to each person, we need a multi-modality association of the devices’ physical ID (i.e., location, the user holding it, visual appearance) with a virtual ID (e.g., IP address/available services). Existing approaches for multi-modality association often require intentional interaction or direct line-of-sight to the device, which is infeasible for a large number of users or when the device is obscured by clothing.We present IDIoT, a calibration-free passive sensing approach that fuses motion sensor information with camera footage of an area to estimate the body location of motion sensors carried by a user. We characterize results across three baselines to highlight how different fusing methodology results better than earlier IMU-vision fusion algorithms. From this characterization, we determine IDIoT is more robust to errors such as missing frames or miscalibration that frequently occur in IMU-vision matching systems.},
journal = {ACM Trans. Internet Things},
month = apr,
articleno = {11},
numpages = {25},
keywords = {text tagging, gaze detection, neural networks, Datasets}
}

@inproceedings{10.1145/3582437.3582474,
author = {Habibi, Reza and Pfau, Johannes and Maram, Sai Siddartha and Li, Jiahong and Larsen, Bjarke and Xu, Jason and Kashani, Atieh and Sisodiya, Shweta and Holmes, Jonattan and Teng, Zhaoqing and Carstensdottir, Elin and El-Nasr, Magy Seif},
title = {Under Pressure: A Multi-Modal Analysis of Induced Stressors in Games for Resilience},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582437.3582474},
doi = {10.1145/3582437.3582474},
abstract = {Emotion regulation and coping strategies are key to resilience, problem solving and eventually well-being in everyday life, but investigating or influencing these without adjustable and ecologically valid environments still poses a major challenge. With respect to video games, stressful events can frequently appear and, depending on the individual’s emotion regulation, lead to frustrating experiences, dissatisfied players or even churn – or to a sense of accomplishment, mastery or positive tension. In the greater endeavor of establishing games for studying, controlling and reinforcing resilience, we developed an alternate reality game that unifies the advantages of a fully customizable game environment with the close connection to real-life interactions. To estimate how emotion regulation strategies could be initiated as well as investigated, this work first induces different kinds of established stressors (time pressure, social encounters and being unchangeably stuck) into ordinary gameplay, quantifies the players’ physiological and psychological responses and addresses the participants’ strategies on dealing with these situations. In this paper we present results of the study showing that the induced stressors were effective in constituting stressful situations comparable to real-life experiences; and each of these conditions resulted in different types of physiological and perceived responses and behaviors, e.g. continuous stress in social encounters versus instantaneous stress when notified about time pressure; or frustrating experiences when being forcibly stuck versus stress that participants underwent but were able to work against. With this platform to study how people cope with stress, instituted within a multi-modal mixed-methods evaluation, we contribute to games beyond entertainment towards educating resilience.},
booktitle = {Proceedings of the 18th International Conference on the Foundations of Digital Games},
articleno = {38},
numpages = {10},
keywords = {Serious games, individual differences in games, resilience, stress detection},
location = {Lisbon, Portugal},
series = {FDG '23}
}

@article{10.1145/3572899,
author = {Liu, Xiao and Gao, Bonan and Suleiman, Basem and You, Han and Ma, Zisu and Liu, Yu and Anaissi, Ali},
title = {Privacy-Preserving Personalized Fitness Recommender System P3FitRec: A Multi-level Deep Learning Approach},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3572899},
doi = {10.1145/3572899},
abstract = {Recommender systems have been successfully used in many domains with the help of machine learning algorithms. However, such applications tend to use multi-dimensional user data, which has raised widespread concerns about the breach of users’ privacy. Meanwhile, wearable technologies have enabled users to collect fitness-related data through embedded sensors to monitor their conditions or achieve personalized fitness goals. In this article, we propose a novel privacy-aware personalized fitness recommender system. We introduce a multi-level deep learning framework that learns important features from a large-scale real fitness dataset that is collected from wearable Internet of Things (IoT) devices to derive intelligent fitness recommendations. Unlike most existing approaches, our approach achieves personalization by inferring the fitness characteristics of users from sensory data, minimizing the need for explicitly collecting user identity or biometric information, such as name, age, height, and weight. Our proposed models and algorithms predict (a) personalized exercise distance recommendations to help users to achieve target calories, (b) personalized speed sequence recommendations to adjust exercise speed given the nature of the exercise and the chosen route, and (c) personalized heart rate sequence to guide the user of the potential health status for future exercises. Our experimental evaluation on a real-world Fitbit dataset demonstrated high accuracy in predicting exercise distance, speed sequence, and heart rate sequence compared with similar studies.1 Furthermore, our approach is novel compared with existing studies, as it does not require collecting and using users’ sensitive information. Thus, it preserves the users’ privacy.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {76},
numpages = {24},
keywords = {sensors, deep learning, recommender system, fitness, Personalization}
}

@article{10.1145/3565973,
author = {Dong, Guimin and Tang, Mingyue and Wang, Zhiyuan and Gao, Jiechao and Guo, Sikun and Cai, Lihua and Gutierrez, Robert and Campbel, Bradford and Barnes, Laura E. and Boukhechba, Mehdi},
title = {Graph Neural Networks in IoT: A Survey},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/3565973},
doi = {10.1145/3565973},
abstract = {The Internet of Things (IoT) boom has revolutionized almost every corner of people’s daily lives: healthcare, environment, transportation, manufacturing, supply chain, and so on. With the recent development of sensor and communication technology, IoT artifacts, including smart wearables, cameras, smartwatches, and autonomous systems can accurately measure and perceive their surrounding environment. Continuous sensing generates massive amounts of data and presents challenges for machine learning. Deep learning models (e.g., convolution neural networks and recurrent neural networks) have been extensively employed in solving IoT tasks by learning patterns from multi-modal sensory data. Graph neural networks (GNNs), an emerging and fast-growing family of neural network models, can capture complex interactions within sensor topology and have been demonstrated to achieve state-of-the-art results in numerous IoT learning tasks. In this survey, we present a comprehensive review of recent advances in the application of GNNs to the IoT field, including a deep dive analysis of GNN design in various IoT sensing environments, an overarching list of public data and source codes from the collected publications, and future research directions. To keep track of newly published works, we collect representative papers and their open-source implementations and create a Github repository at GNN4IoT.},
journal = {ACM Trans. Sen. Netw.},
month = apr,
articleno = {47},
numpages = {50},
keywords = {survey, sensor network, Internet of Things, Graph neural network}
}

@inproceedings{10.1145/3582177.3582183,
author = {Kim, Dong Jun and Li, Wanwan},
title = {A View Direction-Driven Approach for Automatic Room Mapping in Mixed Reality},
year = {2023},
isbn = {9781450397926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582177.3582183},
doi = {10.1145/3582177.3582183},
abstract = {Virtual Reality and Augmented Reality technologies have greatly improved recently, and developers are trying to make the experience as realistic as possible and close the gap between the physical world and the virtual world. In this paper, we propose an efficient and intuitive method to create an immersive Mixed Reality environment by automatically mapping your room. Our method is view direction driven, which allows the users to simply “look at” any indoor space to create a 3-dimensional model of the area the user is located in. This approach is easier and more intuitive for the users to use and reduces the time and effort compared to other MR environment generating methods. We use the Meta Quest 2’s cameras and gyroscope sensor and the Unity engine for the ray casting and the passthrough API. We will present the mathematical details of our method and show that the proposed method achieves better results than previous methods through the user study results.},
booktitle = {Proceedings of the 2023 5th International Conference on Image Processing and Machine Vision},
pages = {29–33},
numpages = {5},
location = {Macau, China},
series = {IPMV '23}
}

@inproceedings{10.1145/3565995.3566037,
author = {Wang, Jianxun and Foster, Marc and Bozkurt, Alper and Roberts, David L.},
title = {Motion-Resilient ECG Signal Reconstruction from a Wearable IMU through Attention Mechanism and Contrastive Learning},
year = {2023},
isbn = {9781450398305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565995.3566037},
doi = {10.1145/3565995.3566037},
abstract = {Wearable electrocardiogram (ECG) sensors can detect dogs’ heartbeat signals and have proven useful in monitoring dogs’ welfare and predicting temperament scores in structured evaluations of potential guide dog puppies. Despite advances in the ergonomics, performance, and usability of ECG sensor technologies specifically designed for dogs, deploying those systems in the real world imposes challenges such as training human operators to ensure electrodes’ proper contact with the skin and, especially in the case of puppies, socialization to achieve comfort and reduce behavioral inhibition. Seismocardiogram signal is an alternate modality for heartbeat signals and is acquired using the Inertial Measurement Unit (IMU), which is commercially available, widely deployed, and does not require skin-contact. However, the extracted signals from IMU are subject to heavy influences from motion and other noise sources. In this paper, we present a method that enables extracting the similar physiological parameters ECG provides using easier-to-deploy IMU sensors. We propose and evaluate a machine learning framework that reconstructs ECG signals from IMU signals even under moderate to heavy movements. Our study investigated two artificial neural network architectures to overcome severe noise artifacts in the IMU signal resulting from dogs’ movements and environmental factors. The first architecture combines the attention mechanism and convolution layers to extract important features from the temporal IMU input. The second architecture adapts contrastive representation learning to the regression problem and learns a more effective embedding for the ECG reconstruction. The qualitative inspection and quantitative analysis based on F1 scores of the R-peak alignment demonstrate the effectiveness of the two proposed models in removing motion noises and reconstructing realistic ECG signals, achieving an F1 score of 0.72 in the best case compared to 0.29 from the baseline.},
booktitle = {Proceedings of the Ninth International Conference on Animal-Computer Interaction},
articleno = {14},
numpages = {11},
keywords = {Machine Learning, Inertial Measurement Unit, ECG, Deep Neural Network},
location = {Newcastle-upon-Tyne, United Kingdom},
series = {ACI '22}
}

@inproceedings{10.1145/3575879.3575976,
author = {Eleftherakis, George and Baxhaku, Fesal and Vasilescu, Anca},
title = {Bio-inspired Adaptive Architecture for Wireless Sensor Networks},
year = {2023},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575879.3575976},
doi = {10.1145/3575879.3575976},
abstract = {Wireless Sensor Networks (WSN) are expected to revolutionize daily life by connecting everyday objects with sensing capabilities, offering numerous opportunities for a wide range of applications. To facilitate the integration and enable all these opportunities to be realized, it becomes a necessity for middleware architectures that: (a) perform well in non-well-defined infrastructures, (b) are able to deal with the large number of users and heterogeneous devices integrated into it (ultra scalable), and (c) enable autonomy of the system overall. This work introduces a bio-inspired middleware optimized for wireless sensor networks proposing a refinement, the regional network, in a work published earlier as a bio-inspired self-adaptive architecture for the Internet of Things, while providing a comparison of other similar middleware approaches and a discussion on the motivating health monitoring scenario.},
booktitle = {Proceedings of the 26th Pan-Hellenic Conference on Informatics},
pages = {116–122},
numpages = {7},
keywords = {eHealth, Wireless Sensor Networks, Middleware, IoT, Bio-inspired Adaptive Systems},
location = {Athens, Greece},
series = {PCI '22}
}

@inproceedings{10.1145/3565995.3566039,
author = {Holder, Timothy and Rahman, Mushfiqur and Summers, Emily and Roberts, David and Wong, Chau-Wai and Bozkurt, Alper},
title = {Contact-Free Simultaneous Sensing of Human Heart Rate and Canine Breathing Rate for Animal Assisted Interactions},
year = {2023},
isbn = {9781450398305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565995.3566039},
doi = {10.1145/3565995.3566039},
abstract = {Animal Assisted Interventions (AAIs) involve pleasant interactions between humans and animals and can potentially benefit both types of participants. Research in this field may help to uncover universal insights about cross-species bonding, dynamic affect detection, and the influence of environmental factors on dyadic interactions. However, experiments evaluating these outcomes are limited to methodologies that are qualitative, subjective, and cumbersome due to the ergonomic challenges related to attaching sensors to the body. Current approaches in AAIs also face challenges when translating beyond controlled clinical environments or research contexts. These also often neglect the measurements from the animal throughout the interaction. Here, we present our preliminary effort toward a contact-free approach to facilitate AAI assessment via the physiological sensing of humans and canines using consumer-grade cameras. This initial effort focuses on verifying the technological feasibility of remotely sensing the heart rate signal of the human subject and the breathing rate signal of the dog subject while they are interacting. Small amounts of motion such as patting and involuntary body shaking or movement can be tolerated with our custom designed vision-based algorithms. The experimental results show that the physiological measurements obtained by our algorithms were consistent with those provided by the standard reference devices. With further validation and expansion to other physiological parameters, the presented approach offers great promise for many scenarios from the AAI research space to veterinary, surgical, and clinical applications.},
booktitle = {Proceedings of the Ninth International Conference on Animal-Computer Interaction},
articleno = {16},
numpages = {10},
keywords = {signal processing, human-canine interaction, contact-free physiological sensing, computer vision},
location = {Newcastle-upon-Tyne, United Kingdom},
series = {ACI '22}
}

@inproceedings{10.1145/3565995.3566043,
author = {Ramey, Charles and Krichbaum, Sarah and Mastali, Arianna and Lin, Jodie and Starner, Thad and Jackson, Melody},
title = {Detecting Canine Mastication: A Wearable Approach},
year = {2023},
isbn = {9781450398305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565995.3566043},
doi = {10.1145/3565995.3566043},
abstract = {Mastication is considered a coping mechanism in dogs, therefore, providing chew toys as an enrichment technique may be particularly important in stressful environments, such as the kennel. However, the relationship between chewing and welfare in kennel-housed dogs has not been systematically examined. The purpose of this study was to develop a sensor to quantify chewing with the intention that this technology could be used to understand the relationship between chewing and welfare in kennel-housed dogs. We show that a collar-based microphone can sense canine bites on a Nylabone chew toy. Four human raters annotated bites on audio samples collected from twelve dogs with five minutes of continuous access to the chew toy. A high degree of reliability was found with an average intraclass correlation coefficient (ICC) of 0.994. Using consensus labeling for training, we created bite detection algorithms using random forest, logistic regression, and convolutional neural network (CNN) based techniques. The CNN-based system achieved the highest performance recognition with an accuracy of 88\% with a 91\% F1 score. This technology will allow us to analyze a larger data set to uncover relationships between chewing styles and stress, cognition, and other characteristics associated with dog welfare.},
booktitle = {Proceedings of the Ninth International Conference on Animal-Computer Interaction},
articleno = {6},
numpages = {11},
keywords = {wearable computing, machine learning, dataset development, canine health monitoring, canine cognition, animal computer interaction, acoustic sensing},
location = {Newcastle-upon-Tyne, United Kingdom},
series = {ACI '22}
}

@inproceedings{10.1145/3575879.3575974,
author = {Karamanou, Areti and Brimos, Petros and Kalampokis, Evangelos and Tarabanis, Konstantinos},
title = {Exploring the Quality of Dynamic Open Government Data for Developing Data Intelligence Applications: The Case of Attica Traffic Data},
year = {2023},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575879.3575974},
doi = {10.1145/3575879.3575974},
abstract = {Dynamic data (including environmental, traffic, and sensor generated data) were, recently, recognised as an important part of the Open Government Data (OGD) movement. These data are of vital importance in the development of data intelligence applications. For example, various business applications exploit traffic data to predict, e.g., traffic demand and an estimated time of arrival. However, this type of data is inherently vulnerable to data quality errors produced by, e.g., failures of sensors and network faults. The objective of this paper is to explore the quality of Dynamic Open Government Data for the development of data intelligence applications. Towards this end, we study a single case about the traffic data provided by the official Greek OGD portal. The portal involves the use of an Application Programming Interface (API), which is essential for the effective dissemination of dynamic data. Our research approach involves the exploration and the evaluation of the provided data with regards to missing values and anomalies. We anticipate that this paper will contribute to the identification of organisational and technical challenges that hamper the effective dissemination of dynamic OGD.},
booktitle = {Proceedings of the 26th Pan-Hellenic Conference on Informatics},
pages = {102–109},
numpages = {8},
keywords = {Open government data, data quality, dynamic data, sensor data, traffic data},
location = {Athens, Greece},
series = {PCI '22}
}

