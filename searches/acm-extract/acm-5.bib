@inproceedings{10.1145/3750069.3750132,
author = {Datla, Venkata Srikanth Varma and Zeppieri, Stefano and Aiuti, Alessandro and Bisante, Alba and Trasciatti, Gabriella and Panizzi, Emanuele},
title = {Towards Context-Aware UX in Automated Mobility: BLE Based Passenger Detection via Smartphones},
year = {2025},
isbn = {9798400721021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3750069.3750132},
doi = {10.1145/3750069.3750132},
abstract = {Understanding how people navigate urban environments is central for designing intelligent transportation systems and adaptive interfaces, particularly in the context of Human Automated Vehicle (HAV) interaction. In this paper, we investigate whether Bluetooth Low Energy (BLE) scanning on smartphones can detect a user’s mode of transport by passively sensing the presence of nearby devices. Unlike traditional approaches that rely on Global Positioning System (GPS) or motion sensors, our approach determines whether a user is traveling alone or in a shared space, such as a metro or bus, by analyzing the quantity and stability of nearby BLE signals. We developed a lightweight iOS app that logs BLE advertisements and trained a classifier to differentiate between cars, buses/trams, and metros. Our findings indicate that BLE patterns are typically consistent enough to support practical classification, demonstrating strong performance in detecting cars and other vehicles while providing valuable insights for shared transportation modes. The system avoids sensitive privacy permissions, and integrates into a user-friendly interface that provides feedback and allows for corrections. We explore how this approach can enhance passenger detection and contextual adaptation in automated vehicles. This includes adjusting interfaces, improving safety features, and facilitating adaptive behaviors. The findings reveal that although BLE-based sensing has limitations in precision, it provides a low-cost and privacy-aware alternative to traditional sensing strategies used in mobility applications.},
booktitle = {Proceedings of the 16th Biannual Conference of the Italian SIGCHI Chapter},
articleno = {43},
numpages = {7},
keywords = {Transportation Mode Detection, Human Automated Vehicle Interaction, Bluetooth Low Energy, Context Aware Interfaces, Mobile Sensing, Passive Sensing, User Aware Systems, Adaptive Interfaces, Proactive Systems, Robotaxi},
location = {
},
series = {CHItaly '25}
}

@inproceedings{10.1145/3750069.3750329,
author = {Salamat Ravandi, Bahram and Fransson, Max and Fabricius, Victor and Vandeleene, Norra and Francois, Cl\'{e}mentine and Lowe, Robert},
title = {Evaluating Biometric and Behavioral Markers of Intoxication in Drivers: A Pilot Study},
year = {2025},
isbn = {9798400721021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3750069.3750329},
doi = {10.1145/3750069.3750329},
abstract = {Intoxication and cognitive impairment are among the major contributors to traffic accidents and decreased traffic safety. The current pilot study focused on identifying and exploring behavioral and physiological markers of intoxication using a simulated driving environment. Eight participants were tested under two conditions: control (sober) and moderately intoxicated (0.05-0.10\% BAC (blood alcohol concentration)). Participants engaged in a driving simulation while data was collected via EEG, eye-tracking, and driver behavior sensors (e.g., steering input). Results from our pilot study indicated that pupil diameters during critical driving events (e.g., turns, overtakes, and collision avoidance) were higher under control conditions compared to intoxicated conditions. Moreover, intoxication led to higher mean acceleration magnitude, greater variability in acceleration and speed, and a lower mean speed. The results also revealed distinct patterns of neural activity associated with alcohol intoxication, particularly in the Pre-Frontal brain region. This study aimed to lay the groundwork for developing algorithms for automating the detection of intoxication and assessing driver fitness.},
booktitle = {Proceedings of the 16th Biannual Conference of the Italian SIGCHI Chapter},
articleno = {45},
numpages = {8},
keywords = {Driver State Detection, Driving Monitoring System, User Experience, Autonomous Vehicles, Intoxication},
location = {
},
series = {CHItaly '25}
}

@inproceedings{10.1145/3716553.3750798,
author = {Moharrer, Golnaz and Rajendran, Kavya and Pinto, Rowena and Kleinsmith, Andrea},
title = {Write! Draw! Move!: Investigating the Effects of Positive and Negative Self-Reflection on Emotion through Self-Expression Modalities},
year = {2025},
isbn = {9798400714993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716553.3750798},
doi = {10.1145/3716553.3750798},
abstract = {Emotional self-expression positively impacts people’s well-being by reducing stress and contributing to overall well-being. As digital mental health interventions become more prevalent in assisting graduate students with stress management and emotional awareness, it is important to understand how self-expression modalities can be effectively integrated. We conducted a study in which graduate students reflected on a positive experience and a negative experience, separately through three self-expression modalities (visual art, writing, and movement). Our results showed that negative reflection elicited a sense of relief as it felt calming and therapeutic. The visual and movement modalities helped participants express themselves creatively, whereas writing pushed them to face their emotions head-on. Our findings contribute to the multimodal interaction community and the field of HCI toward developing intelligent, affective multimodal digital technologies to encourage enhanced well-being through emotional and creative expression and reflection for graduate students.},
booktitle = {Proceedings of the 27th International Conference on Multimodal Interaction},
pages = {540–549},
numpages = {10},
keywords = {Self-reflection; positive and negative affect; PANAS; self-expression; graduate students},
location = {
},
series = {ICMI '25}
}

@inproceedings{10.1145/3731569.3764828,
author = {Schuermann, Leon and Campbell, Brad and Ghena, Branden and Levis, Philip and Levy, Amit and Pannuto, Pat},
title = {Tock: From Research To Securing 10 Million Computers},
year = {2025},
isbn = {9798400718700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3731569.3764828},
doi = {10.1145/3731569.3764828},
abstract = {Tock began 10 years ago as a research operating system developed by academics to help other academics build urban sensing applications. By leveraging a new language (Rust) and new hardware protection mechanisms, Tock enabled "Multiprogramming a 64 kB Computer Safely and Efficiently". Today, it is an open-source project with a vibrant community of users and contributors. It is deployed on root-of-trust hardware in data-center servers and on millions of laptops; it is used to develop automotive and space products, wearable electronics, and hardware security tokens—all while remaining a platform for operating systems research. This paper focuses on the impact of Tock's technical design on its adoption, the challenges and unexpected benefits of using a type-safe language (Rust)—particularly in security-sensitive settings—and the experience of supporting a production open-source operating system from academia.},
booktitle = {Proceedings of the ACM SIGOPS 31st Symposium on Operating Systems Principles},
pages = {36–49},
numpages = {14},
location = {Lotte Hotel World, Seoul, Republic of Korea},
series = {SOSP '25}
}

@inproceedings{10.1145/3747327.3764904,
author = {Ma, Cheng and Joo, Kevin Hyekang and Vail, Alexandria K. and Bhattacharya, Sunreeta and Fern\'{a}ndez Garc\'{\i}a, \'{A}lvaro and Baker-Matsuoka, Kailana and Mathew, Sheryl and Holt, Lori L. and De La Torre, Fernando},
title = {Multimodal Fusion with LLMs for Engagement Prediction in Natural Conversation},
year = {2025},
isbn = {9798400720765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3747327.3764904},
doi = {10.1145/3747327.3764904},
abstract = {Over the past decade, wearable computing devices (“smart glasses”) have undergone remarkable advancements in sensor technology, design, and processing power, ushering in a new era of opportunity for high-density human behavior data. Equipped with wearable cameras, these glasses enable the analysis of non-verbal behavior during natural face-to-face interactions. Our focus lies in predicting engagement in dyadic interactions by scrutinizing verbal and non-verbal cues, aiming to detect signs of disinterest or confusion. Leveraging such analyses may revolutionize our understanding of human communication, foster more effective collaboration in professional environments, provide better mental health support through empathetic virtual interactions, and enhance accessibility for those with communication barriers.In this work, we collect a dataset featuring 34 participants engaged in casual dyadic conversations, each providing self-reported engagement ratings at the end of each conversation. We introduce a novel fusion strategy using Large Language Models (LLMs) to integrate multiple behavior modalities into a “multimodal transcript” that can be processed by an LLM for behavioral reasoning tasks. Remarkably, this method achieves performance comparable to established fusion techniques even in its preliminary implementation, indicating strong potential for further research and optimization. This fusion method is one of the first to approach “reasoning” about real-world human behavior through a language model. Smart glasses provide us the ability to unobtrusively gather high-density multimodal data on human behavior, paving the way for new approaches to understanding and improving human communication with the potential for important societal benefits. The features and data collected during the studies will be made publicly available to promote further research.},
booktitle = {Companion Proceedings of the 27th International Conference on Multimodal Interaction},
pages = {244–259},
numpages = {16},
keywords = {Engagement, Multimodal Machine Learning, Multimodal Fusion, Human Behavior, Affective Computing, Gaze Tracking},
location = {
},
series = {ICMI '25 Companion}
}

@inproceedings{10.1145/3747327.3764788,
author = {Gkikas, Stefanos and Kyprakis, Ioannis and Tsiknakis, Manolis},
title = {Tiny-BioMoE: a Lightweight Embedding Model for Biosignal Analysis},
year = {2025},
isbn = {9798400720765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3747327.3764788},
doi = {10.1145/3747327.3764788},
abstract = {Pain is a complex and pervasive condition that affects a significant portion of the population. Accurate and consistent assessment is essential for individuals suffering from pain, as well as for developing effective management strategies in a healthcare system. Automatic pain assessment systems enable continuous monitoring, support clinical decision-making, and help minimize patient distress while mitigating the risk of functional deterioration. Leveraging physiological signals offers objective and precise insights into a person’s state, and their integration in a multimodal framework can further enhance system performance. This study has been submitted to the Second Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN). The proposed approach introduces Tiny-BioMoE, a lightweight pretrained embedding model for biosignal analysis. Trained on 4.4 million biosignal image representations and consisting of only 7.3 million parameters, it serves as an effective tool for extracting high-quality embeddings for downstream tasks. Extensive experiments involving electrodermal activity, blood volume pulse, respiratory signals, peripheral oxygen saturation, and their combinations highlight the model’s effectiveness across diverse modalities in automatic pain recognition tasks. The model’s architecture (code) and weights are available at https://github.com/GkikasStefanos/Tiny-BioMoE.},
booktitle = {Companion Proceedings of the 27th International Conference on Multimodal Interaction},
pages = {117–126},
numpages = {10},
keywords = {Pain assessment, pain recognition, deep learning, multimodal, foundation model, data fusion},
location = {
},
series = {ICMI '25 Companion}
}

@inproceedings{10.1145/3716553.3750761,
author = {Prasad, Ankit Arvind and Bidwai, Shashank Laxmikant and Zawar, Ashutosh Jitendra and Ahuja, Diven Ashwani and Kalatzis, Apostolos and Girishan Prabhu, Vishnunarayan},
title = {Psychological and Neurophysiological Indicators of Stress and Relaxation in Immersive Virtual Reality Environments: A Multimodal Approach},
year = {2025},
isbn = {9798400714993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716553.3750761},
doi = {10.1145/3716553.3750761},
abstract = {Understanding user affect through multimodal sensing is critical for designing adaptive and effective interactive systems. While Virtual Reality (VR) is increasingly used to induce and regulate affective states, limited research has examined real-time neurophysiological and psychological changes across contrasting VR scenarios. In this exploratory study, fourteen participants were exposed to two immersive VR experiences in a within-subjects design: (1) a custom-designed, nature-based environment integrated with heart rate variability biofeedback (HRVBF) to promote relaxation, and (2) Richie’s Plank Experience, a pre-developed VR scenario designed to elicit stress. We conducted a multimodal analysis of neurophysiological and psychological responses during exposure to these VR environments. Subjective psychological responses were measured pre- and post-intervention using the State-Trait Anxiety Inventory (STAI) and Visual Analog Scale (VAS). Additionally, neurophysiological data were concurrently recorded, including respiration rate (RR), heart rate variability (HRV), and hemodynamic responses, specifically oxygenated (HbO) and deoxygenated hemoglobin (HbR), using functional near-infrared spectroscopy (fNIRS).Richie’s Plank significantly elevated post-exposure subjective anxiety scores (VAS: p = 0.05, STAI: p &lt; 0.01) and increased RR (20.6 ± 2.37) compared to the HRVBF condition (11.2 ± 5.90, p &lt; 0.01). HbO was higher during Richie’s Plank (3.0 ± 1.93), while HbR was elevated in the HRVBF condition (1.0 ± 1.13), both p &lt; 0.01. Furthermore, HRV (RMSSD) was lower during the HRVBF environment (p &lt; 0.01), indicating greater parasympathetic activation during HRVBF. These findings demonstrate that fNIRS, HRV/ECG, and RR exhibit distinct patterns that reflect the unique characteristics of each VR intervention presented. This multimodal physiological responsiveness supports the design of affect-aware systems capable of delivering real-time, personalized interventions. However, given the modest sample size and demographic homogeneity, these exploratory results warrant replication in larger, diverse populations for generalizability.},
booktitle = {Proceedings of the 27th International Conference on Multimodal Interaction},
pages = {96–105},
numpages = {10},
keywords = {Affective computing; Multimodal interaction; Neurophysiological monitoring; User-centered adaptive systems; Virtual reality},
location = {
},
series = {ICMI '25}
}

@inproceedings{10.1145/3716553.3750775,
author = {Yeon, Taeyoung and Xu, Vasco and Hoffmann, Henry and Ahuja, Karan},
title = {WatchHAR: Real-time On-device Human Activity Recognition System for Smartwatches},
year = {2025},
isbn = {9798400714993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716553.3750775},
doi = {10.1145/3716553.3750775},
abstract = {Despite advances in practical and multimodal fine-grained Human Activity Recognition (HAR), a system that runs entirely on smartwatches in unconstrained environments remains elusive. We present WatchHAR , an audio and inertial-based HAR system that operates fully on smartwatches, addressing privacy and latency issues associated with external data processing. By optimizing each component of the pipeline, WatchHAR achieves compounding performance gains. We introduce a novel architecture that unifies sensor data preprocessing and inference into an end-to-end trainable module, achieving 5x faster processing while maintaining over 90\% accuracy across more than 25 activity classes. WatchHAR outperforms state-of-the-art models for event detection and activity classification while running directly on the smartwatch, achieving 9.3&nbsp;ms processing time for activity event detection and 11.8&nbsp;ms for multimodal activity classification. This research advances on-device activity recognition, realizing smartwatches’ potential as standalone, privacy-aware, and minimally-invasive continuous activity tracking devices.},
booktitle = {Proceedings of the 27th International Conference on Multimodal Interaction},
pages = {387–394},
numpages = {8},
keywords = {Smartwatches, On-device processing, Real-time mobile sensing, Human activity recognition, Privacy aware sensing},
location = {
},
series = {ICMI '25}
}

@inproceedings{10.1145/3716553.3750782,
author = {Yin, Zhigang and Nguyen, Ngoc Thi and Zuniga, Agustin and Liyanage, Mohan and Nurmi, Petteri and Flores, Huber},
title = {SpikEy: Preventing Drink Spiking using Wearables},
year = {2025},
isbn = {9798400714993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716553.3750782},
doi = {10.1145/3716553.3750782},
abstract = {Drink spiking, the deliberate act of secretly adding substances to someone’s drink, is a growing concern, exacerbated by easier access to drugs. Effective protection against this threat requires discreet yet accurate liquid analysis solutions. Current methods fail to generalize across various substances and are primarily designed for static environments, lacking adaptability to dynamic real-world situations. We contribute SpikEy, an innovative sensing system that combines optical sensing, embedded AI, and signal processing to overcome these limitations. The key technical contributions of SpikEy are latent modeling of optical signals for robustness and generality, and motion and light calibration for ensuring accurate performance in diverse real-world settings. Extensive experiments with various drug profiles, concentrations, and liquids demonstrate that SpikEy can detect spiked drinks with up to  (86\%)  accuracy, effectively generalizing across different users and unseen drinks. It outperforms state-of-the-art methods by 10-20\%, and shows nearly 20\% improvement over visual inspection in real-world applications.},
booktitle = {Proceedings of the 27th International Conference on Multimodal Interaction},
pages = {78–86},
numpages = {9},
keywords = {Liquid Sensing; Light Sampling; Wearable; IoT; Smart Ring},
location = {
},
series = {ICMI '25}
}

@inproceedings{10.1145/3747327.3764902,
author = {Murakami, Haruka and Inamura, Tetusnari},
title = {Inferring User State from Gaze Dynamics in a VR Throwing Task: Toward Adaptive and User-Centered Rehabilitation Support},
year = {2025},
isbn = {9798400720765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3747327.3764902},
doi = {10.1145/3747327.3764902},
abstract = {Understanding users’ internal states during interaction is key to developing adaptive virtual reality (VR) systems for rehabilitation and cognitive training. While prior systems often rely on post-task questionnaires or task performance, these approaches lack the temporal resolution and implicitness needed for real-time adaptation. Capturing how users’ internal states evolve during interaction requires not only physiological sensing, but also alignment with the structure of their physical actions. This study investigates gaze and pupil dynamics in a VR throwing task, focusing on how visual attention and cognitive responses unfold across discrete motor phases. Participants performed either a target-hitting task, requiring visual precision, or a free-throwing task without visual goals. We recorded five gaze-related metrics—gaze speed, vergence, pupil diameter, blink rate, and eye openness—and synchronized them with four automatically detected movement events: ball reset, throw initiation, release, and impact. The free-throwing group exhibited a smaller range of change in gaze vergence—which reflects the depth of a person's visual focus—compared to the target-hitting group. This finding provides physiological support for the widely reported phenomenon of distance underestimation in VR environments. In terms of pupil diameter, a difference in the timing of peak dilation was observed between the groups: while the free-throwing group showed peak dilation during the throwing motion, suggesting heightened concentration at that moment, the target-hitting group appeared to direct their attention more toward the outcome observation phase. These temporal differences suggest differing attentional demands and perceptual strategies. Our findings demonstrate that synchronizing physiological gaze signals with fine-grained motor events enables implicit tracking of internal cognitive and emotional state fluctuations, offering a foundation for data-driven personalization of difficulty and feedback in VR-based rehabilitation.},
booktitle = {Companion Proceedings of the 27th International Conference on Multimodal Interaction},
pages = {235–239},
numpages = {5},
keywords = {Eye tracking, Gaze Behavior, Rehabilitation, User state estimation, Virtual Reality},
location = {
},
series = {ICMI '25 Companion}
}

@inproceedings{10.1145/3767052.3767056,
author = {Zhang, Wanchen and Bai, Pengxia},
title = {Data Asset Information Disclosure and Corporate Governance in the Digital Economy: From the perspective of stock price crash risk},
year = {2025},
isbn = {9798400716010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3767052.3767056},
doi = {10.1145/3767052.3767056},
abstract = {With the digital economy's rapid growth, data assets are crucial in corporate governance. This paper empirically studies how data asset disclosure affects corporate stock price crash risk using 2007 - 2023 data from A-share listed firms in Shanghai and Shenzhen. It finds that higher data asset disclosure is linked to lower stock price crash risks, and this holds after sensitivity and endogeneity tests. Mechanism tests show it reduces crash risk by improving internal governance and external monitoring. Heterogeneity analyses indicate the impact is more significant for state-owned firms, those in competitive industries, and those facing tight external financing. The paper offers theoretical and practical guidance for data asset disclosure policies, aiding the standardization and transparency of data asset management and the digital economy's healthy development.},
booktitle = {Proceedings of the 2025 International Conference on Big Data, Artificial Intelligence and Digital Economy},
pages = {22–28},
numpages = {7},
keywords = {Data Assets, External Supervision, Information Disclosure, Principal–Agent, Stock Price Crash Risk},
location = {
},
series = {BDAIE '25}
}

@inproceedings{10.1145/3759548.3763372,
author = {Li, Yuze and Yao, Shunyu and Mobin, Jaiaid and Zhan, Tianyu and Rafique, M. Mustafa and Nikolopoulos, Dimitrios and Sundararajah, Kirshanthan and Butt, Ali R.},
title = {Memory Tiering in Python Virtual Machine},
year = {2025},
isbn = {9798400721649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3759548.3763372},
doi = {10.1145/3759548.3763372},
abstract = {Modern Python applications consume massive amounts of memory in data centers. Emerging memory technologies such as CXL have emerged as a pivotal interconnect for memory expansion. Prior efforts in memory tiering that relied on OS page or hardware counters information incurred notable overhead and lacked awareness of fine-grained object access patterns. Moreover, these tiering configurations cannot be tailored to individual Python applications, limiting their applicability in QoS-sensitive environments. In this paper, we introduce Memory Tiering in Python VM (MTP), an extension module built atop the popular CPython interpreter to support memory tiering in Python applications. MTP leverages reference count changes from garbage collection to infer object temperatures and reduces unnecessary migration overhead through a software-defined page temperature table. To the best of our knowledge, MTP is the first framework to offer portability, easy deployment, and per-application tiering customization for Python workloads.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Workshop on Virtual Machines and Intermediate Languages},
pages = {33–44},
numpages = {12},
keywords = {garbage collection, memory tiering, virtual machine},
location = {Singapore, Singapore},
series = {VMIL '25}
}

@article{10.1145/3761806,
author = {Park, Seonghoon and Ahn, Junick and Kim, Daeyong and Cha, Hojung},
title = {Duration-Aware Sound Event Detection on Ultra-Low-Power Sensor Devices},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3761806},
doi = {10.1145/3761806},
abstract = {Sound event detection (SED) based on on-device machine learning (ML) presents considerable energy challenges for ultra-low-power sensor devices. In this article, we propose DASH, a duration-aware SED system designed for energy-constrained sensor devices in domestic environments. As repeated inferences for continuous sound events lead to unnecessary energy consumption, DASH aims to minimize unnecessary inferences by predicting the duration of sound events. However, the variability of sound event durations across different environments and scenarios poses a major challenge in developing a responsive yet energy-efficient duration-aware SED system. To address this, DASH introduces three key solutions: (1) N-probability distribution-based event duration prediction, which identifies checkpoints where new inferences are likely needed; (2) Affinity-guided event classification, which performs low-energy affinity matching at checkpoints to determine whether ML inference is necessary; and (3) Interrupt blocking-enabling cycle-based device state control, which periodically checks for event presence with minimal energy consumption at non-checkpoint times. We implemented DASH on MSP430-based sensor devices deployed in real home environments. Experimental results demonstrate that DASH reduced energy consumption by approximately 97–98\% compared to evaluation baselines, with only a 4.7\% error rate.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = oct,
articleno = {169},
numpages = {26},
keywords = {Sound event detection, ultra-low-power sensor devices, event duration prediction}
}

@inproceedings{10.1145/3745900.3746088,
author = {Hiraki, Hirotaka and Rekimoto, Jun},
title = {MaskClip: Detachable Clip-on Piezoelectric Sensing of Mask Surface Vibrations for Real-time Noise-Robust Speech Input},
year = {2025},
isbn = {9798400715662},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3745900.3746088},
doi = {10.1145/3745900.3746088},
abstract = {Masks are essential in medical settings and during infectious outbreaks but significantly impair speech communication, especially in environments with background noise. Existing solutions often require substantial computational resources or compromise hygiene and comfort. We propose a novel sensing approach that captures only the wearer’s voice by detecting mask surface vibrations using a piezoelectric sensor. Our developed device, MaskClip, employs a stainless steel clip with an optimally positioned piezoelectric sensor to selectively capture speech vibrations while inherently filtering out ambient noise. Evaluation experiments demonstrated superior performance with a low Character Error Rate of 6.1\% in noisy environments compared to conventional microphones. Subjective evaluations by 102 participants also showed high satisfaction scores. This approach shows promise for applications in settings where clear voice communication must be maintained while wearing protective equipment, such as medical facilities, cleanrooms, and industrial environments.},
booktitle = {Proceedings of the Augmented Humans International Conference 2025},
pages = {333–344},
numpages = {12},
keywords = {noise-suppressive microphone, speech enhancement, piezoelectric, wearable device, voice user interface},
location = {
},
series = {AHs '25}
}

@inproceedings{10.1145/3745900.3746086,
author = {Ito, Shino and Peng, Yichen and Wu, Erwin and Koike, Hideki},
title = {SoleLoadEvaluator: A Real-Time Feedback System for Walking Posture with Anterior Load Using Insole Sensors},
year = {2025},
isbn = {9798400715662},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3745900.3746086},
doi = {10.1145/3745900.3746086},
abstract = {Human pose estimation is essential for numerous applications, including sports analysis and rehabilitation. Improper postures, especially during heavy load handling, can lead to physical strain or injuries. While vision-based methods and optical motion capture systems provide solutions for pose estimation, they face significant limitations in environments like factories or construction sites due to occlusions, imaging conditions, and high costs. Wearable sensors offer a flexible alternative but often require multiple bulky IMUs, which can interfere with load-handling tasks. To address these challenges, we propose PoseLoadEvaluator, a system that leverages foot pressure sensors and foot-mounted IMUs to classify load-handling postures and predict back curvature. The system provides real-time feedback through mobile devices and headphones, enhancing user awareness of posture quality. To support this work, we collected a dataset from 12 participants performing 8 load-handling postures and trained a multi-task model to predict posture categories and back curvature. We validated the system through a user study, demonstrating its effectiveness in promoting safe load-handling practices in complex environments.},
booktitle = {Proceedings of the Augmented Humans International Conference 2025},
pages = {345–355},
numpages = {11},
keywords = {Insole sensor, motion capture, pose estimation, foot pressure},
location = {
},
series = {AHs '25}
}

@inproceedings{10.1145/3745900.3746079,
author = {Chin, Sam and Fang, Cathy Mengying and Singh, Nikhil and Ibrahim, Ibrahim and Paradiso, Joe and Maes, Pattie},
title = {Purrfect Pitch: Exploring Pitch Interval Learning through an Audio-Haptic Interface},
year = {2025},
isbn = {9798400715662},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3745900.3746079},
doi = {10.1145/3745900.3746079},
abstract = {We introduce Purrfect Pitch, a system consisting of a wearable haptic device and a custom-designed learning interface for musical ear training. We focus on the ability to identify pitch intervals (sequences of two musical notes), a perceptually ambiguous task that usually requires rote training. With our system, users hear two tones while simultaneously receiving two corresponding vibrotactile stimuli on the back. Providing haptic feedback on the back makes the auditory distance between tones salient, and the back-worn design is comfortable and unobtrusive. During training, users receive multi-sensory feedback from our system and input their guessed interval value on our web-based learning interface. Our study with 18 participants shows that our system enables novice learners to identify intervals more accurately and consistently than those who only received audio feedback, even after removing the haptic feedback. We also share further insights on designing a multisensory learning system.},
booktitle = {Proceedings of the Augmented Humans International Conference 2025},
pages = {241–252},
numpages = {12},
keywords = {Perceptual Learning, Multisensory Integration, Wearable, Haptics, Pitch Intervals},
location = {
},
series = {AHs '25}
}

@article{10.1145/3771551,
author = {Nakamura, Yugo},
title = {AIoT-Driven Health Behavioral Security: Vision and Challenges},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3771551},
doi = {10.1145/3771551},
abstract = {As digital innovation progresses, individuals are increasingly, often unknowingly, exposed to digital temptations and deceptive tactics. These risks, now magnified by foundation AI models, call for protective measures beyond what current digital health technologies can offer. This paper introduces Health Behavioral Security, a novel AIoT-driven framework that positions trusted AIoT devices as a distributed “System 0” layer—an always-on, pre-conscious filter that can sense, interpret, and modulate persuasive cues in real time. Guided by five behavior-centric constructs—Assets, Threats, Vulnerabilities, Risks, and Countermeasures—the framework could establish a secure behavioral ecosystem in which adaptive sensing and nudging loops help preserve autonomy while promoting well-being. We present this vision, outline key research challenges, and discuss future directions for advancing AIoT-based behavioral security in today’s increasingly digital society.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Healthcare},
month = oct,
keywords = {Health Behavioral Security, AIoT as System 0, Digital Nudge, Digital Persuasion, Cognitive Security, Human-AI Interaction, Behavior Change, Sense of Agency, Autonomy Protection, Digital Well-being}
}

@inproceedings{10.1145/3745900.3746104,
author = {Neigel, Peter and Selby, David Antony and Arai, Shota and Tag, Benjamin and van Berkel, Niels and Vollmer, Sebastian and Vargo, Andrew and Kise, Koichi},
title = {Exploring the Alignment of Perceived and Measured Sleep Quality with Working Memory Using Consumer Wearables},
year = {2025},
isbn = {9798400715662},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3745900.3746104},
doi = {10.1145/3745900.3746104},
abstract = {Wearable devices offer detailed sleep-tracking data. However, whether this information enhances our understanding of sleep or simply quantifies already known patterns remains unclear. This work explores the relationship between subjective sleep self-assessments and sensor data from an Oura ring over 4–8 weeks in-the-wild. 29 participants rated their sleep quality daily compared to the previous night and completed a working memory task. Our findings reveal that differences in REM sleep, nocturnal heart rate, N-Back scores, and bedtimes highly predict sleep self-assessment in significance and effect size. For N-Back performance, REM sleep duration, prior night’s REM sleep, and sleep self-assessment are the strongest predictors. We demonstrate that self-report sensitivity towards sleep markers differs among participants. We identify three groups, highlighting that sleep trackers provide more information gain for some users than others. Additionally, we make all experiment data publicly available.},
booktitle = {Proceedings of the Augmented Humans International Conference 2025},
pages = {381–398},
numpages = {18},
keywords = {Self-Assessment, Wearable Devices, Sleep},
location = {
},
series = {AHs '25}
}

@article{10.1145/3771283,
author = {Hadadi, Fatemeh and Xu, Qinghua and Bianculli, Domenico and Briand, Lionel},
title = {LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3771283},
doi = {10.1145/3771283},
abstract = {Most log-based anomaly detectors assume logs are stable, though in reality they are often unstable due to software or environmental changes. Anomaly detection on unstable logs (ULAD) is therefore a more realistic, yet under-investigated challenge. Current approaches predominantly employ machine learning (ML) models, which often require extensive labeled data for training. To mitigate data insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that combines ML models — decision tree, k-nearest neighbors, and a feedforward neural network — with a Large Language Model (Mistral) through ensemble learning. FlexLog also incorporates a cache and retrieval-augmented generation (RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we configured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and SYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points (pp) in F1 score while using much less labeled data (62.87 pp reduction). When trained on the same amount of data as the baselines, FlexLog achieves up to a 13 pp increase in F1 score on ADFA-U across varying training dataset sizes. Additionally, FlexLog maintains inference time under one second per log sequence, making it suitable for most applications, except latency-sensitive systems. Further analysis reveals the positive impact of FlexLog’s key components: cache, RAG, and ensemble learning.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
keywords = {unstable logs, anomaly detection, data efficiency, ensemble learning, large language models}
}

@inbook{10.1145/3757110.3757188,
author = {Liu, Xinming and Bian, Junxin},
title = {Research on Fuel Consumption Prediction for Open-Pit Mine Trucks Using GA-LSTM},
year = {2025},
isbn = {9798400714344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3757110.3757188},
abstract = {Real-time data collection via vehicle-mounted sensors captures multi-dimensional parameters such as fuel consumption, speed, route, and driving behavior of mining trucks. A genetic algorithm-optimized LSTM model is then employed to construct a fuel consumption prediction model, enabling precise fuel consumption forecasting. This approach reduces fuel consumption by 12-18\%, effectively minimizing fuel theft, leakage, and energy waste while promoting green energy conservation and sustainable mining practices aligned with China's sustainable development objectives.},
booktitle = {Proceedings of the 2025 2nd International Conference on Modeling, Natural Language Processing and Machine Learning},
pages = {470–474},
numpages = {5}
}

@inproceedings{10.1145/3649601.3698748,
author = {De Santis, Marco and Esposito, Christian},
title = {Human-in-the-Loop for Trustworthiness of Federated Learning},
year = {2025},
isbn = {9798400706066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649601.3698748},
doi = {10.1145/3649601.3698748},
abstract = {The rapid advancement of Artificial Intelligence (AI) and its pervasive application throughout all societal processes have introduced significant trust, security and privacy challenges to AI-empowere software and systems, particularly in areas where data sensitivity is critical. To this aim, the EU Commission and other institutions have proposed various governance legal tools for AI, such as the recent AIAct, where human control is integrated within AI solutions. This paper proposes an innovative system that incorporates Human-in-the-Loop (HITL), introduced by the AIAct, with Federated Learning (FL) to enhance the trustworthiness of AI systems. By allowing human oversight in the AI decision-making process, HITL addresses bias and model transparency issues, while FL ensures data privacy by decentralized training data. Additionally, the system incorporates blockchain technology to secure and validate model updates, further increasing the system's reliability and transparency. The proposed framework is evaluated using the CIFAR-10 dataset, with results indicating significant improvements in model accuracy and trustworthiness through human intervention. This research offers a comprehensive approach to addressing the inherent challenges in deploying trustworthy AI systems in sensitive and regulated environments.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {91–98},
numpages = {8},
keywords = {federated learning, self-sovereign identity, human-in-the-loop, blockchain, AI act, data privacy, decentralized authentication, artificial intelligence governance, machine learning security, explainable AI, trustworthy AI, AI ethics, digital identity management, collaborative machine learning, privacy-preserving AI},
location = {HABITA79 Pompeii, Pompei, Italy},
series = {RACS '24}
}

@inproceedings{10.1145/3717511.3747074,
author = {Du, Haoyang and Chhatre, Kiran and Peters, Christopher and Keegan, Brian and McDonnell, Rachel and Ennis, Cathy},
title = {Synthetically Expressive: Evaluating gesture and voice for emotion and empathy in VR and 2D scenarios},
year = {2025},
isbn = {9798400715082},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3717511.3747074},
doi = {10.1145/3717511.3747074},
abstract = {The creation of virtual humans increasingly leverages automated synthesis of speech and gestures, enabling expressive, adaptable agents that effectively engage users. However, the independent development of voice and gesture generation technologies, alongside the growing popularity of virtual reality (VR), presents significant questions about the integration of these signals and their ability to convey emotional detail in immersive environments. In this paper, we evaluate the influence of real and synthetic gestures and speech, alongside varying levels of immersion (VR vs. 2D displays) and emotional contexts (positive, neutral, negative) on user perceptions. We investigate how immersion affects the perceived match between gestures and speech and the impact on key aspects of user experience, including emotional and empathetic responses and the sense of co-presence. Our findings indicate that while VR enhances the perception of natural gesture–voice pairings, it does not similarly improve synthetic ones—amplifying the perceptual gap between them. These results highlight the need to reassess gesture appropriateness and refine AI-driven synthesis for immersive environments.},
booktitle = {Proceedings of the 25th ACM International Conference on Intelligent Virtual Agents},
articleno = {16},
numpages = {10},
keywords = {ML for animation, Speech synthesis, User studies, Perception in VR},
location = {
},
series = {IVA '25}
}

@inproceedings{10.1145/3715071.3750411,
author = {Bahmani, Nima and Salami, Dariush and Yigitler, Huseyin and Hietala, Juhapekka and Panula, Tuukka and Sigg, Stephan},
title = {Wearable Non-Invasive Arterial Pulse Detection with a Millimeter-wave Radar},
year = {2025},
isbn = {9798400714818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715071.3750411},
doi = {10.1145/3715071.3750411},
abstract = {Cardiovascular diseases remain a leading cause of mortality and disability. The convenient measurement of cardiovascular health using wearables is a key enabler to foster continuous, accurate and early detection, diagnosis and management of cardiovascular diseases. We integrate Photoplethysmography (PPG) and mmWave sensing in a mobile wearable device to detect the Inter-beat interval (IBI) and arterial pulse wave. Particularly, We have developed a hardware prototype that integrates both sensing modalities and have verified the system against the gold clinical standard reference with a diverse population of 23 subjects.},
booktitle = {Proceedings of the 2025 ACM International Symposium on Wearable Computers},
pages = {184–190},
numpages = {7},
keywords = {arterial pulse wave, mmwave, mobile smart systems, ppg, wearable computing},
location = {Espoo, Finland},
series = {ISWC '25}
}

@inproceedings{10.1145/3715071.3750406,
author = {Hossain, Tahera and Saiedur Rahaman, Mohammad and Yamashita, Keiko and Kono, Aoi and Guowei, Ma and Oyama, Shintaro and Yonezawa, Takuro},
title = {Quantifying Work-Stress in Surgical Intensive Care Unit Nurses: A Multimodal Activity-Aware Physiological Analysis},
year = {2025},
isbn = {9798400714818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715071.3750406},
doi = {10.1145/3715071.3750406},
abstract = {Work-related stress in critical care settings threatens both nurse well-being and patient safety. While understanding and accurately quantifying this stress is important in these high-stakes environments, it remains a significant challenge due to the inherent complexity of clinical workflows and the wide variability in individual stress responses. To address this, our study investigates how physiological signals captured via wearable sensors correspond to nurses' perceived stress during Surgical Intensive Care Unit (SICU) activities. We captured physiological indicators of stress using two wearable devices: a wrist-worn EmbracePlus and an upper arm-worn Apple Watch as nurses engaged in a range of simulated SICU activities. These physiological signals were then synchronized with perceived stress, captured using validated self-report questionnaires. Our analysis shows significant variations in stress responses across different SICU activities, with elevated heart rate variability (HRV) and increased perceived stress observed during physically demanding activities such as ''Patient Transfer''. A machine learning model trained on combined physiological and contextual features related to nursing activities achieved 80\% accuracy under leave-one-participant-out cross-validation, demonstrating the potential of wearable sensing for activity-aware stress recognition. Principal Component Analysis (PCA) further showed that the wrist-worn sensor produced more structured and separable HRV patterns than the arm-worn device, suggesting the critical role of sensor placement and device characteristics in reliable stress assessment.},
booktitle = {Proceedings of the 2025 ACM International Symposium on Wearable Computers},
pages = {127–133},
numpages = {7},
keywords = {surgical intensive care unit (sicu), wearables, work-stress},
location = {Espoo, Finland},
series = {ISWC '25}
}

@inproceedings{10.1145/3715071.3750402,
author = {Kalpande, Sharmad and Sahu, Nilesh Kumar and Lone, Haroon R.},
title = {Investigating the Generalizability of ECG Noise Detection Across Diverse Data Sources},
year = {2025},
isbn = {9798400714818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715071.3750402},
doi = {10.1145/3715071.3750402},
abstract = {Electrocardiograms (ECGs) are vital for monitoring cardiac health, enabling the assessment of heart rate variability (HRV), detection of arrhythmias, and diagnosis of cardiovascular conditions. However, ECG signals recorded from wearable devices are frequently corrupted by noise artifacts, particularly those arising from motion and large muscle activity, which distort R-peaks and the QRS complex. These distortions hinder reliable HRV analysis and increase the risk of clinical misinterpretation. Existing studies on ECG noise detection typically evaluate performance on a single dataset, limiting insight into the generalizability of such methods across diverse sensors and recording conditions. In this work, we propose an HRV-based machine learning approach to detect noisy ECG segments and evaluate its generalizability using cross-dataset experiments on four datasets collected in both controlled and uncontrolled settings. Our method achieves over 90\% average accuracy and an AUPRC exceeding 90\%, even on previously unseen datasets-demonstrating robust performance across heterogeneous data sources. To support reproducibility and further research, we also release a curated and labeled ECG dataset. annotated for noise artifacts.},
booktitle = {Proceedings of the 2025 ACM International Symposium on Wearable Computers},
pages = {113–119},
numpages = {7},
keywords = {ecg noise, generalizability, machine learning, wearables},
location = {Espoo, Finland},
series = {ISWC '25}
}

@inproceedings{10.1145/3715071.3750420,
author = {DeVrio, Nathan and Harrison, Chris},
title = {EverRing: Powering Battery-Free, Highly-Capable Smart Rings with Headset RF Energy},
year = {2025},
isbn = {9798400714818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715071.3750420},
doi = {10.1145/3715071.3750420},
abstract = {Rings are a powerful form factor for interaction in XR. Being located on a finger and near other digits, they are well positioned to enable rich human-computer input and outputs, such as micro-gesture sensing and providing haptic feedback. Compared to other wearables, such as watches and earbuds, rings are less obtrusive, so much so that some users wear them continuously. However, their small size inherently means small batteries, and thus very limited I/O capabilities. This led us to ask: how can we create a ring that does not need to be removed to be recharged, and has sufficient power to continuously support rich interactions? This led us to develop EverRing, a battery-less ring device powered wirelessly by RF energy transmitted from an XR headset (and future glassses). In evaluations, we quantify how much RF power can be captured wirelessly in the interactive volume in front of a user, how much power each input and output capability consumes, and how the ring maintains a balanced power budget in common XR use cases (productivity, gaming, etc.).},
booktitle = {Proceedings of the 2025 ACM International Symposium on Wearable Computers},
pages = {68–75},
numpages = {8},
keywords = {battery-free, input, mixed reality, smart ring, wearable},
location = {Espoo, Finland},
series = {ISWC '25}
}

@inproceedings{10.1145/3715071.3750415,
author = {Hoskeri, Rahul Sidramappa and Lin, Shan and Huang, Hua},
title = {SSC: Simultaneous Signal Separation and Classification for Concurrent Activities},
year = {2025},
isbn = {9798400714818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715071.3750415},
doi = {10.1145/3715071.3750415},
abstract = {Wearable sensing systems, such as smartwatch and wristband, can recognize many activities, including sports activities, and hand gestures. Research projects on activity recognition usually assume users perform a single activity. In reality, users can multitask, such as using gestures while running, taking a phone call while tidying up. Existing research on concurrent activity monitoring often requires the user to attach multiple wearables, but attaching multiple devices is often inconvenient and impractical in real-world scenarios. Therefore, monitoring concurrent activities using a single wearable device becomes a challenging task. In this paper, we present a Simultaneous Separation and Classification framework (SSC), which uses a signal separation approach to recognize concurrent activities. The SSC framework includes a Class-Conditioned Signal Tracker module that isolates sensor measurements from different activities in real-time. The isolated signals are used by gesture and gait recognition module to accurately authenticate users in real time while they perform gestures during walking or running, enabling a seamless wearable interface. We tested the proposed solution on a wrist-worn smartwatch. In a study with 10 participants and over 1800 gesture samples, our algorithm achieved 95\% accuracy in mobility detection and 93\% accuracy in gesture recognition. Furthermore, we could accurately perform user authentication with an accuracy of 91\% during walking and 83\% while running. Our SSC dataset can be found on Github here.},
booktitle = {Proceedings of the 2025 ACM International Symposium on Wearable Computers},
pages = {149–155},
numpages = {7},
keywords = {concurrent activity recognition, signal separation},
location = {Espoo, Finland},
series = {ISWC '25}
}

@inproceedings{10.1145/3715071.3750408,
author = {Kotera, Ryuta and Murao, Kazuya},
title = {Estimating the Position of a Wearable Device Using Pulse Wave Peak Time Differences with a Smartwatch},
year = {2025},
isbn = {9798400714818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715071.3750408},
doi = {10.1145/3715071.3750408},
abstract = {Wearable devices come in various forms. In addition to those designed for specific body parts, such as wristwatch, eyeglass, and ring devices, there are also wearable devices that can be attached to arbitrary body positions, such as instant tattoo-type and bandage-type devices. When a wearable device is attached to an arbitrary position on the body, it is necessary for the device to recognize its attachment site in order to adjust its function and processing accordingly. In this paper, we propose a method for estimating the attachment position of a wearable device with an unknown position by using pulse wave sensors mounted on both a device with a known position and a device with an unknown position. Because the arrival time of pulse waves varies depending on the distance from the heart, the time difference between pulse wave peaks at the wrist and another body part can be used to infer the attachment site. The proposed method estimates the device's attachment position based on the difference in pulse wave peak detection times between the wrist and the unknown body part. For evaluation, data were collected from 15 body positions: forehead, nose, mouth, left ear, right ear, neck, left upper arm, right upper arm, left wrist, right wrist, left finger, right finger, waist, left toe, and right toe. The method was evaluated under two conditions: using data collected on the same day and on different days. When estimating two positions, the F1 score was 0.94 for same-day data and 0.64 for different-day data. With fifteen positions, the scores were 0.64 and 0.14, respectively.},
booktitle = {Proceedings of the 2025 ACM International Symposium on Wearable Computers},
pages = {61–67},
numpages = {7},
keywords = {attachment position estimation, pulse sensor, wearable device},
location = {Espoo, Finland},
series = {ISWC '25}
}

@inproceedings{10.1145/3715071.3750425,
author = {Liu, Mengxi and Ray, Lala Shakti Swarup and Bian, Sizhen and Watanabe, Ko and Bhatt, Ankur and Sorysz, Joanna and Torah, Russel and Zhou, Bo and Lukowicz, Paul},
title = {From Neck to Head: Bio-Impedance Sensing for Head Pose Estimation},
year = {2025},
isbn = {9798400714818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715071.3750425},
doi = {10.1145/3715071.3750425},
abstract = {We present NeckSense, a novel wearable system for head pose tracking that leverages multi-channel bio-impedance sensing with soft, dry electrodes embedded in a lightweight, necklace-style form factor. NeckSense captures dynamic changes in tissue impedance around the neck, which are modulated by head rotations and subtle muscle activations. To robustly estimate head pose, we propose a deep learning framework that integrates anatomical priors, including joint constraints and natural head rotation ranges, into the loss function design. We validated NeckSense on 7 participants using the current state-of-the-art pose estimation model as ground truth. Our system achieves a mean per-vertex error of 25.9 mm across various head movements with a leave-one-person-out cross-validation method, demonstrating that a compact bio-impedance wearable can deliver head-tracking performance comparable to state-of-the-art vision-based methods.},
booktitle = {Proceedings of the 2025 ACM International Symposium on Wearable Computers},
pages = {76–82},
numpages = {7},
keywords = {bio-impedance sensing, pose estimation, textile electrode},
location = {Espoo, Finland},
series = {ISWC '25}
}

@article{10.1145/3748604,
author = {Laato, Samuli and Nummenmaa, Timo and Yoshida, Hironori and Chambers, Philip and Uhlgren, Ville-Veikko and Hu, Botao Amber and Kordyaka, Bastian and Hamari, Juho},
title = {Crowdsourcing Environment Data with Gamified Augmented Reality Mini-Games},
year = {2025},
issue_date = {October 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
url = {https://doi.org/10.1145/3748604},
doi = {10.1145/3748604},
abstract = {Remote sensing for observing and recording our surroundings is becoming mainstream. Technologies, such as light, detection, and ranging (LiDAR), are now part of consumer mobile devices and provide a variety of novel interaction opportunities with the environment. Mobile remote sensing also provides affordances for crowdsourcing through location-based applications such as games and gamified systems. While such use cases today are technologically feasible, there is a lack of understanding of how and what kinds of interactions and applications would be both (1) engaging and motivating for users and also (2) maximize the volume and quality of the data being gathered. In this study, we investigate these challenges by developing and testing four gamified augmented reality prototypes that use LiDAR for collecting point cloud data during location-based gaming. Through field testing, interviews, and surveys with 21 participants, followed by reflexive thematic analysis, we identified five themes of dynamics, which exemplify tensions and challenges to designing gamified AR crowdsourcing. The findings primarily point to hazards in design that may undermine user motivation as well as constraints of the environments themselves in facilitating and affording meaningful and rich (gameful) interaction.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {GAMES009},
numpages = {24},
keywords = {LiDAR, Location-based media, augmented reality, crowdsourcing, user-generated content}
}

@article{10.1145/3748632,
author = {Park, Jisu and Jang, Soyoun and Bolter, Jay David},
title = {Me, the Elephant, and the Virtual World: Exploring Copresence in VR},
year = {2025},
issue_date = {October 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {6},
url = {https://doi.org/10.1145/3748632},
doi = {10.1145/3748632},
abstract = {Fostering mindful relationships with nature is crucial for addressing the environmental crisis. Virtual Reality (VR) shows potential for ecological engagement, but its impact depends on how nature is framed and experienced. This paper explores copresence, the feeling of togetherness, as a design approach for cultivating reciprocal and attentive interactions with nonhuman entities. Drawing on expert interviews, we developed Eyes of the Wild, a VR wildlife experience. In a between-subjects study (n = 32), we examined how copresence, manipulated by enabling or disabling mutual awareness and behavioral responsiveness (e.g., animals responding to user presence), might support nature-connectedness. Surveys showed pre-post improvements across both conditions, with no significant differences between groups. However, qualitative findings revealed nuanced engagement patterns, including temporal-spatial dynamics, perceptual asymmetries, and the complexities of copresent interactions, which shaped participants' sense of trust and connection with nonhuman others. These insights underscore the potential and challenges of leveraging copresence for nature-centric VR.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {GAMES037},
numpages = {19},
keywords = {Human-nature relationship, copresence, nature connectedness, togetherness, virtual nature, virtual reality, virtual wildlife}
}

@inproceedings{10.1145/3742800.3742825,
author = {Till, Sarina C and Wilson, Taryn and Dewlok, Mishka and Verdezoto, Nervo and Densmore, Melissa},
title = {Move With Me: Co-designing a Tangible User Interface To Promote Physical Activity Among Rural South African Children},
year = {2025},
isbn = {9798400715211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3742800.3742825},
doi = {10.1145/3742800.3742825},
abstract = {Childhood physical inactivity and obesity are growing concerns globally, including in rural South Africa, where children often lack access to safe play spaces and well-resourced Early Childhood Development (ECD) centers and face increasing screen time. These challenges limit opportunities for physical development in early childhood. To address this, we developed Move With Me, a wearable tangible user interface (TUI) in the form of a superhero cape that encourages physical activity as a probe for further investigation. While many commercial technologies that encourage movement exist, they are unsuitable to rural communities due to their high cost and reliance on stable internet and electricity infrastructure. In response to these challenges, we co-designed a more contextually relevant solution with rural South African mothers using the technology probe to support the ideation and co-design process. The final cape design integrates motion sensors, LED lights, Bluetooth Low Energy (BLE), and a solar rechargeable battery pack, delivering real-time feedback without internet connectivity or stable electricity. A companion mobile app gamifies movement with culturally relevant animations to engage children. We found that co-design empowered mothers to tailor the technology to their context, suggesting affordable components and re-purposing existing smartphone features such as sound and animations instead of using costly electronics incorporated into the cape. Their contributions led to a low-cost, offline-capable, and personalized TUI. Our work demonstrates how wearable TUIs, developed through inclusive design, can support physical activity in resource-limited settings. We contribute practical insights for designing sustainable technologies for rural contexts, emphasizing affordability, low power consumption, offline functionality, and the value of embedding co-creation throughout the design process.},
booktitle = {Proceedings of the 12th International Conference on Communities \&amp; Technologies},
pages = {173–187},
numpages = {15},
keywords = {co-design, cape, rural, tangible user interface, hci4d, community-based},
location = {
},
series = {C&amp;T '25}
}

@inproceedings{10.1145/3742800.3742842,
author = {Viswanathan, Abhishek and Xu, Vasco and Babay, Amy and Farzan, Rosta and Birdy, Aaron},
title = {Multi-dimensional Engagement for Richer Hyperlocal Citizen Science Data},
year = {2025},
isbn = {9798400715211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3742800.3742842},
doi = {10.1145/3742800.3742842},
abstract = {As global climate patterns become increasingly unpredictable, the need for precise and localized climate data becomes more important. Traditional environmental monitoring systems often fail to engage local communities, creating a disconnect between institutional data collection and grassroots environmental stewardship. Citizen science can offer a valuable paradigm for involving the public in hyperlocal data collection and promoting community awareness. Citizen science empowers communities to collect pertinent environmental data tailored to their unique challenges. However, crafting effective local citizen science initiatives poses challenges like ensuring data reliability and integrating local expertise.Our research team collaborated with a local non-profit organization to develop a "Social Sensor Network" that combines physical environmental sensors with a social network of citizen scientists, aiming to efficiently collect hyperlocal environmental data. Over three months, we deployed the SSN in Pittsburgh, Pennsylvania, USA, engaging 17 residents in a collaborative process of environmental data collection and interpretation. The automatic data collection process using low-cost sensors, complemented by the contextual insights of the participants, led to the gathering of highly detailed air quality data, exceeding official monitoring capacities. Participants actively participated in data comparison, trend identification, and environmental data exploration. The project reveals how experiential learning in citizen science cultivates not just environmental awareness but also collective agency among participants. The data collected by the participants supported our partner non-profit organization to create environmental assessments and carry out interventions. Our study also demonstrates how partnerships between local organizations and researchers can transform citizen science into a platform for community-driven environmental advocacy.},
booktitle = {Proceedings of the 12th International Conference on Communities \&amp; Technologies},
pages = {116–129},
numpages = {14},
keywords = {community science, air quality, low-cost sensor},
location = {
},
series = {C&amp;T '25}
}

@inproceedings{10.1145/3759179.3760449,
author = {Liu, Beiying and Zhu, Bin},
title = {Detecting Psychological Stress in Online Learning Environments via Secure Multimodal Behavioral Analysis},
year = {2025},
isbn = {9798400718632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3759179.3760449},
doi = {10.1145/3759179.3760449},
abstract = {With the rapid development of artificial intelligence and privacy-preserving computing in educational technology, detecting psychological stress in online learning environments has become a pressing research topic. Current approaches for detecting stress often rely on self-reported surveys or single-modal behavioral data, which lead to delayed or inaccurate assessments. Meanwhile, privacy concerns associated with sensitive behavioral data further restrict their practical application. This study proposes a secure multimodal behavioral analysis framework to detect psychological stress in online learners by integrating heterogeneous data types, including interaction logs, facial expressions, typing patterns, and physiological signals such as heart rate variability (HRV) from wearable devices. The framework employs modality-specific encoders combined with federated learning (FL) and differential privacy (DP) mechanisms to ensure data security while capturing synergistic patterns across behavioral modalities. A multi-task learning objective simultaneously optimizes stress classification and stress intensity regression, thereby enhancing real-time applicability. Experimental validation on a cohort of online university students demonstrates superior detection performance compared to conventional unimodal or non-secure models, with interpretability analyses identifying key stress-related behavioral markers. The model's ability to detect stress in real time while preserving data privacy could enable personalized intervention strategies and advance the development of ethical AI-driven support systems in online education. By bridging behavioral psychology, secure machine learning, and educational technology, this work facilitates the translation of privacy-preserving stress detection into scalable tools for online learning environments.},
booktitle = {Proceedings of the 10th International Conference on Cyber Security and Information Engineering},
pages = {311–317},
numpages = {7},
keywords = {Behavioral markers, Multimodal behavioral analysis, Online learning environments, Privacy-preserving machine learning, Psychological stress detection, Secure artificial intelligence},
location = {
},
series = {ICCSIE '25}
}

@inproceedings{10.1145/3759179.3760446,
author = {Yan, Qi},
title = {Explainable Machine Learning for Detecting Malicious Student Behavior in Campus Networks},
year = {2025},
isbn = {9798400718632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3759179.3760446},
doi = {10.1145/3759179.3760446},
abstract = {As digital infrastructure in higher education expands, campus networks face increasing threats from malicious student behaviors such as unauthorized resource access and exam-related cheating. While machine learning models have shown promise in anomaly detection, their lack of interpretability undermines trust and limits deployment in sensitive academic environments. This study proposes a hybrid explainable artificial intelligence (XAI) framework that integrates Extreme Gradient Boosting (XGBoost) and Long Short-Term Memory (LSTM) for behavior classification, enhanced by SHapley Additive exPlanations (SHAP) and Local Interpretable Model-Agnostic Explanations (LIME) for global and local interpretability. Tested on over 2.3 million real-world campus network sessions, the system achieves an F1-score of 88.7\% and an Area Under the Receiver Operating Characteristic Curve (AUC-ROC) of 93.1\%, while increasing administrator trust scores by 38\%. A live deployment during exam periods further demonstrates its practical value, reducing false positives to 10.1\%, cutting average investigation time by 50\%, and supporting proportional policy enforcement. The results highlight the operational, ethical, and governance benefits of embedding explainability into campus cybersecurity systems.},
booktitle = {Proceedings of the 10th International Conference on Cyber Security and Information Engineering},
pages = {299–305},
numpages = {7},
keywords = {Anomaly Detection, Campus Network Security, Explainable Machine Learning, LIME, LSTM, Malicious Behavior Detection, SHAP, XGBoost},
location = {
},
series = {ICCSIE '25}
}

@inproceedings{10.1145/3742886.3756696,
author = {Graf, Philipp and Marquardt, Manuela},
title = {Assisting Under Constraints – Structural Obstacles of Agentic Technology for Health Data Collection},
year = {2025},
isbn = {9798400719967},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3742886.3756696},
doi = {10.1145/3742886.3756696},
abstract = {The healthcare sector is shaped by complex emotional, formal, and structural conditions, creating a strong demand for technological support for both healthcare professionals and patients. Intelligent Virtual Assistants (IVAs) offer promising potential through familiar interfaces such as natural language input and visual or physical representations that aim to enhance accessibility. However, their development and implementation encounter specific challenges which we identify as structural and particular to the medical field. Drawing on qualitative data from the MIA-PROM project, we reflect on three key socio-material interrelations that, in our view, represent central obstacles to the successful use of IVAs in healthcare. First, we highlight the particular difficulties posed by natural language systems in clinical contexts, where communication is emotionally sensitive and highly formalized. Second, we examine the conceptual tension between emotional support and data collection, questioning whether both goals can be meaningfully aligned by technical means alone. Third, we address the complexity of configuration and adaptivity: individual user needs are often too diverse and nuanced to be met by automated processes alone. We conclude by discussing the necessity of human configuration work in one-time interactions and suggest possible approaches to address these challenges.},
booktitle = {Adjunct Proceedings of the 25th ACM International Conference on Intelligent Virtual Agents},
articleno = {42},
numpages = {7},
keywords = {computational agents, healthcare, technical assistance, health data, natural language},
location = {
},
series = {IVA Adjunct '25}
}

@article{10.1145/3769860,
author = {Hadwen-Bennett, Alex and Healy, Lulu and Sentance, Sue},
title = {An embodied sense of programming: an anti-ableist framework for the analysis of learners’ developing senses of programming},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3769860},
doi = {10.1145/3769860},
abstract = {This paper proposes the Critical Embodied Sense framework, which challenges traditional, ableist assumptions in programming education that privilege abstract, decontextualized learning experiences over embodied engagement. The framework is designed to enable researchers to analyse learners’ developing senses of programming concepts through an anti-ableist lens. It draws upon Vygotsky’s cultural-historical theory, contemporary research from the embodied cognition field and perspectives from critical theory. We illustrate the application of the framework using a study which investigated the developing senses of visually impaired learners working with a tangible programming language. Through the application of the framework to this study we demonstrate the integral role that tools, emotions and affect play in shaping learners’ senses of programming concepts. We finish by discussing potential wider applications of the framework which include researching the development of a sense of self as a programmer in under-represented groups and guiding the design of inclusive computing learning environments.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Educ.},
month = sep,
keywords = {programming education, inclusion, embodied cognition}
}

@inproceedings{10.1145/3742886.3756699,
author = {Harnisch, Philipp Lars and Schuhmann, Daniel and Cosic, Ivana},
title = {Towards Automatic Personalization of Speech Dialog for Enhanced User Experience},
year = {2025},
isbn = {9798400719967},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3742886.3756699},
doi = {10.1145/3742886.3756699},
abstract = {As intelligent virtual agents (IVAs) become increasingly integrated into healthcare, their ability to communicate empathetically and adapting to client-specific needs is critical. Yet most current systems rely on generic, neutral language that may limit user engagement, especially in sensitive or emotionally charged contexts. This paper proposes a research agenda toward automatic personalization of speech dialog as one important factor of IVAs — the dynamic adaptation of language style and tone to the individual user — as one important factor of IVAs to improve user experience (UX) and acceptance.. We outline a conceptual framework for dialog personalization, identify core challenges, and propose a concept for system architecture and its evaluation. We ground our vision in relevant literature across human-computer interaction, natural language processing, and health communication. To support the feasibility of this direction, we present results from a proof-of-concept study suggesting that personalized speech dialog can significantly enhance user-perceived system response quality of a task-oriented system to fill out medical surveys. This work lays the foundation for developing more effective and personalized voice communication with IVAs in healthcare applications.},
booktitle = {Adjunct Proceedings of the 25th ACM International Conference on Intelligent Virtual Agents},
articleno = {45},
numpages = {8},
keywords = {Automatic Adaptation, Personalization, Speech Dialog, LLMs},
location = {
},
series = {IVA Adjunct '25}
}

@inproceedings{10.1145/3749859.3749862,
author = {Zhang, Yihao and Ni, Haoran and Sun, Beichen and Chin, Zheng Yang and Ang, Kai Keng},
title = {Depth, Thermal and RGB-Segmented Silhouette Imaging in Human Pose Estimation for Activity Monitoring},
year = {2025},
isbn = {9798400712180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3749859.3749862},
doi = {10.1145/3749859.3749862},
abstract = {Human Pose Estimation (HPE) is a computer vision task that involves deep learning algorithms to estimate keypoints of different human joints in images. The keypoints form a skeleton model of the human body with applications in activity monitoring for healthcare, domotics and surveillance systems. However, HPE that uses RGB images deteriorates under inadequate lighting and raises privacy concerns because identifiable facial features are recorded. Thus, training a HPE deep learning model that predicts keypoints on lighting-invariant and privacy-preserving human silhouette images is proposed. Silhouette Imaging Dataset (SID) was collated from numerous public datasets, consisting of depth images, thermal images and simulated human silhouette images generated from RGB images using image segmentation, which were used to fine-tune the YOLO11s-pose HPE model. To classify predicted keypoints from the HPE model into different actions for activity monitoring, this study proposed a Residual Temporal Convolution Network with Stacked Bidirectional Gated Recurrent Unit (ResTCN-SBiGRU) Human Activity Recognition (HAR) deep learning model. The HAR model was trained on a public dataset to recognise 11 actions. The study shows the following results: the fine-tuned YOLO11s-pose yielded an mAP50-95(P) of 0.851; ResTCN-SBiGRU achieved an F1-score of 0.933. Finally, the study developed a real-time activity monitoring application using HPE, HAR and an additional proposed motion detection algorithm. The application gets live video input from a KinectV2 sensor and a laptop webcam, and displays pose, action and motion visualisations. This allows for testing of the different models and demonstrates the feasibility of activity monitoring using silhouette-based HPE.},
booktitle = {Proceedings of the 2025 7th International Conference on Image, Video and Signal Processing},
pages = {16–24},
numpages = {9},
keywords = {Computer Vision, Human Activity Recognition, Human Pose Estimation},
location = {
},
series = {IVSP '25}
}

@inproceedings{10.1145/3749859.3749861,
author = {Islam, Naeem Ul and Shah, Syed H.},
title = {Edge Enhanced Scene Understanding and Depth Estimation},
year = {2025},
isbn = {9798400712180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3749859.3749861},
doi = {10.1145/3749859.3749861},
abstract = {An essential requirement for intelligent robots is the ability to perceive and comprehend the 3D structure of their surroundings without relying on expensive sensors. This perception capability is achievable with AI-based monocular depth estimation, in which a model uses an RGB image to predict the corresponding depth map. Researchers have developed various models that can produce accurate depth maps from a given RGB image. For instance, a prominent model is ZoeDepth, which combines relative and metric depth estimation to enhance depth prediction accuracy on various datasets. However, it still has challenges in accurately estimating fine details and has limitations in generalization across different environments with varying textures and lighting conditions. To overcome these limitations and improve the depth estimation accuracy, we have combined two essential image processing techniques: Canny Edge detection and high pass filtering. This improves edge detection and increases depth accuracy in scenes with complex object structures. Numerous experimental analyses have been carried out for the performance evaluation of the proposed approach for accurate depth estimation. The model was applied to indoor and outdoor datasets, significantly improving edge recognition and depth estimation, especially in zero-shot generalization tasks.},
booktitle = {Proceedings of the 2025 7th International Conference on Image, Video and Signal Processing},
pages = {9–15},
numpages = {7},
keywords = {Depth Estimation, Canny Edge Detection, High Pass Filters},
location = {
},
series = {IVSP '25}
}

@inproceedings{10.1145/3749859.3749864,
author = {Abubakar, Yahaya Idris and Dia, Mamadou and Siarry, Patrick and Othmani, Alice},
title = {Improving Video Surveillance through ViViTRED: A Transformer-Based Detection Model for Rare and Abnormal Events},
year = {2025},
isbn = {9798400712180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3749859.3749864},
doi = {10.1145/3749859.3749864},
abstract = {Detecting rare and abnormal events during video surveillance is critical to improving security. However, this remains challenging due to limited data, the environment, and the complex dynamics of video sequences. Existing methods often fail in real-time applications and struggle to adapt to the dynamic nature of such events. We proposed ViViTRED, a transformer-based model that uses advanced spatiotemporal feature extraction to address these challenges. Building on the Video Vision Transformer (ViViT) architecture, ViViTRED incorporates tubelet embedding, positional encoding, and a stochastic Local Winner-Takes-All (LWTA) layer to enhance feature extraction and highlight key components in video frames. Evaluated on benchmark datasets such as UBI-Fights and Hockey Fights, ViViTRED achieved higher accuracy rates of 97.48\% and 94.00\%, respectively, outperforming state-of-the-art methods. Our results demonstrate that ViViTRED significantly improves rare event detection in complex environments and shows strong potential for real-time surveillance. Future work will optimize the model for efficient deployment and integration of multimodal data, such as audio and sensor data.},
booktitle = {Proceedings of the 2025 7th International Conference on Image, Video and Signal Processing},
pages = {41–47},
numpages = {7},
keywords = {Abnormal event detection, rare events, deep learning, neural networks},
location = {
},
series = {IVSP '25}
}

@inproceedings{10.1145/3746059.3747771,
author = {Gonzalez, Jesse T. and Zhang, Yanzhen and Zhu, Dian and Yu, Alice and Tayal, Sapna and Furniturewala, Nazm and Qi, Ziying and Moon, Somin Ella and Han, Leyi and Ion, Alexandra and Hudson, Scott E.},
title = {Sculptable Mesh Structures for Large-Scale Form-Finding},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747771},
doi = {10.1145/3746059.3747771},
abstract = {It can be hard to design a physical structure entirely within the confines of a computer monitor. To better capture the interplay between real-world objects and a designer’s work-in-progress, practitioners will often go through a sequence of low-fidelity prototypes (paper, clay, foam) before arriving at a form that satisfies both functional and aesthetic concerns. While necessary, this model-making process can be quite time-consuming, particularly at larger scales, and the resulting geometry can be difficult to translate into a CAD environment, where it will be further refined. This paper introduces a user-adjustable, room-scale, "shape-aware" mesh structure for low-fidelity prototyping. A user physically manipulates the mesh by lengthening and shortening the edges, altering the overall curvature and sculpting coarse forms. The edges are equipped with resistive length sensors, and transmit their configuration to a central computer. The structure can later be reproduced in software, connecting this prototyping stage to the larger computational design pipeline.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {41},
numpages = {14},
keywords = {Digital Twin, Tangible Interface, Reconfigurable Architecture},
location = {
},
series = {UIST '25}
}

@inproceedings{10.1145/3746059.3747601,
author = {Wang, Xue and Zhang, Yang},
title = {Invisibility Cloak: Personalized Smartwatch-Guided Camera Obfuscation},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747601},
doi = {10.1145/3746059.3747601},
abstract = {Cameras are in their golden age due to recent advances in visual AI techniques that significantly extend the applicability and accuracy of vision-based applications including healthcare, entertainment, and security. In public environments, individuals usually have different and changing privacy preferences against their visual information being shared with other entities. To accommodate these varying user needs for visual privacy, we created Invisibility Cloak , a camera obfuscation technique leveraging inertial signals collected from smartwatches to guide an edge device to remove visual information from camera recordings before they are streamed out for cloud-based inferences. Specifically, a smartwatch user can select an obfuscation level that fits their privacy preference in that context and cameras in the environment will use smartwatch signals to identify that user and remove visual information associated with the user. On the conceptual level, our system demonstrates a privacy design rationale which removes information to be shared with a broader internet infrastructure (i.e., cloud) by providing more information to a trusted local camera system (i.e., camera sensor + edge computing device). We developed a custom data-association pipeline and collected data from real-world configurations. Evaluation of our pipeline indicates a user identification accuracy of 95.48\% among 10 individuals when our system is provided with only 2 seconds of data.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {153},
numpages = {15},
keywords = {Camera Obfuscation; Smartwatch; User Identification; Privacy},
location = {
},
series = {UIST '25}
}

@inproceedings{10.1145/3746059.3747392,
author = {Yang, Xiaoying and Lu, Qian and Kim, Jeeeun and Zhang, Yang},
title = {LuxAct: Enhance Everyday Objects for Visual Sensing with Interaction-Powered Illumination},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747392},
doi = {10.1145/3746059.3747392},
abstract = {Imbuing sensing and interactivity into everyday objects has long been sought after within the HCI community to facilitate richer and more immersive user experiences. However, conventional methods rely on costly hardware, such as embedded sensor tags, or passive visual markers that lack digital capabilities to sense user context. We present LuxAct, an interaction-powered visual communication system that enables everyday objects to encode their information and user interaction data into sequences of RGB-colored light. These sequences are decoded by Point of View (POV) cameras on AR headsets or smart glasses to derive meaningful information from interactions. LuxAct mechanisms are self-powered and ultra-low-cost, leveraging striking and plucking on piezoelectric generators to harvest energy from user interactions. Through strategic pattern design, our system transforms visual channels into carriers of both object identification and sensory data, supporting applications with rich sensing needs. We demonstrate a wide range of use cases, including interactive controls, sensate storage, smart water hose, medicine reminders, fingertip probes and beyond, offering a practical alternative for digitizing passive objects to enable ubiquitous sensing in AR-enhanced environments.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {191},
numpages = {18},
keywords = {Interaction Power; Battery-Free; Ubiquitous Computing; Visible Light Communication; Augmented Reality},
location = {
},
series = {UIST '25}
}

@inproceedings{10.1145/3746059.3747694,
author = {Zhang, Chenyang and Ma, Tiffany S and Andrews, John and Gonzalez, Eric J and Gonzalez-Franco, Mar and Yang, Yalong},
title = {ForcePinch: Force-Responsive Spatial Interaction for Tracking Speed Control in XR},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747694},
doi = {10.1145/3746059.3747694},
abstract = {Spatial interaction in 3D environments requires balancing efficiency and precision, which requires dynamic tracking speed adjustments. However, existing techniques often couple tracking speed adjustments directly with hand movements, reducing interaction flexibility. Inspired by the natural friction control inherent in the physical world, we introduce ForcePinch, a novel force-responsive spatial interaction method that enables users to intuitively modulate pointer tracking speed and smoothly transition between rapid and precise movements by varying their pinching force. To implement this concept, we developed a hardware prototype integrating a pressure sensor with a customizable mapping function that translates pinching force into tracking speed adjustments. We conducted a user study with 20 participants performing well-established 1D, 2D, and 3D object manipulation tasks, comparing ForcePinch against the distance-responsive technique Go-Go and speed-responsive technique PRISM. Results highlight distinctive characteristics of the force-responsive approach across different interaction contexts. Drawing on these findings, we highlight the contextual meaning and versatility of force-responsive interactions through four illustrative examples, aiming to inform and inspire future spatial interaction design.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {27},
numpages = {16},
keywords = {Embodied Interaction, Force Input, Gesture, Mixed Reality},
location = {
},
series = {UIST '25}
}

@inproceedings{10.1145/3746059.3747730,
author = {Xu, Tianyu and Li, Jihan and Zu, Penghe and Sahay, Pranav and Kim, Maruchi and Obeng-Marnu, Jack and Miller, Farley and Qian, Xun and Passarella, Katrina and Rachumalla, Mahitha and Nongpiur, Rajeev and Shin, D},
title = {Enhancing XR Auditory Realism via Multimodal Scene-Aware Acoustic Rendering},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747730},
doi = {10.1145/3746059.3747730},
abstract = {In Extended Reality (XR), rendering sound that accurately simulates real-world acoustics is pivotal in creating lifelike and believable virtual experiences. However, existing XR spatial audio rendering methods often struggle with real-time adaptation to diverse physical scenes, causing a sensory mismatch between visual and auditory cues that disrupts user immersion. To address this, we introduce SAMOSA, a novel on-device system that renders spatially accurate sound by dynamically adapting to its physical environment. SAMOSA leverages a synergistic multimodal scene representation by fusing real-time estimations of room geometry, surface materials, and semantic-driven acoustic context. This rich representation then enables efficient acoustic calibration via scene priors, allowing the system to synthesize a highly realistic Room Impulse Response (RIR). We validate our system through technical evaluation using acoustic metrics for RIR synthesis across various room configurations and sound types, alongside an expert evaluation (N=12). Evaluation results demonstrate SAMOSA’s feasibility and efficacy in enhancing XR auditory realism.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {17},
numpages = {16},
keywords = {extended reality, spatial audio rendering, rir synthesis, multimodal machine learning, large language models, scene representation, room acoustics},
location = {
},
series = {UIST '25}
}

@inproceedings{10.1145/3746059.3747667,
author = {Zhang, Jingyi and Lu, Ziwen and Zhu, Changrui and Julier, Simon and Steed, Anthony},
title = {Do You See What I See? Bring Live Pedestrians into an Outdoor Collaborative Mixed Reality Experience},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747667},
doi = {10.1145/3746059.3747667},
abstract = {Collaborative use of mixed reality (MR) devices is blurring the line between virtual and physical worlds. A remote virtual reality (VR) user immersed in a virtual replica of a real-world environment can interact in real-time with an augmented reality (AR) user who is physically present in that location. One challenge with such a setting is that the virtual world experienced by the remote users often lacks the richness of the real world, particularly in outdoor settings where dynamic elements, such as pedestrians, are missing. The first contribution of this paper is to report findings from focus group sessions on an example collaborative outdoor mixed reality system. Participants noted that lack of synchronisation between the AR and VR worlds diminishes the VR user’s sense of having visited the real-world location together with the AR user. To address this, our second contribution is a system that brings live dynamics into a collaborative MR experience using pedestrians as an example. We conducted a user study using a tour-guide scenario, where an in-situ guide using AR interacts with a remote participant in VR. Results showed that in this scenario, most participants perceived the virtual avatars they saw as representations of real humans in situ.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {171},
numpages = {14},
keywords = {Virtual Reality, Augmented Reality, Mixed Reality Collaboration},
location = {
},
series = {UIST '25}
}

@inproceedings{10.1145/3746059.3747767,
author = {Xiao, Yingjing and Huang, Zhichao and Ren, Junbin and Bai, Yuting and Song, Haichuan and Jin, Zhanpeng and Gao, Yang},
title = {Wrist2Finger: Sensing Fingertip Force for Force-Aware Hand Interaction with a Ring-Watch Wearable},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747767},
doi = {10.1145/3746059.3747767},
abstract = {Hand pose tracking is essential for advancing applications in human-computer interaction. Current approaches, such as vision-based systems and wearable devices, face limitations in portability, usability, and practicality. We present a novel wearable system that reconstructs 3D hand pose and estimates per-finger forces using a minimal ring-watch sensor setup. A ring worn on the finger integrates an inertial measurement unit (IMU) to capture finger motion, while a smartwatch-based single-channel electromyography (EMG) sensor on the wrist detects muscle activations. By leveraging the complementary strengths of motion sensing and muscle signals, our approach achieves accurate hand pose tracking and grip force estimation in a compact wearable form factor. We develop a dual-branch transformer network that fuses IMU and EMG data with cross-modal attention to predict finger joint positions and forces simultaneously. A custom loss function imposes kinematic constraints for smooth force variation and realistic force saturation. Evaluation with 20 participants performing daily object interaction gestures demonstrates an average Mean Per Joint Position Error (MPJPE) of 0.57 cm and a fingertip force estimation (RMSE: 0.213, r=0.76). We showcase our system in a real-time Unity application, enabling virtual hand interactions that respond to user-applied forces. This minimal, force-aware tracking system has broad implications for VR/AR, assistive prosthetics, and ergonomic monitoring.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {24},
numpages = {16},
keywords = {Ring, IMU, EMG, Hand Pose Tracking, Finger Force Estimation},
location = {
},
series = {UIST '25}
}

@inproceedings{10.1145/3746059.3747800,
author = {Takashita, Shuto and Steimle, J\"{u}rgen and Inami, Masahiko},
title = {Imaginary Joint: Proprioceptive Feedback for Virtual Body Extensions via Skin Stretch},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747800},
doi = {10.1145/3746059.3747800},
abstract = {Virtual body extensions such as a wing or tail have the potential to offer users new bodily experiences and capabilities in virtual and augmented reality. To use these extensions as naturally as one’s own body—particularly for body parts that are normally hard to see, such as a tail—it is essential to provide proprioceptive feedback that allows users to perceive the position, orientation, and force exerted by these parts, rather than relying solely on visual cues. In this study, we propose a novel approach by introducing an "Imaginary Joint" at the interface between the user’s actual body and the virtual extension, delivering information about joint flexion and force through skin-stretch feedback. We present a wearable device for skin-stretch feedback and explore informing mappings that convey the bending rotation and torque of the Imaginary Joint. The final system presents both types of information simultaneously by superimposing these skin deformations. Results from a controlled experiment with users demonstrate that users could identify tail position and force without relying on visual cues, and do so more effectively than in the vibrotactile condition. Furthermore, the tail was perceived as more embodied than in a vibrotactile condition, resulting in a more naturalistic and intuitive sensation. Finally, we introduce several application scenarios, including Perception of Extended Bodies, Enhanced Bodily Expression, and Body-Mediated Communication, and discuss the potential for future extensions of this system.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {63},
numpages = {15},
keywords = {Augmented human, avatar, XR, virtual reality, embodiment, proprioceptive feedback.},
location = {
},
series = {UIST '25}
}

@inproceedings{10.1145/3746059.3747714,
author = {Li, Yadong and Wang, Shuning and Fu, Yongjian and Chen, Justin and Chen, Xingyu and Ren, Ju and Zhang, Xinyu and Gadre, Akshay and Sun, Ke},
title = {UltraPoser: Pushing the Limits of IMU-based Full-Body Pose Estimation with Ultrasound Sensing on Consumer Wearables},
year = {2025},
isbn = {9798400720376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746059.3747714},
doi = {10.1145/3746059.3747714},
abstract = {Full-body motion capture using IMUs embedded in consumer wearables has the potential to enable convenient, on-the-go tracking with minimal instrumentation. However, the sparse placement of these devices on the body frame presents challenges such as limited body coverage, reduced motion feature diversity, and cumulative drift errors. This paper introduces UltraPoser, a multi-modal full-body motion capture system that integrates ultrasonic sensing with inertial measurements for improved fidelity, broader coverage and increased reliability. UltraPoser&nbsp;leverages built-in microphones and speakers on commodity wearables, such as smartphones and smartwatches, to transmit and receive inaudible ultrasound signals, expanding the range of sensed body areas and providing drift-free acoustic multipath profiles. To implement UltraPoser, we systematically explore ultrasound signal designs to maximize feature quality and propose a graph-based physics-aware fusion architecture to integrate heterogeneous sensing modalities. We evaluate our approach using the UltraPoser&nbsp;Dataset, collected from 10 participants across diverse device placements and activity contexts. Compared to state-of-the-art IMU-only methods, UltraPoser&nbsp;achieves a 28.46\% improvement in overall pose estimation accuracy and up to 67.28\% error reduction for specific limbs without directly attached sensors.},
booktitle = {Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology},
articleno = {93},
numpages = {15},
keywords = {Motion capture, ultrasound sensing, multi-modal fusion},
location = {
},
series = {UIST '25}
}

